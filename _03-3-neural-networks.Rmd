## Neuronale Netzwerke


```{r, echo=FALSE}
library(tidyverse)
library(magrittr)
library(keras)
library(arrow)
Rcpp::sourceCpp('src/calc_payoff_per_title.cpp')
source("R/03-analysis.R")

data_wide_3 <- arrow::read_feather("data/steps/data_wide_3.feather")
data_wide_3_all <- arrow::read_feather("data/steps/data_wide_3_all.feather")

set.seed(321654)
train_idx <- sample(x = nrow(data_wide_3), size = floor(0.8 * nrow(data_wide_3)))
test_idx <- setdiff(seq_len(nrow(data_wide_3)), train_idx)

```


### Unabhängige Vorhersagewerte

#### Ohne Berücksichtigung der Ordinalität

```{r nn-indipendent-categorical-training, echo=FALSE}
n_groups_per_col <- 30

ind_categorical_discretization_low <- multivariate_discretization(data_wide_3_all, train_idx, test_idx, "Low_0", n_groups_per_col)
ind_categorical_discretization_high <- multivariate_discretization(data_wide_3_all, train_idx, test_idx, "High_0", n_groups_per_col)
ind_categorical_discretization_close <- multivariate_discretization(data_wide_3_all, train_idx, test_idx, "Close_0", n_groups_per_col)

### model for close
ind_categorical_model_names <- c("low_pred_prob", "high_pred_prob", "close_pred_prob")
ind_categorical_model_paths <- paste0("data/models/ind/categorical/", ind_categorical_model_names, ".feather")

if (all(map_lgl(ind_categorical_model_paths, file.exists))) {
  ind_categorical_models <- map(ind_categorical_model_paths, read_feather) %>% setNames(ind_categorical_model_names)
} else {
  stop(paste0("At least one ind_categorical model does not exist. Please execute python code first."))
  
  ind_categorical_labels_low <- ind_categorical_discretization_low %$% groups
  ind_categorical_labels_high <- ind_categorical_discretization_high %$% groups
  ind_categorical_labels_close <- ind_categorical_discretization_close %$% groups
  ind_categorical_labels <- tibble(low = ind_categorical_labels_low, high = ind_categorical_labels_high, close = ind_categorical_labels_close)
  
  write_feather(ind_categorical_labels[train_idx, ], "data/ind_categorical_labels_train.feather")
  write_feather(select(data_wide_3, -Ticker, -Date)[train_idx, ], "data/ind_categorical_data_train.feather")
  
  write_feather(ind_categorical_labels[test_idx, ], "data/ind_categorical_labels_test.feather")
  write_feather(select(data_wide_3, -Ticker, -Date)[test_idx, ], "data/ind_categorical_data_test.feather")
  
  # Execute jupyter notebook: py/fit_ind_categorical_model.ipynb
}

```


```{r nn-indipendent-categorical-plotting, fig.cap='Verteilung prognostizierter Tiefst-, Höchst- und Schlusspreise', fig.asp=1, fig.pos = '!H',}
curr_eval_id <- 53

low_prob <- unlist(ind_categorical_models$low_pred_prob[curr_eval_id, ], use.names = FALSE)
high_prob <- unlist(ind_categorical_models$high_pred_prob[curr_eval_id, ], use.names = FALSE)
close_prob <- unlist(ind_categorical_models$close_pred_prob[curr_eval_id, ], use.names = FALSE)

hist_data_low <- eval_hist_data(ind_categorical_discretization_low$borders, low_prob, "Tief")
hist_data_high <- eval_hist_data(ind_categorical_discretization_high$borders, high_prob, "Hoch")
hist_data_close <- eval_hist_data(ind_categorical_discretization_close$borders, close_prob, "Schluss")
plot_price_histogram(
  data = bind_rows(hist_data_low, hist_data_close, hist_data_high) %>% mutate(group = factor(group, levels = unique(group))), 
  title = "Verteilung prognostizierter Tiefst-, Höchst- und Schlusspreise"
)
```

```{r nn-indipendent-categorical-evaluation}

mid_prices <- bind_rows(
  mutate(ind_categorical_discretization_low$borders, prob = 1, type = "Low"),
  mutate(ind_categorical_discretization_high$borders, prob = 2, type = "High"),
  mutate(ind_categorical_discretization_close$borders, prob = 3, type = "Close")
) %>% mutate(mid = (lower + upper) / 2) %>%
  select(Bucket, mid, type, prob) %>%
  pivot_wider(names_from = type, values_from = c("mid", "prob"))




price_combinations <- expand(mid_prices, Low, High, Close) %>%
  rename(Close_0 = "Close") %>%
  mutate(Close_1 = 100)
implausible_price_combinations <- which(
  price_combinations$Close_0 < price_combinations$Low | 
    price_combinations$Close_0 > price_combinations$High |
    !is.finite(price_combinations$Low) | 
    !is.finite(price_combinations$High) | 
    !is.finite(price_combinations$Close_0)
)
price_combinations[implausible_price_combinations, ] <- NA


buy_sell_combinations <- expand(mid_prices, Low, High) %>% select(buy = Low, sell = High)
implausible_buy_sell_combinations <- buy_sell_combinations$buy > buy_sell_combinations$sell |
  !is.finite(buy_sell_combinations$buy) | 
  !is.finite(buy_sell_combinations$sell) 
buy_sell_combinations[implausible_buy_sell_combinations, ] <- NA

buy_first_payoffs <- map2_dfc(
  .x = buy_sell_combinations$buy, 
  .y = buy_sell_combinations$sell, 
  .f = ~calc_payoff_const_gamma(price_combinations, buy = .x, sell = .y, both_first = "buy")
) %>% as.matrix() %>% t()


microbenchmark::microbenchmark({buy_first_payoffs %*% rnorm(27000)}, times = 100)

```



```{r nn-indipendent-binary, echo=FALSE}

n_groups_per_col <- 30

ind_binary_model_names <- c("low_pred_prob",  "high_pred_prob", "close_pred_prob")
ind_binary_model_paths <- paste0("data/models/ind/binary/", ind_binary_model_names, ".feather")

if (all(map_lgl(ind_binary_model_paths, file.exists))) {
  ind_binary_pred <- map(ind_binary_model_paths, read_feather) %>% setNames(ind_binary_model_names)
} else {
  stop(paste0("At least one ind_binary model does not exist. Please execute python code first."))
  
  ind_binary_labels_low <- multivariate_discretization(data_wide_3_all, train_idx, test_idx, "Low_0", n_groups_per_col) %$% groups
  ind_binary_labels_low <- purrr::map_dfc(seq_len(n_groups_per_col) - 1, ~as.integer(.<=ind_binary_labels_low))
  
  ind_binary_labels_high <- multivariate_discretization(data_wide_3_all, train_idx, test_idx, "High_0", n_groups_per_col) %$% groups
  ind_binary_labels_high <- purrr::map_dfc(seq_len(n_groups_per_col) - 1, ~as.integer(.<=ind_binary_labels_high))
  
  ind_binary_labels_close <- multivariate_discretization(data_wide_3_all, train_idx, test_idx, "Close_0", n_groups_per_col) %$% groups
  ind_binary_labels_close <- purrr::map_dfc(seq_len(n_groups_per_col) - 1, ~as.integer(.<=ind_binary_labels_close))
  
  write_feather(ind_binary_labels_low[train_idx, ], "data/ind_binary_labels_low_train.feather")
  write_feather(ind_binary_labels_high[train_idx, ], "data/ind_binary_labels_high_train.feather")
  write_feather(ind_binary_labels_close[train_idx, ], "data/ind_binary_labels_close_train.feather")
  write_feather(select(data_wide_3, -Ticker, -Date)[train_idx, ], "data/ind_binary_data_train.feather")
  
  write_feather(ind_binary_labels_low[test_idx, ], "data/ind_binary_labels_low_test.feather")
  write_feather(ind_binary_labels_high[test_idx, ], "data/ind_binary_labels_high_test.feather")
  write_feather(ind_binary_labels_close[test_idx, ], "data/ind_binary_labels_close_test.feather")
  write_feather(select(data_wide_3, -Ticker, -Date)[test_idx, ], "data/ind_binary_data_test.feather")
  
  # Execute jupyter notebook: py/fit_ind_binary_model.ipynb
}

```



#### Mit Berücksichtigung der Ordinalität



### Abhängigige Vorhersagewerte


```{r, keras-individual-categorial, echo=FALSE}

fit_categorical_model <- function(data_all, data, train_idx, test_idx, cols, n_groups_per_col) {
  
  all_data <- as.matrix(dplyr::select(data_wide_3, -Ticker, -Date))
  train_data <- all_data[train_idx, ]
  test_data <- all_data[test_idx, ]
  
  all_labels <- multivariate_discretization(data_all, train_idx, test_idx, cols, n_groups_per_col) %$% groups
  train_labels <- all_labels[train_idx]
  test_labels <- all_labels[test_idx]
  
  model <- keras::keras_model_sequential() %>%
    keras::layer_dense(units = 512, activation = "relu",  input_shape = dim(train_data)[2]) %>%
    keras::layer_dense(units = 512, activation = "relu") %>%
    keras::layer_dense(units = n_groups_per_col^length(cols), activation = "softmax")
  
  model %>% keras::compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = c('accuracy')
  )
  
  history <- model %>% keras::fit(
    train_data,
    train_labels,
    epochs = 10,
    batch_size = 512,
    validation_split = 0.2
  )
  
  return(list(model = model, history = history))
}

low_win_3_bin_30 <- fit_categorical_model(data_wide_3_all, data_wide_3, train_idx, test_idx, "Low_0", 30)
save_model_hdf5(low_win_3_bin_30$model, "data/low_win_3_bin_30_model.hdf5")
saveRDS(low_win_3_bin_30$history, "data/low_win_3_bin_30_history.rds")
# low_win_3_bin_30$model <- load_model_hdf5("data/low_win_3_bin_30_model.hdf5")

high_win_3_bin_30 <- fit_categorical_model(data_wide_3_all, data_wide_3, train_idx, test_idx, "High_0", 30)
save_model_hdf5(high_win_3_bin_30$model, "data/high_win_3_bin_30_model.hdf5")
saveRDS(high_win_3_bin_30$history, "data/high_win_3_bin_30_history.rds")
# high_win_3_bin_30$model <- load_model_hdf5("data/high_win_3_bin_30_model.hdf5")

close_win_3_bin_30 <- fit_categorical_model(data_wide_3_all, data_wide_3, train_idx, test_idx, "Close_0", 30)
save_model_hdf5(close_win_3_bin_30$model, "data/close_win_3_bin_30_model.hdf5")
saveRDS(close_win_3_bin_30$history, "data/close_win_3_bin_30_history.rds")
# close_win_3_bin_30$model <- load_model_hdf5("data/close_win_3_bin_30_model.hdf5")


low_binary_win_3_bin_30 <- fit_binary_model(data_wide_3_all, data_wide_3, train_idx, test_idx, "Low_0", 30)
save_model_hdf5(low_binary_win_3_bin_30$model, "data/low_binary_win_3_bin_30_model.hdf5")
saveRDS(low_binary_win_3_bin_30$history, "data/low_binary_win_3_bin_30_history.rds")

high_binary_win_3_bin_30 <- fit_binary_model(data_wide_3_all, data_wide_3, train_idx, test_idx, "High_0", 30)
save_model_hdf5(high_binary_win_3_bin_30$model, "data/high_binary_win_3_bin_30_model.hdf5")
saveRDS(high_binary_win_3_bin_30$history, "data/high_binary_win_3_bin_30_history.rds")

close_binary_win_3_bin_30 <- fit_binary_model(data_wide_3_all, data_wide_3, train_idx, test_idx, "Close_0", 30)
save_model_hdf5(close_binary_win_3_bin_30$model, "data/close_binary_win_3_bin_30_model.hdf5")
saveRDS(close_binary_win_3_bin_30$history, "data/close_binary_win_3_bin_30_history.rds")


```




```{r, keras-individual-categorial-evaluation, echo=FALSE}
get_class_prices <- function(borders) {
  class_borders <- borders %>% set_names(c("Bucket", "Lower_Border", "Upper_Border"))
  class_borders[1, "Lower_Border"] <- class_borders[1, "Upper_Border"]
  class_borders[nrow(class_borders), "Upper_Border"] <- class_borders[nrow(class_borders), "Lower_Border"]
  (class_borders$Lower_Border + class_borders$Upper_Border) / 2
}

test_data <- as.matrix(dplyr::select(data_wide_3, -Ticker, -Date))[test_idx, ]
n_groups_per_col <- 100


curr_eval_id <- 781

# buy prices
model_low <- load_model_hdf5("models/model_low_100.hdf5")
pred_prob_low <- predict_proba(model_low, test_data[curr_eval_id, , drop = FALSE], batch_size = 512)
# pred_classes_low <- predict_classes(model_low, test_data, batch_size = 512)

discretization_low <- multivariate_discretization(data_wide_3_all, train_idx, test_idx, "Low_0", n_groups_per_col)
hist_data_low <- discretization_low$borders %>%
  mutate(prob = as.vector(pred_prob_low)) %>%
  rename_at(dplyr::vars(tidyselect::ends_with("lower")), ~"lower") %>%
  rename_at(dplyr::vars(tidyselect::ends_with("upper")), ~"upper") %>%
  select("lower", "upper", "prob") %>%
  mutate(group = "Low")

# sell prices
model_high <- load_model_hdf5("models/model_high_100.hdf5")
pred_prob_high <- predict_proba(model_high, test_data[curr_eval_id, , drop = FALSE], batch_size = 512)
# pred_classes_high <- predict_classes(model_high, test_data, batch_size = 512)

discretization_high <- multivariate_discretization(data_wide_3_all, train_idx, test_idx, "High_0", n_groups_per_col)

hist_data_high <- discretization_high$borders %>%
  mutate(prob = as.vector(pred_prob_high)) %>%
  rename_at(dplyr::vars(tidyselect::ends_with("lower")), ~"lower") %>%
  rename_at(dplyr::vars(tidyselect::ends_with("upper")), ~"upper") %>%
  select("lower", "upper", "prob") %>%
  mutate(group = "High")

plot_price_histogram(bind_rows(hist_data_low, hist_data_high), "Some test")

test <- expand.grid(
  Close_1 = 100,
  Low = discretization_low$borders$Low_0_lower,
  High = discretization_high$borders$High_0_lower,
  Close_0 = (discretization_low$borders$Low_0_lower + discretization_high$borders$High_0_lower) / 2
)

test$Low <- ifelse(!is.finite(test$Low ), 97, test$Low)
test$High <- ifelse(!is.finite(test$Low ), 103, test$Low)
test$Close_0 <- ifelse(!is.finite(test$Low ), 100, test$Low)


set.seed(654951)
both_first <- c("buy", "sell")[sample(c(1, 2), nrow(test), replace = TRUE)]

calc_payoff_const_gamma(tibble(Close_1 = 100, Low = 102, High = 105, Close_0 = 103), buy = -Inf, sell = Inf, both_first = "buy")


microbenchmark::microbenchmark({
  sum(calc_payoff_const_gamma(test, buy = 97, sell = 103, both_first = both_first))
})



sell <- get_class_prices(discretization_high$borders, pred_classes_high)


eval_data <- data_wide_3_all[test_idx, ] %>% select(Close_1, Open = Open_0, Low = Low_0, High = High_0, Close_0)



data_wide_3_all[test_idx, ][1, ]
buy[1]
sell[1]

scale_fct <- sum(calc_payoff_const_gamma(eval_data, both_first = both_first))
sum(calc_payoff_const_gamma(eval_data, buy = buy, sell = sell, both_first = both_first)) / scale_fct

# max
sum(calc_payoff_const_gamma(eval_data, buy = eval_data$Low, sell = eval_data$High, both_first = both_first)) / scale_fct

# open
sum(calc_payoff_const_gamma(eval_data, buy = eval_data$Open * (1 - 0.045), sell = eval_data$Open * (1 + 0.045), both_first = both_first)) / scale_fct


```


```{r, keras-combined-individual-binary, echo=FALSE}

fit_binary_model <- function(data_all, data, train_idx, test_idx, cols, n_groups_per_col) {
  
  # https://www.cs.waikato.ac.nz/~eibe/pubs/ordinal_tech_report.pdf
  # http://orca.st.usm.edu/~zwang/files/rank.pdf
  # https://machinelearningmastery.com/how-to-prepare-categorical-data-for-deep-learning-in-python/
  
  all_data <- as.matrix(dplyr::select(data_wide_3, -Ticker, -Date))
  train_data <- all_data[train_idx, ]
  
  all_labels <- multivariate_discretization(data_all, train_idx, test_idx, cols, n_groups_per_col) %$% groups
  train_labels <- all_labels[train_idx]
  
  # make them "ordinal"
  train_labels <- purrr::map(seq_len(n_groups_per_col) - 1, ~as.integer(.<=train_labels)) %>% unlist() %>% matrix(., ncol = n_groups_per_col)
  
  model <- keras::keras_model_sequential() %>%
    keras::layer_dense(units = 512, activation = "relu",  input_shape = dim(train_data)[2]) %>%
    keras::layer_dense(units = 512, activation = "relu") %>%
    keras::layer_dense(units = n_groups_per_col^length(cols), activation = "sigmoid")
  
  model %>% keras::compile(
    optimizer = 'adam',
    loss = 'binary_crossentropy',
    metrics = c('accuracy')
  )
  
  history <- model %>% keras::fit(
    train_data,
    train_labels,
    epochs = 10,
    batch_size = 512,
    validation_split = 0.2
  )
  
  return(list(model = model, history = history))
}


low_binary_win_3_bin_30 <- fit_binary_model(data_wide_3_all, data_wide_3, train_idx, test_idx, "Low_0", 30)
save_model_hdf5(low_binary_win_3_bin_30$model, "data/low_binary_win_3_bin_30_model.hdf5")
saveRDS(low_binary_win_3_bin_30$history, "data/low_binary_win_3_bin_30_history.rds")

high_binary_win_3_bin_30 <- fit_binary_model(data_wide_3_all, data_wide_3, train_idx, test_idx, "High_0", 30)
save_model_hdf5(high_binary_win_3_bin_30$model, "data/high_binary_win_3_bin_30_model.hdf5")
saveRDS(high_binary_win_3_bin_30$history, "data/high_binary_win_3_bin_30_history.rds")

close_binary_win_3_bin_30 <- fit_binary_model(data_wide_3_all, data_wide_3, train_idx, test_idx, "Close_0", 30)
save_model_hdf5(close_binary_win_3_bin_30$model, "data/close_binary_win_3_bin_30_model.hdf5")
saveRDS(close_binary_win_3_bin_30$history, "data/close_binary_win_3_bin_30_history.rds")

```



```{r, keras-combined-softmax-no-saampling, echo=FALSE}

n_groups_per_col <- 10
cols <- c("Low_0", "High_0", "Close_0")
all_labels <- multivariate_discretization(data_wide_3_all, train_idx, test_idx, cols, n_groups_per_col) %$% groups

train_labels <- all_labels[train_idx]
test_labels <- all_labels[test_idx]

model <- keras::keras_model_sequential() %>%
  keras::layer_dense(units = 512, activation = "relu",  input_shape = dim(train_data)[2]) %>%
  keras::layer_dense(units = 512, activation = "relu") %>%
  keras::layer_dense(units = n_groups_per_col^length(cols), activation = "softmax")

model %>% keras::compile(
  optimizer = 'adam',
  loss = 'sparse_categorical_crossentropy',
  metrics = c('accuracy')
)

history <- model %>% keras::fit(
  train_data,
  train_labels,
  epochs = 2,
  batch_size = 512,
  validation_split = 0.2
)

summary(model)
keras::save_model_hdf5(model, "data/2_dense_512_window_3_epoch_50_batch_128_val_split_20.hdf5")
print("done")

```

##### Old



```{r, knn-calculation, echo=FALSE}
# data_bkp <- data
# data <- data_bkp
orig_order <- order(desc(data$Date))
data <- data %>% select(-Ticker) %>% arrange(desc(Date))

counts <- data %>% select(Date) %>% group_by(Date) %>% summarise(cnt = n()) %>% ungroup() %>% mutate(cum_sum = cumsum(cnt))
n_chunks <- 100
breaks <- nrow(data) / n_chunks * seq_len(n_chunks)

split_dates <- map(breaks, ~counts$Date[[min(which(counts$cum_sum>=.))]])

nn <- function(i, split_dates, data, dates){
  split_date <- split_dates[[i]]
  curr_data <- data[dates <= split_date, ]
  curr_dates <- dates[dates <= split_date]
  curr_query <- data[dates <= split_date & dates > ifelse(i == 1, -Inf, split_dates[[i-1]]), ]
  RANN2::nn2_cpp2(data = curr_data, query = curr_query, group = as.integer(curr_dates), k = 50, k_internal = 1.2*50)
}

knn_eucl_list <- pbmcapply::pbmclapply(
  X = rev(seq_len(n_chunks)),
  FUN = nn,
  split_dates = split_dates,
  data = as.matrix(select(data, -Date)),
  dates = data$Date,
  mc.cores = parallel::detectCores()
)

knn_eucl_list_hash <- digest::digest(knn_eucl_list)
# saveRDS(knn_eucl_list_hash, paste0("tmp/knn_eucl_list_", knn_eucl_list_hash, ".rds"))

knn_eucl <- purrr::transpose(knn_eucl_list) %>% map(~do.call(rbind, .[rev(seq_along(.))]))
knn_eucl_hash <- digest::digest(knn_eucl)
# arrow::write_feather(as_tibble(knn_eucl$nn.idx), paste0("tmp/knn_eucl_idx_", knn_eucl_hash, ".feather"))
# arrow::write_feather(as_tibble(knn_eucl$nn.dists), paste0("tmp/knn_eucl_dists_", knn_eucl_hash, ".feather"))

```



```{r, knn-single-plot, echo=FALSE}

knn_eucl_idx <- arrow::read_feather("tmp/knn_eucl_idx_7d3356cf64a5f81081840f6b1b370d77.feather")
knn_eucl_dists <- arrow::read_feather("tmp/knn_eucl_dists_7d3356cf64a5f81081840f6b1b370d77.feather")


idx <- 123456

knn_eucl_idx[idx, ]
knn_eucl_dists[idx, ]
select(data[idx, ], -c("Ticker", "Date"))
select(data[head(as.integer(knn_eucl_idx[idx, ]), 1), ], -c("Ticker", "Date"))

sqrt(sum((select(data[idx, ], -c("Ticker", "Date")) - select(data[head(as.integer(knn_eucl_idx[idx, ]), 1), ], -c("Ticker", "Date")))^2))

id_cols <- c("Ticker", "Date")
knn <- RANN::nn2(
  data = select(data, -id_cols),
  query = select(data[idx, ], -id_cols),
  k = 10
)

plot_nn(
  data_wide_curr = data_all[idx, ],
  data_wide_nn = data_all[c(3660437, 2876442, 1411227), ]
)

# id <- 280000
# valid_idx <- seq_len(n)[rowSums(is.na(nn$nn.idx)) == 0]
#
# k <- 10
# plot_nn(data_wide_0[valid_idx[id], ], data_wide_0[nn$nn.idx[valid_idx[id], seq_len(k)],])

```

```{r, knn-prediction-power, echo=FALSE}
# k <- 20
#
# nn_idx <- as.matrix(arrow::read_arrow("data/nn_idx_eucl_olhc_w3_38a896430298c738055505dc89e042ac.feather"))
#
# nn_pred <- pred_nn(select(data_wide_0, c("Low_0", "High_0")), nn_idx = nn_idx[, seq_len(k)])
# na_row_bool <- rowSums(is.na(nn_pred)) > 0
#
# nn_pred_sample <- nn_pred[!na_row_bool, ] %>% rename(Buy = Low_0, Sell = High_0)
# quotes_line_sample <- quotes_line[!na_row_bool, ]
#
# plot_ratio_history(quotes_line = quotes_line_sample, data_pred = nn_pred_sample)
#
#
# ### perform bootstraping
# size_map <- quotes_line %>%
#   select(Date) %>%
#   group_by(Date) %>%
#   summarize(count = n()) %>%
#   ungroup() %>%
#   mutate(size = cumsum(count) - count) %>%
#   select(-count)
# size <- quotes_line %>% select(Date) %>% left_join(size_map, by = "Date") %>% .[["size"]]
# ###
#
# boot_nn_idx_1 <- bootstrap_nn_idx(nn_idx, size, 20, 123456)
#
# nn_pred_boot <- pred_nn(select(data_wide_0, c("Low_0", "High_0")), nn_idx = boot_nn_idx_1)
# na_row_bool_boot <- rowSums(is.na(nn_pred_boot)) > 0
#
# nn_pred_boot_sample <- nn_pred_boot[!na_row_bool_boot, ] %>% rename(Buy = Low_0, Sell = High_0)
# quotes_line_boot_sample <- quotes_line[!na_row_bool_boot, ]
#
# plot_ratio_history(quotes_line = quotes_line_boot_sample, data_pred = nn_pred_boot_sample)
#
#

```



```{r, knn-bootstraping, echo=FALSE}
# k <- 20
# quotes_line <- readRDS("tmp/quotes_line.rds")
# quotes_line_sorted <- quotes_line %>% arrange(Date)
#
# nn_idx <- arrow::read_arrow("data/nn_idx_eucl_olhc_w3_38a896430298c738055505dc89e042ac.feather")
#
# nn_idx_sorted <- readRDS("data/nn_eucl_olhc_w3_38a896430298c738055505dc89e042ac.rds") %>% .[["nn.idx"]] %>% sort_nn_idx(quotes_line$Date)
#
#
# set.seed(123456)
# boot_pred <- map(seq_len(100), function(i) {
#   print(i)
#   curr_nn_idx_boot <- bootstrap_nn(sort(quotes_line$Date), sort_nn_idx(nn_idx_sorted), k = k)
#   pred_nn(select(quotes_line_sorted, c("Low", "High")), nn_idx = curr_nn_idx_boot)
# })
# # saveRDS(boot_pred, "tmp/boot_pred.rds")
#
# boot_pred <- readRDS("tmp/boot_pred.rds")
#
#
# all_complete <- rep(TRUE, nrow(quotes_line_sorted))
# for(i in seq_along(boot_pred)) {
#   print(i)
#   all_complete <- all_complete * rowSums(is.na(boot_pred[[i]])) == 0
#   gc()
# }
#
# na_row_bool <- rowSums(is.na(nn_idx_sorted)) + rowSums(is.na())
#
# test <- map(boot_pred, ~sum(calc_payoff_const_gamma(quotes_line_sorted[all_complete], buy = .$Low, sell = .$High, both_first = 234567)))
#
# na_row_bool <- rowSums(is.na(nn$nn.idx)) == 0

# nn_idx[4418184, ]
# quotes_line[4418184, ]




```

## Neuronale Netzwerke

## Autoregressive Modelle
