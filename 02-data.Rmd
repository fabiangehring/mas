# Daten

## Bezug und Umfang

### Aktienuniversum

```{r, retrieve-etf-data, echo=FALSE, warning=FALSE, message=FALSE}
suppressPackageStartupMessages({
  library(digest)
  library(here)
  library(dplyr)
  library(stringr)
  library(kableExtra)
  library(purrr)
  library(tidyr)
  library(arrow)
  library(ggplot2)
})

source("global.R")
source("R/download_scripts.R")
source("R/02-data.R")
Rcpp::sourceCpp('src/calc_payoff_per_title.cpp')
source("R/03-analysis.R")


# load stocks_raw
file_stocks_raw <- here::here(paste0("data/stocks_raw.feather"))
if (file.exists(file_stocks_raw)) {
  stocks_raw <- arrow::read_feather(file_stocks_raw)
} else {
  stocks_raw <- download_stocks() 
  arrow::write_feather(stocks_raw, file_stocks_raw)
}
```

Für die vorliegende Analyse werden die Aktienpreise grosser Unternehmen weltweit herangezogen. Konkret werden alle Aktienkomponenten des "iShares MSCI World UCITS ETF" [vgl. @blackrock] per 28. Februar 2020 ^[Das Startdatum dieser Arbeit.] verwendet. Dieser Exchange Traded Fund (ETF) besteht zu diesem Zeitpunkt aus `r nrow(stocks_raw)` Aktien, welche sich automatisiert auslesen lassen. Ferner stellt die Webseite des ETF Emittenten weitere Attribute zur Verfügung. Diese lauten am Beispiel Nesté wie folgt:

``` {r etf-data-nestle-stock, echo=FALSE}
stock_nesn <- dplyr::filter(stocks_raw, Ticker == "NESN")
knitr::kable(tibble(Attribut = names(stock_nesn), Wert = t(stock_nesn)), caption = "Attribute der Nestlé Aktie per 28. Februar 2020") %>%
  kableExtra::kable_styling(position = "center", latex_options = "HOLD_position")
```

Neben eindeutigen Identifiern wie "Ticker" und "ISIN" enthält der Datensatz auch Informationen zur "Asset Class" der Komponente. Für die vorliegende Analyse von Aktien ist hierbei lediglich die Ausprägung "Equity" zulässig. Die Kennzahlen "Weight", "Market Value" und "Notional Value" geben Auskunft über die Grösse der betachteten Unternehmung und eignen sich auch zum Vergleich ebendieser. Zu beachten gilt, dass im Falle von Aktien der "Notional Value" dem "Market Value" entspricht und sich dieser bis auf rundungsbedingte Differenzen auch aus der Anzahl ausgegebener Titel mal Tagesendkurs ("Shares" x "Price") ermitteln lässt. 

Als weitere Unterscheidungsmerkmale sind der Hauptbörsenplatz "Exchange" (`r length(unique(stocks_raw$Exchange))` unterschiedliche Ausprägungen), der Sitz "Location" (`r length(unique(stocks_raw$Location))` Ausprägungen) sowie die Währung "Market Currency" (`r length(unique(stocks_raw[["Market Currency"]]))` Ausprägungen) aufgeführt. Alle drei Attribute bilden stark verwandte Informationen ab. Geschlüsselt auf Kontinente ergeben sich für den Börsensitz folgende Anteile: 

- Nordamerika: `r round(sum(stocks_raw$Location %in% c("United States", "Canada"))/nrow(stocks_raw) * 100, 0)`% 
- Europa: `r round(sum(stocks_raw$Location %in% c("Portugal", "Austria", "Ireland", "Norway", "Belgium", "Finland", "Israel", "Denmark", "Netherlands", "Spain", "Italy", "Sweden", "Switzerland", "Germany", "France", "United Kingdom"))/nrow(stocks_raw) * 100, 0)`% 
- Asien: `r round(sum(stocks_raw$Location %in% c("Japan", "Hong Kong", "Singapore"))/nrow(stocks_raw) * 100, 0)`% 
- Australien: `r round(sum(stocks_raw$Location %in% c("Australia", "New Zealand"))/nrow(stocks_raw) * 100, 0)`%

Als letzes Attribut enthält der Datensatz Angaben zum "Sector" in welchem die jeweilige Unternehmung tätigt ist. Bis auf die Kategorien "Energy" und "Other" ist jeder der `r length(unique(stocks_raw$Sector))` aufgeführten Sektoren mit mindestens 5 Anteil vorhanden. Die beiden Sektoren mit dem höchsten Anteil sind "Industrials" und "Financials".


``` {r, fig.cap='Anzahl im Datensatz vorhandene Titel je Sektor', fig.asp=1, fig.pos = '!H', analysis-etf-data, echo=FALSE, message=FALSE, warning=FALSE}
table(stocks_raw$Sector) %>%
  sort(decreasing = FALSE) %>%
  tibble(Sektor = factor(names(.), levels = unique(names(.))), Anteil = . / sum(.) ) %>%
  ggplot2::ggplot(ggplot2::aes(x = Sektor, y = Anteil)) + 
  ggplot2::geom_bar(stat = "identity") +
  ggplot2::scale_y_continuous(labels=scales::percent) +
  ggplot2::coord_flip()
```

### Preisinformationen

``` {r, retrieve-quotes-data, echo=FALSE}
# load quotes
file_quotes_raw <- here::here(paste0("data/quotes_raw.feather"))
if (file.exists(file_quotes_raw)) {
  quotes_raw <- arrow::read_feather(file_quotes_raw)
} else {
  quotes_raw <- download_quotes(stocks_raw) # takes about 35mins
  arrow::write_feather(quotes_raw, file_quotes_raw)
}
```

Für alle Titel werden in einem zweiten Schritt die historischen Aktienkurse inkl. Höchst- und Tiefstkurs bezogen. Diese Daten stehen via Yahoo Finance auf täglicher Basis zur freien Nutzung zur Verfügung [vgl. @yahoo_finance]. Zu beachten gilt es hierbei, dass die Yahoo Ticker für ausserhalb der USA gehandelte Titel einen Suffix je Börsenplatz verwenden. Für die Analyse kommen so `r formatC(nrow(quotes_raw), big.mark = "'")` tägliche Datenwerte für `r formatC(length(unique(quotes_raw$Ticker)), big.mark = "'")` Titel zusammen. Für `r formatC(nrow(stocks_raw) - length(unique(quotes_raw$Ticker)), big.mark = "'")` Aktien können keine Werte gefunden werden. Tabelle \@ref(tab:quote-data-nestle) zeigt einen Beispieleintrag für die Aktie von Nesté per 14. Februar 2019. 


``` {r quote-data-nestle, echo=FALSE}
quote_nesn <- dplyr::filter(quotes_raw, Ticker == "NESN.SW" & Date == as.Date("2019-04-12"))

knitr::kable(tibble(Attribut = names(quote_nesn), Wert = t(quote_nesn)), caption = "Kursinformationen der Nesté Aktie per 12. April 2019") %>%
  kableExtra::kable_styling(position = "center", latex_options = "HOLD_position")
```

Die Werte "Open", "Low", "High" und "Close" zeigen Eröffnungs-, Tiefst-, Höchst- und Schlusskurs des Titels. Mit "Volume" werden die Anzahl gehandelter Titel am jeweiligen Tag angegeben. Unter "Adjusted" ist der um Dividendenausschüttungen korrigierte Schlusskurs aufgeführt.^[Allfällige Aktiensplits sind in allen Werten bereits berücksichtigt.] 

Die Werte in Tabelle \@ref(tab:quote-data-nestle) sind insofern speziell, als dass sie den letzten Tag vor dem Ex-Dividend Datum von Nestlé für 2019 betreffen. Das heisst, es sind die Kurse des letzten Tages, bevor die Aktie ohne die für das Jahr 2019 ausgeschüttete Dividende gahandelt wurde. Die Dividende betrug in jenem Jahr CHF 2.45 [vgl. @nestle]. Diese Differenz widerspiegelt sich in den Daten als Differenz des Close- und Adjusted-Preises. Da alle Werte (auch Open, Low, High und Close) am Folgetag ohne den Anspruch auf diese Dividende gehandelt werden, fallen diese typischerweise tiefer aus. Um eine Vergleichbarkeit der Renditen über die Zeit zu gewährleisten ist daher eine Anpassung der Werte mit Hilfe des Adjustement-Faktors nötig. Dieser ergibt sich als Quotient von Adjusted und Close Preis und wird auf allen Einträgen angewendet.

Weiter erwähnenswert ist, dass Yahoo den Adjusted Kurs des jeweils aktuellsten Tages - ausser eben am Tag vor Ex-Dividend - mit auf aktuellen Kurs festlegt. Im Lauf der Zeit und mit neuen Ausschüttungen verändert sich damit auch die Historie der Adjusted Werte. Dies lässt sich zeigen, wenn der Beispieleintrag von Nestlé per 12. April 2019 nach der nächsten Dividendenausschüttung (27. April 2020) noch einmal abgerufen wird [vgl. @nestle]. Während alle Preise ausser "Adjusted" identisch ausgewiesen sind, hat sich dieser neu auf 90.72 verändert [vgl. @yahoo_finance]. Mit der nächsten Dividenenausschüttung (voraussichtlich im April 2021) wird sich dieser Wert dann wieder ändern. Mit Hilfe der Adjustierung ist aber gewährleistet, dass die Werte vergleichbar bleiben.


## Aufbereitung
### Bereinigung

Mit Yahoo Finance wird ein erfahrener und häufig verwendeter Datenanbieter gewählt. Der Blick auf einige Quantilskennzahlen der Rohdaten in Tabelle \@ref(tab:summary_before_clean) zeigt aber, dass dennoch einige Datenprobleme ausgemacht werden können.

```{r, summary_before_clean, echo=FALSE}
summary(quotes_raw[, setdiff(names(quotes_raw), c("Date", "Ticker", "Volume"))])
```

Die Behebung dieser Mängel und die damit einhergehende Bereinigung erfolgt in verschiedenen Schritten. Allen gemeinsam ist, dass die Bereinigung keinen Ausschluss der Daten zur Folge hat, sondern betroffene Werte als "Nicht verfügbar, (NA)" klassifiziert werden. Diese Unterscheidung ist insbesondere bei rollierender Betrachtung eines Zeitfensters der Vergangenheit von Bedeutung.

``` {r, echo=FALSE}
file_quotes_clean <- here::here(paste0("data/quotes_clean.feather"))
if (!file.exists(file_quotes_clean)) {
  quotes_clean <- quotes_raw
}

```

```{r, filter-data-adjusted, echo=FALSE}
wrong_corporate_action_threshold <- 0.001
if (!file.exists(file_quotes_clean)) {
  quotes_clean <- na_wrong_corporate_actions(quotes_clean, wrong_corporate_action_threshold)
  quotes_clean <- adjust_quotes(quotes_clean)
}
```
1. **Entfernen von fehlerhaften Adjustierungsdaten**  
Eine Eigenschaft von Aktienpreisen ist es, dass sie nicht negativ sein können. Der Datensatz weist aber vereinzelt negative Adjusted Werte aus. Dies lässt sich auch durch die Dividendenbereinigung nicht erklären. Bei diesen Einträgen scheinen daher Datenfehler vorzuliegen. Erschwerend kommt hinzu, dass sich Fehler bei der Adjustierung nicht auf den jeweiligen Eintrag beschränken müssen. Aufgrund der Funktionsweise der Adjustierung (vgl. \@ref(preisinformationen)) ist ein vererben des Fehlers auf andere Einträge des jeweiligen Titels wahrscheinlich. Bei genauerer Betrachtung der Adjusted Werte fällt ferne auf, dass auch einige sehr sehr kleine (im Bereich $-10^{-6}$), wenn auch postive Werte gefunden werden können. Aus diesen Grund werden alle Ticker, welche in ihrer Historie einen Adjusted Preis von weniger als `r wrong_corporate_action_threshold` aufweisen, ausgeschlossen.

```{r, filter-data-measure-order, echo=FALSE}
if (!file.exists(file_quotes_clean)) {
  quotes_clean <- na_unreasonable_measure_order(quotes_clean)
}
```
1. **Entfernen von Einträgen mit unerwarterer Reihenfolge**  
Die Werte für Open, Low, High und Close implizieren eine klare Reihenfolge. Keine anderer der Werte darf höher als das High oder kleiner als das Low sein. Ist dies der Fall, werden die entsprechenden Werte von der Analyse ausgeschlossen.

```{r, filter-data-typo, echo=FALSE}
max_typo_ratio <- 8
if (!file.exists(file_quotes_clean)) {
  quotes_clean <- na_typos(quotes_clean, max_typo_ratio)
}
```
1. **Erkennen und Ausschluss von Tippfehlern**  
Einzelne Einträge lassen sich als Tippfehler identifizieren. Der Titel "AV.L" weist per 09. August 2019 beispielsweise einen Low-Wert von 3.87 aus, währenddem alle andern Werte des gleichen Tages wie auch der benachbarten Tage bei ca. 380 liegen. Es liegt auf der Hand, dass dieser Wert um einen Faktor 100 falsch erfasst wurde. Solchen Fehlern wird begegnet, indem alle paarweisen Verhältnisse von Open, Low, High und Close Preis kleiner als `r max_typo_ratio` sein müssen. Andernfalls erfolgt ein Ausschluss der als Tippehler identifizierten Kennzahl.

```{r, filter-data-no-intraday-moves, echo=FALSE}
if (!file.exists(file_quotes_clean)) {
  quotes_clean <- na_no_intraday_moves(quotes_clean)
}
```
1. **Fehlende Preisbewegungen innerhalb des Tages**  
Der Aktienkurs eines grösseren Unternehens bewegt sich typischerweise auch an ruhigen Börsentagen immer ein wenig. Die vorhandenen Preise  liegen mit der Genauigkeit mehrer Nachkommastellen vor. Unterscheiden sich hierbei Tagestiefst- und Tageshöchstpreis nicht, muss von einem Datenfehler ausgegangen werden. Einträge ohne Preisbewegung innerhalb des Tages werden daher von der Analyse ausgeschlossen.

```{r, filter-data-no-daily-changes, echo=FALSE}
if (!file.exists(file_quotes_clean)) {
  quotes_clean <- quotes_clean %>%
    group_by(Ticker) %>%
    dplyr::group_modify(~na_no_daily_changes(.)) %>%
    ungroup()
}
```
1. **Fehlende Kursbewegungen über nacheinanderfolgende Börsentage**  
Ähnlich wie bei den Preisbewegungen innerhalb des Tages verhält es sich auch bei Bewegungen über sich folgende Börsentage hinweg. Es ist zu erwarten, dass sich mindestens einer der fünf betrachteten Preise vom Vortag unterscheidet. Ist dies nicht der Fall, wird der Eintrag ausgeschlossen.

```{r, filter-data-too-large-daily-changes, echo=FALSE}
too_large_daily_change_ratio <- 2
if (!file.exists(file_quotes_clean)) {
  quotes_clean <- quotes_clean %>%
    group_by(Ticker) %>%
    dplyr::group_modify(~na_too_large_daily_changes(., too_large_daily_change_ratio)) %>%
    ungroup()
}
```
1. **Aussergewöhnlich hohe Preisbewegungen**  
Es liegt in der Natur der Sache, dass sich Aktienkurse verändern. Grosse Kurssprünge sind bei Aktien sehr grosser Unternehmen wie sie in dieser Arbeit betrachtet werden aber selten. Die Chance, dass das Verhältnis zwischen adjustiertem Preis des Vortages und adjustiertem Preis des aktuellen Tages (und vice versa) um mehr als einen Faktor `r too_large_daily_change_ratio` unterscheidet erachten wir als kleiner, als dass es sich dabei um einen Datenfehler handelt. Entsprechende Einträge werden deshalb entfernt.

```{r, filter-data-extremes, echo=FALSE}
if (!file.exists(file_quotes_clean)) {
  index_factor <- 100 / quotes_clean$Adjusted
  quotes_clean <- normalize_quotes(quotes_clean, base_col = "Adjusted", target_cols = c("Low", "High", "Close", "Open"))
  quotes_clean <- na_extremes(quotes_clean, col = c("Open", "Low", "High", "Close", "Adjusted"), tail = 0.01)
  quotes_clean <- mutate_at(quotes_clean, c("Open", "Low", "High", "Close", "Adjusted"),  ~. / index_factor)
}

if (!file.exists(file_quotes_clean)) {
  quotes_clean <- quotes_clean %>% select(-Adjusted)
  write_feather(quotes_clean, paste0("data/quotes_clean.feather"))
  quotes_clean <- read_feather(paste0("data/quotes_clean.feather"))
  quotes_clean_hash <- digest::digest(quotes_clean)
} else {
  quotes_clean <- read_feather(paste0("data/quotes_clean.feather"))
}
```
1. **Ausschluss von Extremwerten**  
Die der vorliegenden Analyse zugrundliegende Payoff-Funktion ist abhängig von den Preisbewegungen einer Aktie innerhalb des Tages (vgl. \@ref(forschungsfrage)). Speziell dabei ist, dass die Höhe der Bewegung nicht nur linear, sondern gar quadratisch Niederschlag findet. Dies führt dazu, dass die Analyse sehr sensitiv auf einzelne extreme Ausreisser reagiert. Hinzu kommt, dass es Ziel der Arbeit ist, Aussagen über Kursbewegungen innerhalb eines typischen Börsentages machen zu können. Eine Prognose von Werten an aussergewöhnlichen Tagen liegt ausserhalb des Geltungsbereiches der Analyse. Aus diesem Grund werden nach obigen Breinigungen für jede der fünf Preiskennzahlen die jeweils 1% extremsten Werte nach oben wie auch unten ausgeschlossen. Der Ausschluss erfolgt aufgrund der unterschiedlichen Preislevels der Aktien auf einer täglich auf den Eröffnungspreis indexierten Wert.

Zusammenfassend lässt sich festhalten, dass der rohe Datensatz aus `r nrow(quotes_raw)` Einträgen besteht. Davon weisen `r sum(rowSums(is.na(quotes_raw)) != 0)` Zeilen vor Bereinigung einen fehlenden Wert auf. Nach Bereinigung erhöht sich dieser Wert auf `r sum(rowSums(is.na(quotes_clean)) != 0)`. Für die Analyse bleiben somit `r sum(rowSums(is.na(quotes_clean)) == 0)` verwendbare Einträge.


### Normalisierung

Ein weiteres Problem, das sich beim Vergleich von Preisen verschiedener Aktien ergibt, ist deren unterschiedlichesw Preisniveau. Während eine Aktie bei USD 30 handelt, bewegt sich eine andere auf einem Niveau von USD 1000. Eine Vergleichbarkeit lässt sich dadurch herstellen, indem nicht absolue Preise, sondern relative Returns in der Analyse verwendet werden. Tatsächlich ist dies in der Payoff-Funktion (vgl. Gleichung \@ref(eq:payoff)) bereits grösstenteils sichergestellt. Das absolute Preisniveau fliesst allerdings auch in die Berechnung des Gamma Cash (vgl. Gleichung \@ref(eq:gamma_cash)) mit ein. Um auch hier eine Vergleichbarkeit der Werte sicherzustellen, wird bei allen nachfolgenden Analysen der Preis auf ein Niveau von 100 per adjustiertem Vortagesendkurs (dies entspricht dem letzten Neutralisierungszeitpunkt des Deltas) standardisiert.



```{r, knn-data-preparation, echo=FALSE}

# quotes_clean <- arrow::read_feather(paste0("data/steps/quotes_clean.feather"))
window <- 10
data_wide_10 <- quotes_clean %>%
  dplyr::group_by(Ticker) %>%
  dplyr::group_modify(~widen(., window = window, cols = c("Close", "Open", "Low", "High"), keep = c("Date", "Close", "Open", "Low", "High"))) %>%
  dplyr::ungroup() %>%
  dplyr::rename(Close_0 = "Close", Open_0 = "Open", Low_0 = "Low", High_0 = "High") %>%
  normalize_quotes("Close_1", setdiff(names(.), c("Ticker", "Date"))) %>%
  drop_na()

# sort columns nicely

rev_lags <- rev(seq_len(window))
data_wide_10 <- select(data_wide_10, c("Ticker", "Date"), paste0("Open_", rev_lags),  paste0("Low_", rev_lags), paste0("High_", rev_lags), paste0("Close_", rev_lags))

arrow::write_feather(data_wide_10, "data/steps/data_wide_10.feather")
# data_wide_10 <- arrow::read_feather("data/data_wide_10.feather")
# data_wide_10_hash <- digest::digest(data_wide_10)

rm(quotes_clean)

```



Sind all diese Schritte durchgeführt, lässt sich der Payoff berechnen. Da der Payoff - wie bereits erwähnt - quadratisch auf Preisveränderungen reagiert liegt, lohnt sich an dieser Stelle eine Untersuchung der Daten auf (zu) einflussreiche Beobachtungen. Ist der gesamte Payoff über alle Datensätze hinweg durch wenige Einträge dominiert könnten nachfolgende Analysen verfälscht werden. In diesem Fall besteht die Gefahr, dass insbesondere lernende Algorithmen insbesondere diese einzelnen Beobachtungen optimiert. Weniger einflussreiche aber häufiger realsierite Beobachtungen gingen darin unter. Dies ist inbesondere unter dem Gesichtspunkt von Relevanz, als dass vorliegende Analysen auf eine Prognose "normaler" Marktsituationen abzielt. 

Zur Veranschaulichung einflussreicher Beobachtungen zeigt Abbildung \@ref(fig:lorenz-curve) die Lorenzkurve des Payoffs über alle für in der Analyse berücksichtigten Datensätze. Hierfür werden die Payoffs der einzelnen Einträge in aufsteigender Weise sortiert. Deren kumulativer Anteil (Y-Achse) wird dem kumulativen Anteil der Datenpunkte (X-Achse) gegenübergestellt. Bei geneu gleichverteiltem Beitrag jedes einzlenen Eintrages zum Payoff würde eine Gerade mit Steiguzng 45° resultieren. Je stärker der einfluss einzelner Beobachtungen, desto stärker konvex die Kurve. 

```{r lorenz-curve, fig.cap='Lorenzkurve des Payoffs', fig.asp=0.6, fig.pos = '!H'}

if (!file.exists("data/data_wide_0_all.feather")) {
  window <- 0
  data_wide_0_all <- quotes_clean %>%
    dplyr::group_by(Ticker) %>%
    dplyr::group_modify(~widen(., window = window + 1, cols = c("Close"), keep = c("Date", "Close", "Open", "Low", "High"))) %>%
    dplyr::ungroup() %>%
    dplyr::rename(Close_0 = "Close") %>%
    normalize_quotes("Close_1", setdiff(names(.), c("Ticker", "Date")))
  
  data_wide_0_all <- drop_na(data_wide_0_all)
  arrow::write_feather(data_wide_0_all, "data/data_wide_0_all.feather")
  data_wide_0_all <- read_feather("data/data_wide_0_all.feather")
  # data_wide_0_all_hash <- digest(data_wide_0_all)
} else {
  data_wide_0_all <- read_feather("data/data_wide_0_all.feather")
}

set.seed(123456)
both_first <- c("buy", "sell")[sample(c(1, 2), nrow(data_wide_0_all), replace = TRUE)]
payoff <- calc_payoff_const_gamma(data_wide_0_all, both_first = both_first)

n_steps <- 100
payoff_sorted <- sort(payoff)
x <- seq(0, 1, length.out = n_steps)
y <- quantile(cumsum(payoff_sorted), seq(0, 1, length.out = n_steps)) / sum(payoff_sorted)

ggplot(tibble(x = x, y = y), aes(x = x, y = y)) + 
  geom_line() +
  scale_x_continuous(labels = scales::percent) + 
  scale_y_continuous(labels = scales::percent) + 
  geom_abline(linetype = "dashed") +
  ggtitle("Lorenzkurve des Payoffs") + 
  xlab("Kumulativer Anteil der Datenpunkte") + 
  ylab("Kumulativer Anteil des Payoffs") + 
  theme_bw()

```


Im vorliegenden Fall weist die Lorenzkurve eine gutsichtbare Konvexität auf. Zumal im Datensatz sowohl unterschiedliche Titel als auch ein langer Zeithorizont mit volatileren als auch ruhigeren Marktsituatioen repräsentiert ist, überrascht dieses Ergebnis nicht. Eine gewisse Vielfalt der Daten ist im Hinblick auf die Vorhersage von unterschiedlichen Kaufs- und Verkaufspreisen gar erwünscht. Sollen nochfolgende Algorithmen ja gerade versuchen, die unterschiedlichen Preisschwankungen zu prognostizieren. Andererseits zeigt die Darstellung auch, dass kein extremer Einfluss einzelner Einträge auszumachen ist. In diesen Fällen wäre die Kurve quasi auf dem unteren und rechten Rand der Grafik zu liegen gekommen. Es lässt sich damit der Schluss ziehen, dass die Bereinigung der Daten den erfolgreich war. Die bereinigten Daten zeigen sowohl die gewünschte Variabilität ohne dass dabei wenige Extremwerte das Ergebis verfälschen würden.
