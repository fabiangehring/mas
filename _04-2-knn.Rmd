## Nearest Neighbor Ansätze

```{r message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
suppressPackageStartupMessages({
  library(tidyverse)
  library(RANN)
  library(RANN.L1)
  library(digest)
  library(pbmcapply)
  library(parallel)
  library(arrow)
  library(knitr)
  library(kableExtra)
})

source("R/03-analysis.R")
Rcpp::sourceCpp('src/calc_payoff_per_title.cpp')


data_wide_10 <- arrow::read_feather("data/steps/data_wide_10.feather")

set.seed(123456)
train_idx <- sample(x = nrow(data_wide_10), size = floor(0.8 * nrow(data_wide_10)))
test_idx <- setdiff(seq_len(nrow(data_wide_10)), train_idx)

both_first <- c("buy", "sell")[sample(c(1, 2), nrow(data_wide_10), replace = TRUE)]


```


Die ökomische Theorie scheint keinen offensichtlichen Grund zu liefern, weshalb die Volatilität des aktuellen Tages in genau 2 (oder x) Gruppen eingeteilt werden sollte. Auf der andern Seite haben bisherige Analysen gezeigt, dass die aktuelle Volatilität ein erklärender Faktor für die aktuelle Volatilität sein kann. Im vorliegenden Kapitel soll dieser Gedanke weiter verfolgt werden. 


### Distanzmasse

Die Idee des Ansatzes dieses Kapitels besteht darin, nicht lediglich x vordefinierte Gruppen für symmetrische Abweichungen zu finden, viel mehr soll versucht werden, ähnliche Kursverläufe wie den aktuellen in der Vergangenheit zu finden und individuell darauf zu reagieren. Die zugrund liegende Hypothese ist dabei, dass bei ausreichender Historie in der Vergangenheit ähnliche Muster erkannt und daraus Rückschlüsse auf die Zukunft (genauer: die Kursentwicklung des aktuellen Tages) gemacht werden können. 

Da es sehr unwahrscheinlich ist, die genau gleichen Kursverläufe in der Histore wiederzufinden, muss ein Distanzmass definiert werden, welches die Nähe der Kursverläufe quantifiziert. Um diese zu berechnen formulieren wir für jeden Eintrag $i$ den bisherigen Kursverlauf als Vektor $hist$. 

$$ hist_{i, t} = (Open_{i, t}, High_{i, t}, Low_{j, t+1}, Close_{i, t+1}, Open_{i, t+1}, ..., Close_{i, 1}, Open_{i, 0}) $$

Der zweite Index gibt dabei an, wieviele Tage der Vergangenheit mit einbezogen werden. Ein Index von 0 bezieht sich auf den zu prognostizierenden, aktuellen Tag. Um die Ähnlichkeit zweier Einträge $i$ und $j$ zu berechnen, bieten sich 2 Distanzmasse an:

1) Die Manhattan Distanz (auch L1-Norm)  
$$ 
\Vert hist_{i,t} - hist_{j,t}\rVert_1 =  \lvert Open_{i,t} - Open_{j,t} \rvert + \lvert High_{i,t} - High_{j,t} \rvert + \ldots + \lvert Open_{j,0} - Open_{j,0} \rvert
$$


2) Die Euklidische Distanz (auch L2-Norm)
$$ 
\Vert hist_{i,t} - hist_{j,t}\rVert_2  = \sqrt{(Open_{i,t} - Open_{j,t})^2 + (High_{i,t} - High_{j,t})^2 + \ldots + (Open_{j,0} - Open_{j,0})^2}
$$

Die Ergebnisse beider Masse sollen anschliessend miteinander verglichen werden.

### Setup und Berechnungsdauer

Mithilfe obiger Distanzmasse lassen sich für jeden Kursverlauf, die k ähnlichsten Kursverläufe der Vergangenheit ermitteln. Zuvor seien an dieser Stelle aber eingige Überlegungen zur Berechnungskomplexität des Problems gemacht: Der vorliegende bereinigte Datensatz weist `r format(nrow(data_wide_10), big.mark = "'")` tägliche Kursverläufe auf. Soll jeder Kursverlauf mit jedem andern verglichen werden, so ergeben sich bei einem Brute-Force Ansatz `r format(nrow(data_wide_10), big.mark = "'")`^2 Distanzberechungen. Unter Berücksichtigung der Tatsache, dass das Problem symmetrisch ist und jeder Kursverlauf nicht mit sich selbst verglichen werden muss, halbiert sich die Komplexität zwar um mehr als die Hälfte, bleibt aber so gross, dass es zum Zeitpunkt des Schreibens dieser Arbeit nicht innerhalb weniger Sekunden oder Minuten auf einem handelsüblichen Heim-Computer berechnet werden kann.

Die für die Nearest Neighbors Suche eingesetzten Bibliotheken greifen daher typischerweise auf sophistiziertere Vorgehensweisen zurück. Eine davon sieht die Verwendung von Multidimensionalen Suchbäumen (Search Trees, auch Kd-Trees) vor. Diese gehen auf eine Idee von @bentley zurück. Sie basiert darauf, dass jede Dimension in 2 Bereiche aufgeteilt wird (z.B. beim Median). Dadurch wird der Raum in viele kleinere Sub-Räume aufgeteilt. Der Algorithmus macht sich danach zu Nutze, dass er nicht den ganzen Raum absuchen muss. Beginnend im aktuellen Sub-Raum werden schrittweise alle benachtbarten Räum abgesucht, bis die geforderte Anzahl nächster Nachbarn gefunden ist. Allerdings benötigt dabei sowohl der Aufbau des Baumes wie auch die Suche im Baum Berechnungszeit. 

Die Anzahl der Dimensionen beeinflusst dabei die benötigte Zeit erheblich. Im vorliegenden Fall liegt diese bei Historie von 10 Tagen à 4 Werten bei 41, wenn zusätzlich auch der (bekannte) Eröffnungspreis des aktuellen Tages mit einbezogen wird. Tests ergaben, dass die dafür benötigte Rechenzeit zu hoch war. Die zu verwendende Zeitperiode wurde daher auf 3 Tage beschränkt. Dies resultiert in einer deutlichen Reduktion der Suchdimensionen auf 13.

Der vorliegende Fall unterscheidet sich von andern Nearest Neighbor Problemen ferner dadurch, als dass für jeden Eintrag lediglich Kursverläufe der Vergangenheit bertrachtet werden sollten. Im Hinblick auf die Suchstrukur bedeutet dies, dass der KD-Tree nicht nur einmalig aufgebaut und danach für alle Verläufe auf Nachbarn durchsucht werden kann. Vielmehr muss der KD-Tree für jedes Datum mit allen Kursverläufen vor diesem Datum neu aufgebaut werden.[^Es sind auch Mischlösungen denkbar, bei denen nur alle x Daten der Tree neu aufgebaut wird und dafür mehr Nachbarn ermittelt werden, die im Nachgang um nicht vorher realsierte Verläufe gefiltert würden. Dies hat aber das Problem, dass a) mehr Nachbarn ermittelt werden müssen und b) nicht 100% sichergestellt ist, dass die Anzahl "Reservenaachbarn" ausreichen.] Dieser spezielle Setup kommt einer Brute-Force Methode ihrerseits wieder entgegen, da aufgrund des Datums-Filter nicht stets alle Einträge durchsucht werden müssten.

Beiden Ansätzen gemein ist hingegen, dass sie sich sehr gut parallelisieren lassen und Bibliotheken zur Verfügung stehen, welche diese Methoden effizient implementieren. Wir entscheiden uns im vorliegenden Fall für eine KD-Tree Implementation ohne Approximation. Auf einem Rechner mit 40 Cores (20 physisch, 20 Hyperthreading Cores) dauert die Berechnung von 50 Nachbarn ca. 2  Stunden im Falle des euklidischen Distanzmasses und ca. 3 Stunden im Falle der Manhattan Distanz. Dieser Hohe Anspruch eines KNN-Vorgehens an die Rechenleistung sollte bei der späteren Abwägung verschiedener Algorithmen mit berücksichtigt werden. Erwähnt sei an dieser Stelle aber auch, dass eine allfälligen Anwendung der Methode später nur wenige Titel (resp. nur diejenigen des aktuellen Tages) umfasst. Dies ist auch unter der Verwendung einer KNN-Methode in wenigen Sekunden möglich. 


### Bestimmung Kaufs- und Verkaufspreise


```{r, knn-calculation, echo=FALSE, results='hide', warning=FALSE}

data_wide_10 <- arrow::read_feather("data/steps/data_wide_10.feather")
data_knn_3 <- select(data_wide_10, c("Date", levels(interaction(c("Open_", "Low_", "High_", "Close_"), 1:3, sep="")), "Open_0")) %>%
  arrange(Date)

nn_3_eucl_path <- c("data/knn/nn_3_eucl_idx.feather", "data/knn/nn_3_eucl_dists.feather")
if (!all(map_lgl(nn_3_eucl_path, ~file.exists(.)))) {
  
  nn_3_eucl <- find_nn(data = data_knn_3, distance = "euclidean", k = 50, mc.cores = ceiling(detectCores() * 0.51))
  write_feather(as_tibble(nn_3_eucl$nn.idx), nn_3_eucl_path[1])
  write_feather(as_tibble(nn_3_eucl$nn.dists), nn_3_eucl_path[2])
  
  nn_3_eucl_idx <- read_feather(nn_3_eucl_path[1])
  nn_3_eucl_idx_hash <- digest(nn_3_eucl$idx)
  
  nn_3_eucl_dists <- read_feather(nn_3_eucl_path[2])
  nn_3_eucl_dists_hash <- digest(nn_3_eucl_dists)
  
} else {
  nn_3_eucl_idx <- read_feather(nn_3_eucl_path[1])
  nn_3_eucl_dists <- read_feather(nn_3_eucl_path[2])
}

nn_3_manh_path <- c("data/knn/nn_3_manh_idx.feather", "data/knn/nn_3_manh_dists.feather")
if (!all(map_lgl(nn_3_manh_path, ~file.exists(.)))) {
  
  nn_3_manh <- find_nn(data = data_knn_3, distance = "manhattan", k = 50, mc.cores = ceiling(detectCores() * 0.51))
  write_feather(as_tibble(nn_3_manh$nn.idx), nn_3_manh_path[1])
  write_feather(as_tibble(nn_3_manh$nn.dists), nn_3_manh_path[2])
  
  nn_3_manh$idx <- read_feather(nn_3_manh_path[1])
  nn_3_manh_idx_hash <- digest(nn_3_manh$idx)
  
  nn_3_manh$dists <- read_feather(nn_3_manh_path[2])
  nn_3_manh_dists_hash <- digest(nn_3_manh$dists)
  
} else {
  nn_3_manh_idx <- read_feather(nn_3_manh_path[1])
  nn_3_manh_dists <- read_feather(nn_3_manh_path[2])
}

```


Sind die Nachbarn eines Eintrages ermittelt lässt sich eine Prognose für den weiteren Kursverlauf des aktuellen Eintrages ableiten. Dazu wird aus den bekannten Kursverläufen der Nachbarn mittels geeigneter Metrik ein Prognosewert für den Folgetag abgeleitet. Abbildung \@ref(fig:example-nearest-neighbor-outlook) illustriert dieses Vorgehen anhand eines Beispiels. Die Grafiken zeigen die Kursverläufe des aktuellen Eintrages (rot) sowie diejenigen der nächsten 5 Nachbarn für den aktuellen zu prognostizierenden Tag (t) sowie die 3 jeweils vorangegangenen Handelstage (t-1, ... , t-3). Die gestrichelte rote Linie zeigt den auf Basis der Nachbarn prognostizierten Wert.

``` {r, example-nearest-neighbor-outlook, fig.cap='Exemplarische Kursprognose auf Basis nächster Nachhbarn', fig.asp=1, fig.pos = '!H',  echo=FALSE, message=FALSE, warning=FALSE}
k <- 5
curr_idx <- 432100


cols_3_incl_0 <- as.vector(outer(c("Open_", "Low_", "High_", "Close_"), 0:3, FUN = paste0))

data_knn_3_incl_0 <- select(data_wide_10, c("Date", cols_3_incl_0)) %>%
  arrange(Date)


data_wide_curr <- data_knn_3_incl_0[curr_idx,]
data_wide_nn_eucl <- data_knn_3_incl_0[unlist(nn_3_eucl_idx[curr_idx, ], use.names = FALSE)[seq_len(k)], ]

data_wide_nn_manh <- data_knn_3_incl_0[unlist(nn_3_manh_idx[curr_idx, ], use.names = FALSE)[seq_len(k)], ]


# data_wide_nn - map_df(seq_len(k), ~data_wide_curr)

# plot_nn(data_wide_curr = data_wide_curr, data_wide_nn = data_wide_nn_eucl)
plot_nn(data_wide_curr = data_wide_curr, data_wide_nn = data_wide_nn_manh)

```

Werden die prognostizierten Werte für Höchst- und Tiefstpreise als Kaufs- resp. Verkaufsschranken gewählt, lässt sich der Payoff berechnen und mit demjenigen der Referenzstartegie vergleichen. Wie bereits ausgeführt werden bei der Suche nach Nachbarn nur Einträge der Vergangenheit berücksichtigt. Die Menge an verfügbaren Vergleichsverläufen nimmt damit mit fortlaufender Zeit zu. Stellt man den Payoff-Vergleich für das Training Set über die Zeit dar (vgl. Abbildung \@ref(fig:knn-learning-history)), wird der Lerneffekt der Methode sichtbar. Insbesondere 3 Eigenschaften der Kurve lassen sich ausmachen:

1. Hohe Volatilität des Vergleichsfaktor zu Beginn
1. Steigende Faktorhöhe mit fortlaufender Zeitdauer
1. Abflachung im Laufe der Zeit auf ein stabiles Level

Beide Eigenschaften decken sich mit der Intuition. Da zu Beginn der Datenreihe nur sehr wenige Vergleichsverläufe zur Verfügung stehen, reagiert die Kurve sehr sensitiv und schlägt entsprechend aus. Dies glättet sich im Laufe der Zeit mit dem Vorhandensein von mehr Vergleichsmöglichkeiten. Die zweite Eiegenschaft des steigenden Faktors zeigt, dass der erhoffte Lerneffekt einzutreten scheint. Tatsächlich scheinen ähnliche Kursverläufe in der Vergangenheit zukünftige Entwicklungen teilweise erklären zu können. Dies deckt sich mit der Erkenntnis des vergangenen Kapitels. Anders als zuvor kann dieser Lernmechanismus hier aber sehr individuell und nicht beschränkt auf 2 Gruppen erfolgen. Die dritte Eigenschaft zeigt, dass dieser Lerneffekt  nach gewisser Zeit gesättigt scheint.

``` {r, knn-learning-history, fig.cap='Entwicklung der Modellperformance über die Zeit', fig.asp=1, fig.pos = '!H', echo=FALSE, message=FALSE, warning=FALSE}
data_train <- select(data_knn_3_incl_0, Date, Close_1, Open_0, Low_0, High_0, Close_0)[train_idx, ]

hist_pred_train_path <- "data/knn/hist_pred_train.feather"
if (!file.exists(hist_pred_train_path)) {
  hist_pred_train <- pred_nn(select(data_train, Open_0, Low_0, High_0), as.matrix(nn_3_eucl_idx[train_idx, seq_len(k)]))
  write_feather(hist_pred_train, hist_pred_train_path)
} else {
  hist_pred_train <- read_feather(hist_pred_train_path)
}

plot_ratio_history(data_train %>% rename(Low = "Low_0", High = "High_0"), data_pred = hist_pred_train, both_first = both_first[train_idx], use_spread = TRUE)

```

Während sich bisherige Ausführungen auf die Analyse mit 5 Nachbarn ermittelt auf Basis der euklidischen Distanz beziehen, sind bei der Ermittlung der Prognosewerte auch andere Parametrisierungen denkbar. Neben der Unterscheidung des Distanzmasses sind dies insbesondere die Anzahl der zu berücksichtigenden Nachbarn und die Metrik zur Prognoseermittlung.

- **Anzahl Nachbarn:**  
In der vorliegenden Arbeit werden die 50 nächsten Nachbarn jedes Eintrages ermittelt. Einmal ermittelt lassen sich davon auch weniger verwenden. Damit kann sehr einfach der Einfluss der Anzahl Nachbarn auf die Prognosequalität ermittelt werden. Denkbar sind dabei grundsätzlich unterschiedliche Ergebnisse. So lässt sich argumentieren, dass bei der Verwendung weniger Nachbarn auch diejenigen mit grösster Ähnlichkeit verwendet werden. Insbesondere bei Marktbewegungen die relativ selten sind, könnten fernere, weniger gut passende Nachbarn das Ergebnis hier nicht verzerren. Umgekehrt lässt sich argumentieren, dass bei häufigeren Marktsituationen die Berücksichtigung und Mittelung von mehr Nachbarn zu einem unverzerrteren Ergebnis führen könnte. Schliesslich könnte sich als drittes Ergebnis auch eine Konfiguration mit "mittlerer" Anzahl Nachbarn beähren, wenn beide vorherhigen Argumentationen verschmolzen werden. Aus diesem Grund analysiert und vergleicht die vorliegende Arbeit die Ergebnisse bei der Verwendung von 5, 20 und 50 Nachbarn.

- **Metrik der Prognoseermittlung:**  
Eine erste Möglichkeit zur Prognose von Tiefst-, Höchst- und Schlusskurs besteht darin, den Mittelwert der jeweiligen Kursfortsetzungen der Nachbarn zu wählen. Alternativ zur einfachen Mittelwertbildung sind auch andere Verfahren denkbar. Beispielsweise wäre auch die Berücksichtigung der Distanz als Gewichtungsfaktor denkbar. Aus Gründen der Einfachheit verzichten wir an dieser Stelle allerdings darauf und verwenden neben dem gleichgewichteten Mittelwert den Median als zweites Prognosemass. Dieses reagiert weniger sensitiv auf Ausreisser innerhalb der Nachbarn.  

Die zweite hier betrachtete Möglichkeit besteht darin, die Kaufs- und Verkaufspreise nicht direkt, sondern als Abweichung vom Eröffnungskurs zu modellieren. Hierzu wird für jeden Nachbarn die Differenz von Eröffnungs- und Tiefstpreis resp. Eröffnungs- und Höchstpreis berechnet. Die entsprechende Metrik (Mittelwert oder Median) wird dann auf diese Werte angewandt und auf den tatsächlichen Eröffnungspreis des zu analysierenden Titels appliziert. Dies hat den Vorteil, dass der bekannte Eröffnungspreis keiner Unsicherheit mehr unterliegt.

Tabelle \@ref(knn-payoff-factors) stellt die Ergebnisse aller Parametrisierungen für des Testingset einander gegenüber. Es zeigt sich, dass eine direkte Prognose der Kaufs- und Verkaufspreise derjenigen einer Prognose der Veränderung im Vergleich zum Eröffnungspreis klar unterlegen ist. Bezüglich Distanzmass lässt sich kein eindeutiger Gewinner feststellen, beide Masse weisen ähnliche Performance aus. Ähnliches gilt für die Anzahl der berücksichtigten Nachbarn.

Allen Ergebnissen gemein ist hingegen, dass sie der einfachen Strategie des Vorkapitels nicht überlegen sind. Im Gegenteil fallen die Payoffs im Vergleich trotz deutlich höherem Berechnungsaufwand in der Tendenz schlechter aus, wenn auch der Payoff der Referenzstrategie weiter deutlich geschlagen wird. Die Hoffnung, dass mit der Individualisierung der einzelnen Einträge mehr kursrelevante Information extrahiert werden kann, bestätigt sich nicht. Ein Grund könnte in der durch die Berechnungskomplexität beschränkte Begrenzung auf ein Zeitfenster von 3 vorangegangenen Tagen sein.

```{r} 

k_levels <- c(5, 20, 50)
aggr_levels <- c("mean", "median")
factor_config <- expand_grid(aggr = aggr_levels, k = k_levels) %>% as.list()
calc_nn_factors <- function(data, factor_config, nn_idx, use_spread, both_first, test_idx, mc.cores = 3) {
  
  factor_names <- paste("payoff_factor", factor_config$aggr, factor_config$k, sep = "_")
  pbmclapply(transpose(factor_config), function(curr_config, nn_idx, use_spread) {
    if (curr_config$aggr == "mean") {
      fct <- function(x) mean(x, na.rm = TRUE)
    } else if (curr_config$aggr == "median") {
      fct <- function(x) median(x, na.rm = TRUE)
    } else {
      stop("Unexpected aggregation")
    }
    
    calc_nn_payoff_factor(
      data = select(data, Close_1, Open_0, Low_0, High_0, Close_0)[test_idx, ],
      nn_idx = nn_idx[test_idx, seq_len(curr_config$k)],
      both_first = both_first[test_idx],
      fct = fct,
      use_spread = use_spread
    )
  }, use_spread = use_spread, nn_idx = nn_idx, mc.cores = mc.cores) %>% set_names(factor_names)
}

factor_table_paths <- c("data/knn/eucl_table.feather", "data/knn/manh_table.feather")
if (all(map_lgl(factor_table_paths, ~!file.exists(.)))) {
  
  mc.cores <- ceiling(0.51 * detectCores())
  
  nn_factors_eucl <- calc_nn_factors(data_knn_3_incl_0, factor_config, nn_3_eucl_idx, FALSE, both_first, test_idx, mc.cores)
  nn_factors_eucl_spread <- calc_nn_factors(data_knn_3_incl_0, factor_config, nn_3_eucl_idx, TRUE, both_first, test_idx, mc.cores)
  nn_factors_manh <- calc_nn_factors(data_knn_3_incl_0, factor_config, nn_3_manh_idx, FALSE, both_first, test_idx, mc.cores)
  nn_factors_manh_spread <- calc_nn_factors(data_knn_3_incl_0, factor_config, nn_3_manh_idx, TRUE, both_first, test_idx, mc.cores)
  
  eucl_table <- tibble(
    `Anzahl Nachbarn` = k_levels,
    `Mittelwert_Direkt` = unlist(nn_factors_eucl[seq_along(k_levels)], use.names = FALSE),
    `Mittelwert_Addon` = unlist(nn_factors_eucl_spread[seq_along(k_levels)], use.names = FALSE),
    `Median_Direkt` = unlist(nn_factors_eucl[length(k_levels) + seq_along(k_levels)], use.names = FALSE),
    `Median_Addon` =  unlist(nn_factors_eucl_spread[length(k_levels) + seq_along(k_levels)], use.names = FALSE)
  )
  write_feather(eucl_table, factor_table_paths[1])
  
  manh_table <- tibble(
    `Anzahl Nachbarn` = k_levels,
    `Mittelwert_Direkt` = unlist(nn_factors_manh[seq_along(k_levels)], use.names = FALSE),
    `Mittelwert_Addon` = unlist(nn_factors_manh_spread[seq_along(k_levels)], use.names = FALSE),
    `Median_Direkt` = unlist(nn_factors_manh[length(k_levels) + seq_along(k_levels)], use.names = FALSE),
    `Median_Addon` =  unlist(nn_factors_manh_spread[length(k_levels) + seq_along(k_levels)], use.names = FALSE)
  )
  write_feather(manh_table, factor_table_paths[2])
  
  
} else {
  eucl_table <- read_feather(factor_table_paths[1])
  manh_table <- read_feather(factor_table_paths[2])
}


```


```{r}
kable(eucl_table, col.names = c("", "Mittelwert", "Median", "Mittelwert", "Median"), digits = 3) %>%
  add_header_above(c(" ", "Euklidisch" = 2, "Manhattan" = 2)) %>%
  column_spec(1, bold = TRUE)
# kable_styling(latex_options = c("repeat_header"))
```


```{r, echo=FALSE, results='hide'}
rm(list = ls())
gc()
```



