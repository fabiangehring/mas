## Neuronale Netzwerke


```{r, echo=FALSE, results="hide"}
library(tidyverse)
library(magrittr)
library(keras)
library(arrow)
library(pbmcapply)
Rcpp::sourceCpp('src/calc_payoff_per_title.cpp')
source("R/03-analysis.R")

# Rcpp::sourceCpp('src/find_best_buy_sell_ind.cpp')
# Rcpp::sourceCpp('src/find_best_buy_sell_dep.cpp')

data_wide_10 <- arrow::read_feather("data/steps/data_wide_10.feather")

set.seed(123456)
train_idx <- sample(x = nrow(data_wide_10), size = floor(0.8 * nrow(data_wide_10)))
test_idx <- setdiff(seq_len(nrow(data_wide_10)), train_idx)
both_first <- c("buy", "sell")[sample(c(1, 2), nrow(data_wide_10), replace = TRUE)]

sample_idx <- seq_len(500)

```


Als dritte Analysemethode sollen im vorliegenden Kapitel neuronale Netzwerke verwendet werden. Diese Methoden erlauben den Umgang mit sehr grossen Datenmengen, weshalb die Beschränkung auf ein sehr kurzes Zeitfenster hier nicht nötig ist. Für die vorliegenden Analysen verwenden wir eine einfache Architektur mehrerer aufeinanderfolgender Dense Layer. Das Ziel der Analyse bleibt hingegen das gleiche wie in den Kapiteln zuvor: Es soll basierend auf vergangenen Kursverläufen möglich optimale Kaufs- und Verkaufskurse für den laufenden Tag prognositiert werden. Wie bereits früher ausgeführt ist der resultierende Payoff dabei abhängig von der Höhe der Preisbewegung und der der Wahrscheinlichkeit der Ausführung. Da der Payoff weiter in quadratischer Form von der Höhe der Kursbewegung abhängt haben frühere Überlegungen bereits gezeigt, dass es allenfalls lukrativ sein könnte eher breite Preisschranken zu setzen, welche zwar weniger oft erreicht werden, in diesem Fall aber einen umso höheren Payoff abwerfen. Als erste der betrachteten Methoden sollen nachfolgend daher nicht nur der kommende Höchst- und Tiefstwerte pronostiziert werden, sondern auch deren Verteilung. Das Vorgehen zum Finden optimaler Kaufs- und Verkaufspreise wird dadurch zweistufig. In einem ersten Schritt wird die Verteilung der prognostizierten Tiefst-, Höchst- und Schlusspreise geschätzt.


### Diskretisierung
Zur Modellierung der Verteilung werden die zu prognostizierenden zukünftigen Preis (Tiefst-, Höchst- und Schlusspreis) in einem ersten Schritt diskretisiert. Für jeden Preisbereich [Bucket] lässt sich mit dem Modell später eine Eintretenswahrscheinlichkeit berechnen. Wichtig für die ein solches Klassifikationsproblem ist dabei, dass die einzelnen Buckets möglichst gleich viele Werte enthalten. Ist dies nicht der Fall bestehen für den Algorithmus Anreize, ein Element dem grössten Bucket zuzuordnen. Im vorliegenden Fall werden dazu zwei Strategien verfolgt:

**1. Unabhängige Modellierung von Low, High und Close Preisen**  
Eine erste Möglichkeit besteht darin, die Diskretisierung von Tiefts-, Höchst- und Schlusspreisen unabhängig voneinander zu gestalten. Dies hat den Nachteil, dass allenfalls wertvolle Information verloren geht. Dies zeigt sich inbesondere auch dadurch, dass Bucketkombinationen enstehen, welche in Realität nicht möglich sind. Beispielsweise sind dies Kombinationen, bei denen der Tiefstpreis höher vorausgesagt wird als der Tiefstpreis. Gelöst wird dieses Problem in der vorliegenden Analyse so, dass für solche Fälle keine Kaufs- und Verkaufspreise gestellt werden. All dies Fälle werden damit gleich wie im Referenzfall behandelt - namentlich wird damit aufgelaufenes Delta erst zum Tagesendkurs ausgeglichen. Der Vorteil dieses Vorgehens liegt darin, dass die Ermittlung gleich grosser Buckets sehr einfach gelingt. Zudem lässt sich das Problem unter der Annahme der Unabhängigkeit in 3 kleinere Klassifikationsprobleme aufteilen. Werden beispielsweise sowohl Tiefts-, Höchst- und Schlusspreise mit jeweils 30 Buckets modelliert, resultiert dies in 3 Klassifikationsproblemen mit 30 Klassen. Kombiniert man diese, resultieren $27'000 \ (= 30^3)$ Preisszenarien, wobei sich die Wahrscheinlichkeit für jedes diese Szeanrien als Multipliaktion der einzelnen Preiswahrscheinlichkeiten ermitteln lässt.

**2. Abhängige Modellierung von Low, High und Close Preisen**
Eine zweite Möglichkeit besteht darin, die Diskretisierung ohne Annahme der Unabhängigkeit der einzelnen Tagespreise zu gestalten. Die Diskretisierung unter dem Ziel möglich gleich grosser Buckets gestaltet sich dabei etwas schwieriger. Wir verwenden dazu den Ansatz, welcher zuerst die Tiefstpreise in gleich grosse Buckets aufteilt. Für jedes dieser Bucket werden danach die Höchstkurse in gleich grosse Buckets aufgeteilt. Die resultierenden Buckets werden danach wiederum in möglichst gleich grosse Buckets bezüglich Schlusspreise aufgeteilt. Anders als im Unabhängigen Fall sind die Bucket-Grenzen damit nicht immer gleich. Auch das Klassifikationsproblem lässt sich damit nicht mehr auf kleinere Modelle aufteilen. Im obogen Beispiel müssen damit alle 27'000 Szeanrien auf einmal bearbeitet werden. Dies erhöht die Komplexität der Bearbeitung. Der Vorteil dieser Methode liegt darin, dass eine Abhängikeit der Preise ökonomisch plausibler ist und auch unmögliche Szenarien nicht mehr vorkommen.


### Architektur
Eine zweite Fragestellung die sich bei der Verwebdung von neuronalen Netzen ergibt, ist diejenige nach der geeigneten Architektur. Neuronale Netze haben sich inbesondere im Bereich der Computer Vision - beispielsweise zur Klassifikation des Bildinhaltes - als sehr leistungsfähig herausgestellt. Diese Probleme zeichenen sich dadurch aus, dass mit der Verwendung von Pixel-Daten sehr grosse Datenmengen zur Verfügung stehen und verarbeitet werden müssen. Im Vorliegenden Fall sind die Datenmengen in Relation zu Bilddaten bedeutend kleiner. Die positiven Erfahrungen, welche bei der Bilderkennung mit Convolutional Layers gemacht wurden lassen sich beim vorliegenden Sachverahlt auch nicht direkt übertragem. Aus diesem Grund und aus Überlegungen der Einfachheit entscheiden wir uns daher für eine einfache Architektur mit 2 hidden Dense Layers mit Grösse 512. Diverse Tests zur Erhöhung der Anzahl Knoten oder Gestskltung des Output-Layer dem Beifügen weiterer Layer haben zu keinen wesentlichen Verbesserungen geführt. Neben der Anzahl Layers stelltsich zudem die Frage nach der geeigneten Abbildung des Output Layers. Hierbei wurden zwei Vorgehensweisen untersucht.

**1. Klassische Klassifikation**
Eine erste Möglichkeit besteht darin, die Ordinalität der Klassen zu vernachlässigen. Eine solche Vernachlässigung macht insbesondere dann Sinn, wenn die einzlenen Klassen in keiner Abhängigkeit zueinander stehen. Wiederum bietet sich dabei der Vergleich mit der Bildklassifikation an - beispielsweise der Klassifikation des Bildes einer Katze. Es lässt sich je nach Verwendungszweck argumentieren, dass es bei einer Fehlklassifikation keine Rolle spielt, ob das Bild als Auto oder als Apfel erkannt wurde. Keines der beiden ist weniger oder mehr falsch, beide sind einfach falsch. Für diese Art der Klassifikation bietet sich eine Verlustfunktion der Art "Categorical Crossentropy" mit Aktivierung "softmax" des Outputlayers an. 

**2. Ordinale Klassifikation**
Im vorliegenden Fall - insbesondere bei unabhängiger Klassifikation von Tiefst-, Höchst- und Schlusspreisen - lassen sich die Buckets in eine logische Reihenfolge bringen. Wird ein Wert von 100 anstatt Klasse 10 fälschlicherweise Klasse 1 zugeordnet, so scheint dieser Fehler grösser als wenn die Fehlklassifikation in Klasse 9 erfolgt wäre. Eine Möglichkeit, diese Art von Klassifikationen in neuronalen Netzen zu modellieren zeigen @frank_hall. Grob besteht die Idee darin, nicht die Wahrscheinlichkeit der aktuellen Klasse zu modellieren, sondern die kumulierte Wahrscheinlichkeit der aktuellen und der darunterliegenden. Die Wahrscheinlichkeit der Zugehörigkeit zu einer spezifischen Klasse lässt sich dann einfach als Differnz der kumulierten Wahrscheinlichkeiten benachbater Klassen ableiten. Zu berücksichtigen gilt es hierbei, dass die kumulative Wahrscheinlichkeit nicht sinken darf, dies durch das geschätzte Modell aber nicht unbedingt garantiert ist. In der praktischen Umsetzung unterscheidet sich die Architektur zur Schätzung dieser Art von Modellen nicht gross von derjenigen der klassischen Klassifikation. Die Unterscheidungen beziehen sich auf eine andere Verlustfunktion (Binary Crossentropy), andere Aktivierung (Sigmoid) des Output Layers sowie eine etwas anderen Kodierung der Labels.


### Zielgrösse

### Ergebnisse

Alle der nachfolgenden Modelle wurden mit 30 Buckets im Falle unabhängig modellierter Preise, respektive 27'000 Preisszenarien im Falle abhängiger Preise gerechnet. Ebenfalls allen Modellen gemein ist, dass eine Batch-Grösse von 512 und ein Training über 10 Epochen gewählt wurde. Ferner werden als Features alle 4 Preiskennzahlen der letzten 10 Tage plus der Eröffnungskurs des aktuellen Tages verwendet. Jedes dieser Features wird vor dem Training skaliert.[^Im Detail erfolgt die Skalierung auf Basis des Mittelwertes und Standardabweichung jedes Features des Trainingsets.] Das eigentliche Taring erfolgt auf 80% des Trainingsets, 20% der Trainingsdaten dienen der Validierung.

#### Unabhängige Modelle

```{r}
n_groups_per_col <- 30

data <- select(data_wide_10, c(as.vector(outer(c("Open", "High", "Low", "Close"), 10:1, function(x, y) paste(x, y, sep = "_"))), "Open_0"))
scale <- map_dbl(data[train_idx, ], sd)
center <- map_dbl(data[train_idx, ], mean)

x <- scale(data, center, scale)
rm(data)

calc_mid_prices <- function(lower, upper) {
  out <- (lower + upper) / 2
  out[!is.finite(out)] <- if_else(is.finite(lower[!is.finite(out)]), lower[!is.finite(out)], upper[!is.finite(out)])
  return(out)
}
```


```{r}
ind_categorical <-  NULL
ind_categorical$discretization <- list(
  low = multivariate_discretization(data_wide_10, train_idx, test_idx, "Low_0", n_groups_per_col),
  high = multivariate_discretization(data_wide_10, train_idx, test_idx, "High_0", n_groups_per_col),
  close = multivariate_discretization(data_wide_10, train_idx, test_idx, "Close_0", n_groups_per_col)
)

y_low <- ind_categorical$discretization$low$groups
y_high <- ind_categorical$discretization$high$groups
y_close <- ind_categorical$discretization$close$groups

fit_model_cat <- function(x, y, epochs = 10, n_classes = 30, callbacks = NULL) {
  model <- keras_model_sequential() 
  model %>% 
    layer_dense(units = 512, activation = 'relu', input_shape = ncol(data)) %>% 
    layer_dense(units = 512, activation = 'relu') %>%
    layer_dense(units = n_classes, activation = 'softmax')
  
  model %>% compile(
    loss = 'sparse_categorical_crossentropy',
    optimizer = optimizer_adam(),
    metrics = c('accuracy')
  )
  
  history <- model %>% fit(
    x = x, 
    y = y, 
    epochs = epochs, batch_size = 512, 
    validation_split = 0.2,
    callbacks = callbacks
  )
  list(model = model, history = history)
}

model_ind_low_path  <- "data/models/ind/categorical/2_dense_512/model_low.h5"
history_ind_low_path  <- "data/models/ind/categorical/2_dense_512/history_low.rds"
pred_ind_low_path  <- "data/models/ind/categorical/2_dense_512/pred_low.rds"
if (file.exists(model_ind_low_path) && file.exists(pred_ind_low_path) && file.exists(history_ind_low_path)) {
  model_cat_low <- load_model_hdf5(model_ind_low_path)
  history_cat_low <- readRDS(history_ind_low_path)
  pred_cat_low <- readRDS(pred_ind_low_path)
} else {
  cp_callback_low <- callback_model_checkpoint(
    filepath = file.path("data/models/ind/categorical/2_dense_512", "low.{epoch:02d}-{val_loss:.2f}.hdf5")
  )
  model_cat_low <- fit_model_cat(x[train_idx, ], y_low[train_idx], 30, callbacks = list(cp_callback_low))
  save_model_hdf5(model_cat_low$model, model_ind_low_path)
  saveRDS(model_cat_low$history, history_ind_low_path)
  pred_cat_low <- predict_proba(model_cat_low$model, x[test_idx, ], batch_size = 512)
  saveRDS(pred_cat_low, pred_ind_low_path)
}

model_ind_high_path  <- "data/models/ind/categorical/2_dense_512/model_high.h5"
history_ind_high_path  <- "data/models/ind/categorical/2_dense_512/history_high.rds"
pred_ind_high_path  <- "data/models/ind/categorical/2_dense_512/pred_high.rds"
if (file.exists(model_ind_high_path) && file.exists(pred_ind_high_path) && file.exists(history_ind_low_path)) {
  model_cat_high <- load_model_hdf5(model_ind_high_path)
  history_cat_high <- readRDS(history_ind_high_path)
  pred_cat_high <- readRDS(pred_ind_high_path)
} else {
  cp_callback_high <- callback_model_checkpoint(
    filepath = file.path("data/models/ind/categorical/2_dense_512", "high.{epoch:02d}-{val_loss:.2f}.hdf5")
  )
  model_cat_high <- fit_model_cat(x[train_idx, ], y_high[train_idx], 30, callbacks = list(cp_callback_high))
  save_model_hdf5(model_cat_high$model, model_ind_high_path)
  saveRDS(model_cat_high$history, history_ind_high_path)
  pred_cat_high <- predict_proba(model_cat_high$model, x[test_idx, ], batch_size = 512)
  saveRDS(pred_cat_high, pred_ind_high_path)
}

model_ind_close_path  <- "data/models/ind/categorical/2_dense_512/model_close.h5"
history_ind_close_path  <- "data/models/ind/categorical/2_dense_512/history_close.rds"
pred_ind_close_path  <- "data/models/ind/categorical/2_dense_512/pred_close.rds"
if (file.exists(model_ind_close_path) && file.exists(history_ind_close_path) && file.exists(pred_ind_close_path)) {
  model_cat_close <- load_model_hdf5(model_ind_close_path)
  history_cat_close <- readRDS(history_ind_close_path)
  pred_cat_close <- readRDS(pred_ind_close_path)
} else {
  cp_callback_close <- callback_model_checkpoint(
    filepath = file.path("data/models/ind/categorical/2_dense_512", "close.{epoch:02d}-{val_loss:.2f}.hdf5")
  )
  model_cat_close <- fit_model_cat(x[train_idx, ], y_close[train_idx], 30, callbacks = list(cp_callback_close))
  save_model_hdf5(model_cat_close$model, model_ind_close_path)
  saveRDS(model_cat_close$history, history_ind_close_path)
  pred_cat_close <- predict_proba(model_cat_close$model, x[test_idx, ], batch_size = 512)
  saveRDS(pred_cat_close, pred_ind_close_path)
}
```

Während des Trainings der Netze für Tiefst-, Höchst und Schlusskurs zeigt sich der in Abbildung \@ref(fig:ind_train_progress) visualisierte Lernfortschritt. Als Genauigkeit wird hierbei der Anteil welcher dem richtigen Bucket zugeordnet wurde angegeben. Bei der Betrachtung des Lernprozesses lassen sich folgende Erkenntnisse gewinnen:

- Die Genauigkeit richtig zugeordner Klassen liegt deutlich über demjenigen, welcher bei zufälliger Zuteilung erreicht werden könnte. Im vorliegenden Fall von 30 Buckets bei ungefähr gleichverteilter Anzahl Datenbunkte wäre dies lediglich 3.3%.
- Die Genauigkeit der Prognose für Tiefst- und Höchstkurs ist deutlich höher als diejenige des Schlusskurses.
- Über den Traingsverlauf nimmt die Genauigkeit zu. Dies gilt sowohl für die Trainings- wie auch die Validierungsdaten. Ein Overfitting ist damit im vorliegenden Fall nicht auszumachen.


```{r, fig.cap='Trainingsfortschritte im Falle unabhängiger Modelle', fig.asp=1, fig.pos = '!H', "ind_train_progress", echo=FALSE, message=FALSE, warning = FALSE} 
train_history <- as_tibble(history_cat_low) %>% mutate(price = "low") %>%
  bind_rows(as_tibble(history_cat_high) %>% mutate(price = "high")) %>%
  bind_rows(as_tibble(history_cat_close) %>% mutate(price = "close")) %>%
  mutate(
    price = factor(price, levels = c("low", "high", "close")),
    data = do.call(recode, list(.x = data, validation = "Validierung", training = "Training"))
  )

train_history %>% 
  filter(metric == "accuracy") %>%
  ggplot(mapping = aes(x = epoch, y = value, color = data)) +
  geom_line() + 
  facet_wrap(vars(price), ncol = 3,  scales = "free_y") +
  scale_x_continuous(labels = scales::number_format(accuracy = 1)) +
  ylab("Genauigkeit") + 
  xlab("Epoche") +
  labs(color = "Daten") + 
  theme_bw()
```


Mit Hilfe der trainierten Modelle lassen sich nun Voraussagen über die Wahrscheinlichkeitsverteilungen der Daten im Testset machen. Für einzelne Beobachtungen lassen sich diese gar visualisieren. Abbildung \@ref(fig:plot_binary_histogram) zeigt die prognostizierten Verteilungen zweier exemplarischen Einträge im Testset. Diese unterscheiden sich deutlich. Während für ersteren Eintrag ein wenig volatiler Kursverlauf prognostiziert ist, erwartet das Modell im zweiten Fall eine deutlich volatilere Entwicklung.

```{r, fig.cap='Exemplarische Verteilung der vorausgesagten Preise', fig.asp=1, fig.pos = '!H', "plot_binary_histogram", echo=FALSE, message=FALSE, warning = FALSE}
eval_idx <- c(52, 12)
ind_categorical$pred <- list(low = pred_cat_low, high = pred_cat_high, close = pred_cat_close)
plot_neural_sample_histogram(eval_idx, ind_categorical)
```

Tatsächlich unterscheiden sich die realisierten Tiefts- und Schlusskurse, wenn auch nicht so deutlich wie dies vom Modell prognostiziert wird. Die entsprechenden Realisierungen für Low und High sind `r round(data_wide_10$Low_0[test_idx][eval_idx[1]], 2)` und `r round(data_wide_10$High_0[test_idx][eval_idx[1]], 2)` für das erste und `r round(data_wide_10$Low_0[test_idx][eval_idx[2]], 2)` und `r round(data_wide_10$High_0[test_idx][eval_idx[2]], 2)` für das zweite Beispiel.



```{r}

mid_low <- calc_mid_prices(ind_categorical$discretization$low$borders$lower, ind_categorical$discretization$low$borders$upper)
mid_high <- calc_mid_prices(ind_categorical$discretization$high$borders$lower, ind_categorical$discretization$high$borders$upper)
mid_close <- calc_mid_prices(ind_categorical$discretization$close$borders$lower, ind_categorical$discretization$close$borders$upper)
rm(ind_categorical)

glob_scen <- expand_grid(Close = mid_close, High = mid_high, Low = mid_low) %>% mutate(ID = seq_len(nrow(.))) %>% filter(Low <= Close, High >= Close)

data_test <- select(data_wide_10[test_idx, ], Close_1, High = "High_0", Low = "Low_0", Close_0)

```

Zur Überprüfung der Modelleignung soll erneut der Payoff des Testsets ermittelt und mit demjenigen der Referenzstrategie verglichen werden. Eine erste Möglichkeit besteht darin, die Kaufs- resp. Verkaufsschranken als Tiefst- respektive den Höchstpreis desjenigen Buckets vorauszusagen, welchem das Modell die höchste Wahrscheinlichkeit vorhersagt.[^Da ein Bucket durch untere und obere Grenze bestimmt ist, verwenden wir den Mittelwert von oberer und unterer Grenze als Vorhersagewert. Ist eine der Grenzen nicht finit, wird die andere Grenze als Vorhersagewert verwendet.] 

```{r}

ind_buy_sell_max_prob_path <- "data/models/ind/categorical/2_dense_512/ind_buy_sell_max_prob.rds"
if (file.exists(ind_buy_sell_max_prob_path)) {
  ind_buy_sell_max_prob <- read_feather(ind_buy_sell_max_prob_path)
} else {
  n_eval <- length(test_idx)
  ind_buy_sell_max_prob <- pbmclapply(seq_len(n_eval), function(i) {
    probs <- as.vector(pred_cat_low[i, ] %o% pred_cat_high[i, ] %o% pred_cat_close[i, ])[glob_scen$ID]
    threshold <- quantile(probs, q)
    calc_best_buy_sell(probs, max(probs), glob_scen$Low, glob_scen$High, glob_scen$Close, both_first = both_first[test_idx[i]])
  }, mc.cores = 3) %>% bind_rows()
  write_feather(ind_buy_sell_max_prob, ind_buy_sell_max_prob_path)
}

factor_ind_max_prob <- sum(calc_payoff_const_gamma(data_test, buy = ind_buy_sell_max_prob$buy, sell = ind_buy_sell_max_prob$sell, both_first = both_first[test_idx])) / sum(calc_payoff_const_gamma(data_test, both_first = both_first[test_idx]))

```


Es resultiert ein Überschusspayoff von `r round((factor_ind_max_prob - 1) * 100, 1)`%. Bereits in dieser einfachen Form ist das gelernte Modell damit gleich gut gut, wie das Model mit Kaufs- und Verkaufsschranken als symmetrischer Abweichungen vom aktuellen Eröffnungskurs bei Unterscheidung von Hoch- und Tiefvolatilitätsphasen. Bei der Konzeption des Modelles war es gerade das Ziel, nicht nur eine Klassifikation in das wahrscheinlichste Bucket zu machen, sondern auch Voraussagen über die Verteilung der prognostizierten Preise machen zu können. Lediglich das Bucket mit höchster Wahrscheinlichkeit als Prognosewert zu verwenden ging damit zuwenig weit. Viel können nun verschiedene Kaufs- und Verkaufskombinationen für alle Preisszenarien berechnet und mit der jeweiligen Wahrscheinlichkeit gewichtet werden. Die optimale Strategie lässt sich dann diejenige mit dem höchsten erwarteten Payoff auswählen. Hierbei lohnt wiederum ein Blick auf die Berchnungskomplexität des Problems. In der vorliegenden Analyse wurden je Preistyp 30 Buckets verwendet. Dies resultiert in 27'000 möglichen Preisszenarien. Diese können für jeden Eintag des Testsets (rund 1'000'000 Einträge) ermittelt werden. Für jedes dieser Preisszenarien sollen wiederum verschiedene Paare von Kaufs- und Verkaufspreisen getestet werden. Orientiert man sich dabei an einer ähnlichen Granularität wir für die Preise resultieren 900 Szenarien für die Schranken (je 30 Werte für die Kaufs- und Verkaufsschranke). Damit müssen zur kompletten Evaluation $27'000 \times 1'000'000  \times 900$ Payoffs berechnet werden. Dies ist mit Hilfe der in Kapitel \@ref(infrastruktur_und_tools) vorgestellen Cloud-Infrastruktur und Tools nicht innerhalb kurzer Zeit möglich. Zur Reduktion der Komplexität lässt sich dabei ausnützen, dass nicht alle Szenarien von gleicher Bedeutung sind. Während die Verwendung des häufigsten Szenarios und die Verwendung aller Szenarios die beiden Extrempositionen einnehmen, ist auch die Verwendgung einiger wichtiger Preis- und Schrankenszenarien denkbar. Konkret werden diehjenigen Szenarien ausgewählt, welche die höchsten Eintretenswahrscheinlichkeiten aufweisen. Dabei hat es sich bewährt, diese Grenze als Quantil der jeweiligen Wahrscheinlichkeiten zu wählen. Damit werden nur sehr wenige, dafür wichtige Szenarien berücksichtigt. Die Berechungszeits lässt sich damit sehr deutlich senken. Diese Vorgehensweise hat zudem den Vorteil, dass mit einer Reduzierung des Quantil sehr einfach mehr Werte einbezogen werden könnnen. Ferner wird auch die Schrankenszenarien sehr stark reduziert, indem als Kaufs- und Verkaufspreise nur die Tiefs- und Höchstwerte der betrachteten Preisszenarien berechnet werden. 

Das beschriebene Vorgehen sowohl für unabhängige wie auch abhängige Modelle möglich. Bei den unabhängigen Modellen gibt es ferner zusätzlich zu beachten, dass nicht mögliche Szenarien (bsp. prognostizierter Höchstpreis < prognostizierter Tieftspreis) entfernt, resp. deren prognostizierte Wahrscheinlichkeiten auf 0 gesetzt werden. Bei den Modellen mit voneinander abhängigen Preisen treten solche Szenarien nicht auf.


```{r}

ind_buy_sell_0_999_prob_path <- "data/models/ind/categorical/2_dense_512/ind_buy_sell_0_999_prob.rds"
if (file.exists(ind_buy_sell_0_999_prob_path)) {
  ind_buy_sell_0_999_prob <- read_feather(ind_buy_sell_0_999_prob_path)
} else {
  n_eval <- length(test_idx)
  q <- 0.999
  ind_buy_sell_0_999_prob <- pbmclapply(seq_len(n_eval), function(i) {
    probs <- as.vector(pred_cat_low[i, ] %o% pred_cat_high[i, ] %o% pred_cat_close[i, ])[glob_scen$ID]
    threshold <- quantile(probs, q)
    calc_best_buy_sell(probs, threshold, glob_scen$Low, glob_scen$High, glob_scen$Close, both_first = both_first[test_idx[i]])
  }, mc.cores = 4) %>% bind_rows()
  write_feather(ind_buy_sell_0_999_prob, ind_buy_sell_0_999_prob_path)
}

factor_ind_buy_sell_0_999_prob <- sum(calc_payoff_const_gamma(data_test, buy = ind_buy_sell_0_999_prob$buy, sell = ind_buy_sell_0_999_prob$sell, both_first = both_first[test_idx])) / sum(calc_payoff_const_gamma(data_test, both_first = both_first[test_idx]))


```


Bezieht man für die Pronose der Preisschranken alle Buckets mit 0.1% höchster Wahrscheinlichkeit mit ein (99.9% Quantil) resultiert für den Testdatensatz ein ÜBershusspayoff von `r round((factor_ind_buy_sell_0_999_prob - 1) * 100, 1)`% gegenüber der Referenzstartegie. Durch Berücksichtigung (eines Teils) der geschätzen Verteilung kann dieser damit noch einmal um `r round((factor_ind_buy_sell_0_999_prob - factor_ind_max_prob) * 100, 1)` Prozentpunkte gesteiegert werden und übertrifft damit die Performance einfacherer früherer Modelle.

``` {r}
ind_binary <-  NULL
ind_binary$discretization <- list(
  low = multivariate_discretization(data_wide_10, train_idx, test_idx, "Low_0", n_groups_per_col),
  high = multivariate_discretization(data_wide_10, train_idx, test_idx, "High_0", n_groups_per_col),
  close = multivariate_discretization(data_wide_10, train_idx, test_idx, "Close_0", n_groups_per_col)
)

y_low <- ind_binary$discretization$low$groups
y_high <- ind_binary$discretization$high$groups
y_close <- ind_binary$discretization$close$groups

fit_model_bin <- function(x, y, n_classes, epochs = 30, callbacks = NULL) {
  
  y <- as.matrix(purrr::map_dfc(seq_len(n_classes) - 1, ~as.integer(.<=y))) %>% unname()
  
  model <- keras_model_sequential()
  model %>%
    layer_dense(units = 512, activation = 'relu', input_shape = ncol(x)) %>%
    layer_dense(units = 512, activation = 'relu') %>%
    layer_dense(units = n_classes, activation = 'sigmoid')
  
  model %>% compile(
    loss = 'binary_crossentropy',
    optimizer = optimizer_rmsprop(),
    metrics = c('accuracy')
  )
  
  history <- model %>% fit(
    x = x,
    y = y,
    epochs = epochs,
    batch_size = 512,
    validation_split = 0.2,
    callbacks = callbacks
  )
  list(model = model, history = history)
}


model_ind_bin_low_path <- "data/models/ind/binary/2_dense_512/model_low.h5"
history_ind_bin_low_path <- "data/models/ind/binary/2_dense_512/history_low.rds"
if (file.exists(model_ind_bin_low_path) && file.exists(history_ind_bin_low_path)) {
  model_ind_bin_low <- load_model_hdf5(model_ind_bin_low_path)
  history_ind_bin_low <- readRDS(history_ind_bin_low_path)
} else {
  cp_callback_bin_low <- callback_model_checkpoint(
    filepath = file.path("data/models/ind/binary/2_dense_512", "low.{epoch:02d}-{val_loss:.2f}.hdf5")
  )
  model_cat_bin_low <- fit_model_bin(x[train_idx, ], y_low[train_idx], n_classes = 30, callbacks = list(cp_callback_bin_low))
  save_model_hdf5(model_cat_bin_low$model, model_ind_bin_low_path)
  saveRDS(model_cat_bin_low$history, history_ind_bin_low_path)
}

model_ind_bin_high_path <- "data/models/ind/binary/2_dense_512/model_high.h5"
history_ind_bin_high_path <- "data/models/ind/binary/2_dense_512/history_high.rds"
if (file.exists(model_ind_bin_high_path) && file.exists(history_ind_bin_high_path)) {
  model_ind_bin_high <- load_model_hdf5(model_ind_bin_high_path)
  history_ind_bin_high <- readRDS(history_ind_bin_high_path)
} else {
  cp_callback_bin_high <- callback_model_checkpoint(
    filepath = file.path("data/models/ind/binary/2_dense_512", "high.{epoch:02d}-{val_loss:.2f}.hdf5")
  )
  model_ind_bin_high <- fit_model_bin(x[train_idx, ], y_high[train_idx], n_classes = 30, callbacks = list(cp_callback_bin_high))
  save_model_hdf5(model_cat_bin_high$model, model_ind_bin_high_path)
  saveRDS(model_cat_bin_high$history, history_ind_bin_high_path)
}

model_ind_bin_close_path <- "data/models/ind/binary/2_dense_512/model_close.h5"
history_ind_bin_close_path <- "data/models/ind/binary/2_dense_512/history_close.rds"
if (file.exists(model_ind_bin_close_path) && file.exists(history_ind_bin_close_path)) {
  model_ind_bin_close <- load_model_hdf5(model_ind_bin_close_path)
  history_ind_bin_close <- readRDS(history_ind_bin_close_path)
} else {
  cp_callback_bin_close <- callback_model_checkpoint(
    filepath = file.path("data/models/ind/binary/2_dense_512", "close.{epoch:02d}-{val_loss:.2f}.hdf5")
  )
  model_ind_bin_close <- fit_model_bin(x[train_idx, ], y_close[train_idx], n_classes = 30, callbacks = list(cp_callback_bin_close))
  save_model_hdf5(model_cat_bin_close$model, model_ind_bin_close_path)
  saveRDS(model_cat_bin_close$history, history_ind_bin_close_path)
}
```

Wie in Abbildung \@(fig:plot_binary_histogram) gezeigt schien das Modell auch ohne explizite Modellierung der Ordinalität der Klassen diese implizit gelernt zu haben. Dies zeigt sich dadurch, dass benachbarte Klassen typsischerweise auch ähnliche Wahrscheinlichkeiten aufweisen. Eine explizite Modelierung der Ordinalität scheint damit im vorliegenden Fall nicht unbedingt nötig. Dies zeigt sich auch beim Vergleich der Ergebnisse mit ebendiesem Modell. 

...

```{r, "make_ind_binary_predictions"}

translate_cum_prob <- function(cum_prob) {
  n_col <- ncol(cum_prob)
  n_row <- nrow(cum_prob)
  for (i in tail(seq_len(n_col), -1)) {
    cum_prob[, i] <- pmin(cum_prob[, i], cum_prob[, i-1])
  }
  cum_prob - cbind(cum_prob[, head(1 + seq_len(n_col), -1)],rep(0, n_row))
}

pred_bin_low <- predict_proba(model_ind_bin_low, x[test_idx, ], batch_size = 512) %>% translate_cum_prob()
pred_bin_high <- predict_proba(model_ind_bin_high, x[test_idx, ], batch_size = 512) %>% translate_cum_prob()
pred_bin_close <- predict_proba(model_ind_bin_close, x[test_idx, ], batch_size = 512) %>% translate_cum_prob()

```


```{r, fig.cap='Exemplarische Verteilung der vorausgesagten Preise', fig.asp=1, fig.pos = '!H', "plot_binary_histogram", echo=FALSE, message=FALSE, warning = FALSE}
eval_idx <- c(52, 12)
ind_binary$pred <- list(low = pred_bin_low, high = pred_bin_high, close = pred_bin_close)
plot_neural_sample_histogram(eval_idx, ind_binary)
```


```{r}

ind_bin_buy_sell_max_prob_path <- "data/models/ind/binary/2_dense_512/ind_buy_sell_max_prob.rds"
if (file.exists(ind_bin_buy_sell_max_prob_path)) {
  ind_bin_buy_sell_max_prob <- read_feather(ind_bin_buy_sell_max_prob_path)
} else {
  n_eval <- length(test_idx)
  ind_bin_buy_sell_max_prob <- pbmclapply(seq_len(n_eval), function(i) {
    probs <- as.vector(pred_bin_low[i, ] %o% pred_bin_high[i, ] %o% pred_bin_close[i, ])[glob_scen$ID]
    calc_best_buy_sell(probs, max(probs), glob_scen$Low, glob_scen$High, glob_scen$Close, both_first = both_first[test_idx[i]])
  }, mc.cores = 3) %>% bind_rows()
  write_feather(ind_bin_buy_sell_max_prob, ind_bin_buy_sell_max_prob_path)
}

factor_ind_bin_max_prob <- sum(calc_payoff_const_gamma(data_test, buy = ind_bin_buy_sell_max_prob$buy, sell = ind_bin_buy_sell_max_prob$sell, both_first = both_first[test_idx])) / sum(calc_payoff_const_gamma(data_test, both_first = both_first[test_idx]))

```

```{r}

ind_bin_buy_sell_0_999_prob_path <- "data/models/ind/binary/2_dense_512/ind_bin_buy_sell_0_999_prob.rds"
if (file.exists(ind_bin_buy_sell_0_999_prob_path)) {
  ind_bin_buy_sell_0_999_prob <- read_feather(ind_bin_buy_sell_0_999_prob_path)
} else {
  n_eval <- length(test_idx)
  q <- 0.999
  ind_bin_buy_sell_0_999_prob <- pbmclapply(seq_len(n_eval), function(i) {
    probs <- as.vector(pred_bin_low[i, ] %o% pred_bin_high[i, ] %o% pred_bin_close[i, ])[glob_scen$ID]
    threshold <- quantile(probs, q)
    calc_best_buy_sell(probs, threshold, glob_scen$Low, glob_scen$High, glob_scen$Close, both_first = both_first[test_idx[i]])
  }, mc.cores = 2) %>% bind_rows()
  write_feather(ind_bin_buy_sell_0_999_prob, ind_bin_buy_sell_0_999_prob_path)
}

factor_ind_bin_buy_sell_0_999_prob <- sum(calc_payoff_const_gamma(data_test, buy = ind_bin_buy_sell_0_999_prob$buy, sell = ind_bin_buy_sell_0_999_prob$sell, both_first = both_first[test_idx])) / sum(calc_payoff_const_gamma(data_test, both_first = both_first[test_idx]))

```


### Abhängige Modelle


```{r, echo=FALSE}
n_groups_per_col <- 30

dep_categorical <-  NULL
dep_categorical$discretization <- multivariate_discretization(data_wide_10, train_idx, test_idx, c("Low_0", "High_0", "Close_0"), n_groups_per_col)

y <- dep_categorical$discretization$groups

model_dep_path  <- "data/models/dep/categorical/2_dense_512/model.h5"
history_dep_path  <- "data/models/dep/categorical/2_dense_512/history.rds"
if (file.exists(model_dep_path) && file.exists(history_dep_path)) {
  model_dep <- load_model_hdf5(model_dep_path)
  # model_dep <- load_model_hdf5("data/models/dep/categorical/2_dense_512/dep.04-9.31.hdf5")

  history_dep <- readRDS(history_dep_path)
} else {
  cp_callback_dep <- callback_model_checkpoint(
    filepath = file.path("data/models/dep/categorical/2_dense_512", "dep.{epoch:02d}-{val_loss:.2f}.hdf5")
  )
  model_dep <- fit_model_cat(x[train_idx, ], y[train_idx], callbacks = list(cp_callback_dep), epochs = 30, n_groups = 27000)
  save_model_hdf5(model_dep$model, model_dep_path)
  saveRDS(model_dep$history, history_dep_path)
  
}

```


```{r}
dep_categorical$discretization$borders
mid_dep_low <- calc_mid_prices(dep_categorical$discretization$borders$Low_0_lower, dep_categorical$discretization$borders$Low_0_upper)
mid_dep_high <- calc_mid_prices(dep_categorical$discretization$borders$High_0_lower, dep_categorical$discretization$borders$High_0_upper)
mid_dep_close <- calc_mid_prices(dep_categorical$discretization$borders$Close_0_lower, dep_categorical$discretization$borders$Close_0_upper)

n_eval <- length(test_idx)
dep_buy_sell_max_prob_path <- "data/models/dep/categorical/2_dense_512/dep_buy_sell_max_prob.rds"
if (file.exists(dep_buy_sell_max_prob_path)) {
  dep_buy_sell_max_prob <- read_feather(dep_buy_sell_max_prob_path)
} else {
  chunk_size <- 4000
  chunks <- chunk2(seq_len(length(test_idx)), ceiling(length(test_idx)/chunk_size))
  dep_buy_sell_max_prob <- map_df(seq_along(chunks), function(curr_chunk_id) {
    print(curr_chunk_id)
    curr_chunk <- chunks[[curr_chunk_id]]
    probs <- predict_proba(model_dep, x[test_idx[curr_chunk], ], batch_size = 512)
    pbmclapply(seq_along(curr_chunk), function(i) {
      calc_best_buy_sell(probs[i, ], max(probs[i, ]), mid_dep_low, mid_dep_high, mid_dep_close, both_first = both_first[test_idx[curr_chunk[i]]])
    }, mc.cores = 4) %>% bind_rows()
  })
  
  write_feather(dep_buy_sell_max_prob, dep_buy_sell_max_prob_path)
}

factor_dep_max_prob <- sum(calc_payoff_const_gamma(data_test, buy = dep_buy_sell_max_prob$buy, sell = dep_buy_sell_max_prob$sell, both_first = both_first[test_idx[seq_len(n_eval)]])) / sum(calc_payoff_const_gamma(data_test, both_first = both_first[test_idx[seq_len(n_eval)]]))

```



```{r}
n_eval <- length(test_idx)
dep_buy_sell_0_999_prob_path <- "data/models/dep/categorical/2_dense_512/dep_buy_sell_0_999_prob.rds"
if (file.exists(dep_buy_sell_0_999_prob_path)) {
  dep_buy_sell_0_999_prob <- read_feather(dep_buy_sell_0_999_prob_path)
} else {
  chunk_size <- 4000
  chunks <- chunk2(seq_len(length(test_idx)), ceiling(length(test_idx)/chunk_size))
  q <- 0.999
  dep_buy_sell_0_999_prob <- map_df(seq_along(chunks), function(curr_chunk_id) {
    print(curr_chunk_id)
    curr_chunk <- chunks[[curr_chunk_id]]
    probs <- predict_proba(model_dep, x[test_idx[curr_chunk], ], batch_size = 512)
    pbmclapply(seq_along(curr_chunk), function(i) {
      threshold <- quantile(probs[i, ], q)
      calc_best_buy_sell(probs[i, ], threshold, mid_dep_low, mid_dep_high, mid_dep_close, both_first = both_first[test_idx[curr_chunk[i]]])
    }, mc.cores = 4) %>% bind_rows()
  })
  
  write_feather(dep_buy_sell_0_999_prob, dep_buy_sell_0_999_prob_path)
}


factor_dep_0_999_prob <- sum(calc_payoff_const_gamma(data_test, buy = dep_buy_sell_0_999_prob$buy, sell = dep_buy_sell_0_999_prob$sell, both_first = both_first[test_idx[seq_len(n_eval)]])) / sum(calc_payoff_const_gamma(data_test, both_first = both_first[test_idx[seq_len(n_eval)]]))

```
