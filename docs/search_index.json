[
["index.html", "Die Bestimmung optimaler Kauf- und Verkaufspreise von Basiswerten zur Wahrung der täglichen Delta-Neutralität beim Handel von Aktionoptionen mit Hilfe verschiedener Machine Learning Methoden Management Summary", " Die Bestimmung optimaler Kauf- und Verkaufspreise von Basiswerten zur Wahrung der täglichen Delta-Neutralität beim Handel von Aktionoptionen mit Hilfe verschiedener Machine Learning Methoden Fabian Gehring, Zürcher Hochschule für Angewandte Wissenschaften Management Summary TODO "],
["intro.html", "Kapitel 1 Einleitung 1.1 Motivation 1.2 Forschungsfrage 1.3 Daten 1.4 Methoden 1.5 Disclaimer", " Kapitel 1 Einleitung 1.1 Motivation Aktienoptionen sind derivative Finanzinstrumente, welche dem Inhaber das Recht (nicht aber die Pflicht) geben, die zugrunde liegende Aktie (auch Basiswert oder Underlying) zu einem im Voraus festgelegten Ausübungspreis (auch Strike) zu kaufen oder zu verkaufen. Wie sich der Preis einer Option entwickelt, ist dabei von verschiedenen Faktoren abhängig. Dazu gehören unter anderem der Preis des Basiswertes, dessen Volatilität oder die Restlaufzeit der Option. Der Zusammenhang zwischen Preis des Underlyings und Preis der Option ist nicht linear, sondern nimmt bei steigendem Preis zu (Kaufoption), resp. ab (Verkaufsoption). Diese nicht-Linearität wird mit Hilfe der Kennzahl Gamma (\\(\\Gamma\\)) beschrieben. Im profesionellen Optionenhandel ist es üblich, den linearen Teil der Veränderung (Delta, auch \\(\\Delta\\)) durch Kauf oder Verkauf des Underlyings abzusichern. Damit lassen sich Strategien verfolgen, welche von der Richtung der Preisbewegung unabhängig sind (Delta-Neutral Trading). Der Nicht-lineare Zusammenhang führt allerdings dazu, dass die Delta-Neutralität mit sich ändernden Preisen wieder verloren geht. Dies sein an einem Beispiel demonstriert. Sei dafür angenommen, dass es eine Aktie zum Preis von 10 gehandelt wird, welche als Basiswert sowohl für eine Kaufoption (Call) wie auch eine Verkaufsoption (Put) dient. Die Laufzeit betrage 6 Monate, der Strike ebenfalls 10. Mit dem Kauf der Option erwirbt der Besitzer damit das Recht, die Aktie in 6 Monaten zu 10 zu kaufen (im Fall des Calls) oder zu verkaufen (im Falle des Puts). In diesem Beispiel entspricht der Aktienkurs dem Strike. Solche Optionen bezeichnet man als “at-the-money (atm)”. Bei Veränderung des Basispreises werden diese zu “in-the-money (itm)” oder “out-of-the-money (otm)” Optionen. Abbildung 1.1: Zusammenhang zwischen Preis der Underlyings und Preis der Option. Den Zusammenhang zwischen Preis der Option und Preis des Underlyings ist in 1.1 abgebildet. Die gestrichelten Linien bilden die Tangenten am Strikepreis der Optionen. Deren Steigungen (\\(\\Delta\\)) betragen 1.42 und im Falle des Calls, und -1.37 im Falle des Puts. Dies bedeutet, dass sich der Besitzer der Option sich gegen Preisbewegungen absichern (hegden) kann, indem zusätzlich zum Besitz der Option diese Anzahl an Aktien verkauft (Call), resp. kauft (Put). Wertänderungen der Option werden dann durch die entgegenläufige Wertenwicklung in der Aktienposition ausgeglichen. Eine solche Position wird “deltaneutral” genannt. Bei grösseren Preisbewegungen führt die Konvexität des Optionspreises allerdings dazu, dass es trotz Absicherung zu einer Abweichung kommt. Diese Fehler sind in Abbildung 1.1 mit roten Linien dargestellt. Es mag auf den ersten Blick erstaunen, dass diese Abweichung immer zugunsten des Optionsbesitzers ausfällt (sowohl für Call- als auch Put-Option verlaufen die Tangenten unterhalb des Optionspreises). Dass es sich dabei aber nicht eine Geldmaschine handelt liegt daran, dass die beschriebenen Preisbewegungen erst im Laufe der Zeit erfolgen und die Restlaufzeit der Option kleiner wird (Theta, auch \\(\\Theta\\)). Mit sinkender Restlaufzeit auch der Wert der Option ab. In effizienten Märkten gleichen sich diese Effekte im Durchschnitt aus. Wenn die zukünftigen Bewegungen des Preises aber grösser sind, als dies durch den Markt mit dem Wertverlust aufgrund der Zeit einpreist, kann die Verfolgung einer solchen Handelsstrategie allerdings durchaus lukrativ sein. Die Schwierigkeit liegt hier natürlich bei der Auswahl der richtigen Titel. Diese Strategie wird als Gamma-Scalping bezeichnet. Sie verdankt ihren Namen der zweiten Ableitung des Optionspreises nach dem Underlyingpreis - dem Gamma (\\(\\Gamma\\)) - welches die Nichtlinearität der Beziehung beschreibt. Der Einfluss des Gammas ist aber nicht nur zu Spekulationszwecken interessant. Dessen Bewirtschaftung ist auch für die Steuerung des eingegangenen Risikos von Relevanz. Soll nämlich die Delta-Neutralität nach Preisbewegungen wieder hergestellt werden, müssen laufend neue Titel des Underlyings gekauft oder verkauft werden. Grafisch entspricht dies der Einnahme einer Aktienposition, welche wieder der Steigung der Tangente beim nun vorherrschenden Preis entspricht. Typischerweise ist es so, dass diese Anpassungen abends (zur Reduktion des Risikos über Nacht) oder je nach Entscheidung des Händlers auch untertags erfolgen. Der Händler wird dabei versuchen, den Ausgleich nach möglichst grosser Preisbewegung zu machen, d.h. negatives Delta am Tiefstpunkt zu kaufen resp. positives Delta am Höchstpunkt zu verkaufen. Verpasst er diesen Zeitpunkt und die Preise entwickeln sich wieder Richtung Ausgangspunkt, so entgehen ihm mögliche Gewinne. 1.2 Forschungsfrage Als Forschungsfrage der vorliegenden Arbeit ergibt sich damit folgende Forschungsfrage: Kann mit Hilfe datengestützter Methoden eine Strategie gefunden werden, welche einen Händler von Aktienoptionen unterstützt, die idealen (intraday) Preise zum Kauf / Verkauf von aufgelaufenem Delta zu finden? Als Vergleich bietet sich dabei eine Strategie an, welche keine intraday Anpassungen vornimmt, sondern die aufgelaufene Delta-Position erst am Abend glattstellt. Die Voraussage jeweils eines Verkaufs- und eines Kaufpreises erfolgt dabei am Morgen des Arbeitstages bei Markteröffnung. Bei Tagesende noch vorhandenes Delta (aufgrund nicht erreichter Kaufs- oder Verkaufspreise oder neu aufgelaufenes Delta bis Tagesende) wird zum Tagesschlusskurs ausgeglichen. Als Vergleichsstrategie dient die Strategie, bei welchem aufgelaufenes Delta erst am Abend zum Börsenschlusskurs ausgeglichen wird. Mathematisch lässt sich beide Strategien mit Hilfe einer Payoff-Funkt ausdrücken. Für die einfachere Referenzstrategie, welche die Delta-Neutraliät erst bei Tagesende wieder herstellt, lautet diese wie folgt: \\[\\begin{equation} \\begin{aligned} Payoff ={} &amp; \\frac{1}{2} \\times 100 \\times \\$\\Gamma_{close_{t-1}} \\times \\ln{\\left(\\frac{close_t}{close_{t-1}}\\right)}^2 \\end{aligned} \\end{equation}\\] Für die Notation werden log-Returns verwendet. Diskrete Returns sind aber genauso möglich. Als \\(close_i\\) wird der Tagesendkurs am Tag \\(i\\) bezeichnet. Unter \\(\\$\\Gamma_i\\) verstehen wir dabei das Gamma Cash (auch Dollar Cash). Die Herleitung des Payoffs ergibt sich aufgrund folgender Überlegungen:(vgl. SP-Finance 2020) Bezeichne dazu \\(S_0\\) den Preis des Underlying und \\(P_0\\) den Preis der Option zu Beginn und \\(S_1\\) sowie \\(P_1\\) die jeweiligen Preise nach einer Preisbewegung des Underlyings um \\(d_S\\). \\(\\Delta_0\\) und \\(\\Gamma_0\\) bezeichnen die erste und zweite Ableitung des Optionspreises zum Ausgangszeitpunkt. Ferner sei als Delta Cash (\\(\\$\\Delta = \\Delta \\times S\\)) der Wert des Hedge Portfolio bezeichnet und Gamma Cash (\\(\\$\\Gamma = \\Gamma \\times S^2/100\\)) bezeichne die Veränderung des \\(\\$\\Delta\\) bei einer 1-prozentigen Veränderung des Underlying-Preises. Dies lässt sich wie folgt herleiten: Verändert sich der Underlying-Preis um 1% \\(S =&gt; S + S / 100\\), verändert sich auch das Delta \\(\\Delta =&gt; \\Delta + \\Gamma \\times S/100\\). Das neue Delta Cash verändert sich damit in der Höhe von Gamma Cash \\(\\$\\Delta =&gt; \\$\\Delta + \\Gamma \\times S^2/100\\). Mit Hilfe dieser Nomenklatur lässt sich der Gewinn einer zu Beginn delta-gehedgten Position herleiten: Veränderung des Underlying Preises \\(S_1 = S_0 + dS\\) Veränderung des Delta \\(\\Delta_1 = \\Delta_0 + \\Gamma_0 \\times dS\\) Durchschnittliches Delta \\(\\Delta_{Avg} = (\\Delta_0 + \\Delta_1) / 2 = \\Delta_0 + \\Gamma_0 \\times dS / 2\\) Neuer Optionspreis \\(P_1 = P_0 + \\Delta_{Avg} \\times dS = P_0 + \\Delta_0 \\times dS + \\Gamma_0 \\times dS² / 2\\) Gewinn als Summe der Veränderungen des Optionspreis und des Hedge-Portfolios: \\(\\Delta_0 \\times dS + \\Gamma_0 \\times dS^2 / 2 - \\Delta_0 \\times dS = \\Gamma_0 \\times dS^2 / 2\\) Etwas umgeformt lässt sich der Gewinn schreiben als: \\(\\Gamma_0 \\times dS^2 / 2 = \\Gamma_0 \\times S^2 /100 * dS^2 / S^2 * 100 = \\$\\Gamma \\times (dS/S)^2 \\times 100\\) Auch im Falle der etwas komplexeren Strategie mit der Möglichkeit von Käufen und Verkäufen innerhalb des Tages lässt sich diese Struktur wiederfinden: \\[\\begin{eqnarray} {Payoff} &amp; = &amp; \\frac{1}{2} \\times 100 \\times \\$\\Gamma_{close_{t-1}} \\times \\ln{\\left(\\frac{first}{close_{t-1}}\\right)}^2 \\times I_{first} \\times (1 - I_{second}) \\ +\\\\ &amp; &amp; \\frac{1}{2} \\times 100 \\times \\$\\Gamma_{first} \\times \\ln{\\left(\\frac{close_t}{first}\\right)}^2 \\times I_{first} \\times (1 - I_{second}) \\ +\\\\ &amp; &amp; \\frac{1}{2} \\times 100 \\times \\$\\Gamma_{close_{t-1}} \\times \\ln{\\left(\\frac{second}{close_{t-1}}\\right)}^2 \\times I_{second} \\times (1 - I_{first}) \\ +\\\\ &amp; &amp; \\frac{1}{2} \\times 100 \\times \\$\\Gamma_{second} \\times \\ln{\\left(\\frac{close_t}{second}\\right)}^2 \\times I_{second} \\times (1 - I_{first}) \\ +\\\\ &amp; &amp; \\frac{1}{2} \\times 100 \\times \\$\\Gamma_{close_{t-1}} \\times \\ln{\\left(\\frac{first}{close_{t-1}}\\right)}^2 \\times I_{second} \\times I_{first} \\ +\\\\ &amp; &amp; \\frac{1}{2} \\times 100 \\times \\$\\Gamma_{first} \\times \\ln{\\left(\\frac{second}{first}\\right)}^2 \\times I_{first} \\times I_{second} \\ +\\\\ &amp; &amp; \\frac{1}{2} \\times 100 \\times \\$\\Gamma_{second} \\times \\ln{\\left(\\frac{close_t}{second}\\right)}^2 \\times I_{second} \\times I_{first} \\ +\\\\ &amp; &amp; \\frac{1}{2} \\times 100 \\times \\$\\Gamma_{close_{t-1}} \\times \\ln{\\left(\\frac{close_t}{close_{t_1}}\\right)}^2 \\times (1 - I_{first}) \\times (1 - I_{second}) \\tag{1.1} \\end{eqnarray}\\] wobei, \\[\\begin{equation} \\$\\Gamma_i = \\Gamma \\times S_i^2 / 100 \\tag{1.2} \\end{equation}\\] Zu beachten gilt es insbesondere, dass der Payoff quadratisch auf Preisbewegungen reagiert. Dies führt dazu, dass es von Relevanz ist, ob zuerst der Kaufs- (untere Grenze) oder der Verkaufsschwelle erreicht wird. Die Ausführung zum jeweiligen Schwellenwert wird durch die Indikatorvariablen \\(I_{first}\\) und \\(I_{second}\\) modelliert. Damit es zu einer Ausfühung kommt müssen dabei folgende beiden Kriterien kumulativ erfüllt sein: Der Schwellenwert wird erreicht. Die vorangegangene Transaktion hat bei einem höheren Preis für Aktion an unterer Schwelle, resp. tieferen Preis für eine Aktion an der oberen Schwelle stattgefunden.1 1.3 Daten Als Datengrundlage stehen historische Open-, Low-, High- und Close- (adjustiert wie unadjustiert) Aktienpreise zur Verfügung. Diese sind öffentlich und können von Yahoo Finance bezogen werden. Zu Testzwecken wurden 9.5 Mio. historische Datenpunkte für die grössten rund 1600 Unternehmen weltweit angezogen. Die Datenqualität scheint auf den ersten Blick gut. Gewisse Datenbereinigungsschritte sind nötig. Die Verwendung bankinterner Daten ist aktuell nicht geplant. Dies hat den grossen Vorteil, dass die Daten auch problemlos auf Cloud-Infrastruktur (bsp. unabdingbar im Falle des Einsatzes von Neuronalen Netzen / Deep Learning Techniken) prozessiert werden kann. 1.4 Methoden Es werden verschiedene Methoden versucht und gegeneinander verglichen. Diese sind: 1.5 Disclaimer Etliche Untersuchungen haben sich bereits damit beschäftigt, kursrelevante Informationen vorauszusagen. Viele der dabei gefundenen Erkenntnisse sind nicht oder nur unter speziellen Voraussetzungen anwendbar. Es sei deshalb an dieser Stelle erwähnt, dass sich dem sowohl der Autor als auch dessen Auftraggeber bewusst ist. Auch wenn diese ausbleiben bietet diese Arbeit sowohl für den Autoren wie auch dessen Arbeitgeber die Chance, verschiedene moderne Analysetechniken/ -methoden an Finanzdaten anzuwenden, welche auch für andere Projekte wertvoll sein können. References "],
["daten-1.html", "Kapitel 2 Daten 2.1 Bezug und Umfang 2.2 Aufbereitung", " Kapitel 2 Daten 2.1 Bezug und Umfang 2.1.1 Aktienuniversum Für die vorliegende Analyse werden die Aktienpreise grosser Unternehmen weltweit herangezogen. Konkret werden alle Aktienkomponenten des “iShares MSCI World UCITS ETF” (vgl. Blackrock 2020) per 28. Februar 20202 verwendet. Dieser Exchange Traded Fund (ETF) besteht zu diesem Zeitpunkt aus 1639 Aktien, welche sich automatisiert auslesen lassen. Ferner stellt die Webseite des ETF Emittenten weitere Attribute zur Verfügung. Diese lauten am Beispiel Nesté wie folgt: Tabelle 2.1: Attribute der Nestlé Aktie per 28. Februar 2020 Attribut Wert Ticker NESN Name NESTLE SA Asset Class Equity Weight (%) 0.74265 Price 104.76 Shares 361433 Market Value 37862437 Notional Value 37862437 Sector Consumer Staples ISIN CH0038863350 Exchange Six Swiss Exchange Ag Location Switzerland Market Currency CHF Neben eindeutigen Identifiern wie “Ticker” und “ISIN” enthält der Datensatz auch Informationen zur “Asset Class” der Komponente. Für die vorliegende Analyse von Aktien ist hierbei lediglich die Ausprägung “Equity” zulässig. Die Kennzahlen “Weight”, “Market Value” und “Notional Value” geben Auskunft über die Grösse der betachteten Unternehmung und eignen sich auch zum Vergleich ebendieser. Zu beachten gilt, dass im Falle von Aktien der “Notional Value” dem “Market Value” entspricht und sich dieser bis auf rundungsbedingte Differenzen auch aus der Anzahl ausgegebener Titel mal Tagesendkurs (“Shares” x “Price”) ermitteln lässt. Als weitere Unterscheidungsmerkmale sind der Hauptbörsenplatz “Exchange” (27 unterschiedliche Ausprägungen), der Sitz “Location” (23 Ausprägungen) sowie die Währung “Market Currency” (14 Ausprägungen) aufgeführt. Alle drei Attribute bilden stark verwandte Informationen ab. Geschlüsselt auf Kontinente ergeben sich für den Börsensitz folgende Anteile: Nordamerika: 44% Europa: 28% Asien: 23% Australien: 5% Als letzes Attribut enthält der Datensatz Angaben zum “Sector” in welchem die jeweilige Unternehmung tätigt ist. Bis auf die Kategorien “Energy” und “Other” ist jeder der 12 aufgeführten Sektoren mit mindestens 5 Anteil vorhanden. Die beiden Sektoren mit dem höchsten Anteil sind “Industrials” und “Financials”. Abbildung 2.1: Anzahl im Datensatz vorhandene Titel je Sektor 2.1.2 Preisinformationen Für alle Titel werden in einem zweiten Schritt die historischen Aktienkurse inkl. Höchst- und Tiefstkurs bezogen. Diese Daten stehen via Yahoo Finance auf täglicher Basis zur freien Nutzung zur Verfügung (vgl. Yahoo 2020). Zu beachten gilt es hierbei, dass die Yahoo Ticker für ausserhalb der USA gehandelte Titel einen Suffix je Börsenplatz verwenden. Für die Analyse kommen so 9’212’052 tägliche Datenwerte für 1’636 Titel zusammen. Für 3 Aktien können keine Werte gefunden werden. Tabelle 2.2 zeigt einen Beispieleintrag für die Aktie von Nesté per 14. Februar 2019. Tabelle 2.2: Kursinformationen der Nesté Aktie per 12. April 2019 Attribut Wert Ticker NESN.SW Date 2019-04-12 Open 96.22 Low 95.11 High 96.6 Close 95.53 Adjusted 93.08 Volume 6991647 Die Werte “Open”, “Low”, “High” und “Close” zeigen Eröffnungs-, Tiefst-, Höchst- und Schlusskurs des Titels. Mit “Volume” werden die Anzahl gehandelter Titel am jeweiligen Tag angegeben. Unter “Adjusted” ist der um Dividendenausschüttungen korrigierte Schlusskurs aufgeführt.3 Die Werte in Tabelle 2.2 sind insofern speziell, als dass sie den letzten Tag vor dem Ex-Dividend Datum von Nestlé für 2019 betreffen. Das heisst, es sind die Kurse des letzten Tages, bevor die Aktie ohne die für das Jahr 2019 ausgeschüttete Dividende gahandelt wurde. Die Dividende betrug in jenem Jahr CHF 2.45 (vgl. Nestle 2020). Diese Differenz widerspiegelt sich in den Daten als Differenz des Close- und Adjusted-Preises. Da alle Werte (auch Open, Low, High und Close) am Folgetag ohne den Anspruch auf diese Dividende gehandelt werden, fallen diese typischerweise tiefer aus. Um eine Vergleichbarkeit der Renditen über die Zeit zu gewährleisten ist daher eine Anpassung der Werte mit Hilfe des Adjustement-Faktors nötig. Dieser ergibt sich als Quotient von Adjusted und Close Preis und wird auf allen Einträgen angewendet. Weiter erwähnenswert ist, dass Yahoo den Adjusted Kurs des jeweils aktuellsten Tages - ausser eben am Tag vor Ex-Dividend - mit auf aktuellen Kurs festlegt. Im Lauf der Zeit und mit neuen Ausschüttungen verändert sich damit auch die Historie der Adjusted Werte. Dies lässt sich zeigen, wenn der Beispieleintrag von Nestlé per 12. April 2019 nach der nächsten Dividendenausschüttung (27. April 2020) noch einmal abgerufen wird (vgl. Nestle 2020). Während alle Preise ausser “Adjusted” identisch ausgewiesen sind, hat sich dieser neu auf 90.72 verändert (vgl. Yahoo 2020). Mit der nächsten Dividenenausschüttung (voraussichtlich im April 2021) wird sich dieser Wert dann wieder ändern. Mit Hilfe der Adjustierung ist aber gewährleistet, dass die Werte vergleichbar bleiben. 2.2 Aufbereitung 2.2.1 Bereinigung Mit Yahoo Finance wird ein erfahrener und häufig verwendeter Datenanbieter gewählt. Der Blick auf einige Quantilskennzahlen der Rohdaten in Tabelle @ref(tab:summary_before_clean) zeigt aber, dass dennoch einige Datenprobleme ausgemacht werden können. ## Open Low High Close ## Min. : 0 Min. : 0 Min. : 0 Min. : 0 ## 1st Qu.: 12 1st Qu.: 12 1st Qu.: 12 1st Qu.: 12 ## Median : 36 Median : 35 Median : 36 Median : 36 ## Mean : 1564 Mean : 1548 Mean : 1589 Mean : 1564 ## 3rd Qu.: 157 3rd Qu.: 155 3rd Qu.: 158 3rd Qu.: 157 ## Max. :3750000 Max. :3600000 Max. :9046660 Max. :3656250 ## NA&#39;s :135267 NA&#39;s :135267 NA&#39;s :135267 NA&#39;s :135267 ## Adjusted ## Min. :-1.181e+20 ## 1st Qu.: 7.000e+00 ## Median : 2.600e+01 ## Mean : 7.285e+17 ## 3rd Qu.: 1.220e+02 ## Max. : 5.487e+22 ## NA&#39;s :135267 Die Behebung dieser Mängel und die damit einhergehende Bereinigung erfolgt in verschiedenen Schritten. Allen gemeinsam ist, dass die Bereinigung keinen Ausschluss der Daten zur Folge hat, sondern betroffene Werte als “Nicht verfügbar, (NA)” klassifiziert werden. Diese Unterscheidung ist insbesondere bei rollierender Betrachtung eines Zeitfensters der Vergangenheit von Bedeutung. Entfernen von fehlerhaften Adjustierungsdaten Eine Eigenschaft von Aktienpreisen ist es, dass sie nicht negativ sein können. Der Datensatz weist aber vereinzelt negative Adjusted Werte aus. Dies lässt sich auch durch die Dividendenbereinigung nicht erklären. Bei diesen Einträgen scheinen daher Datenfehler vorzuliegen. Erschwerend kommt hinzu, dass sich Fehler bei der Adjustierung nicht auf den jeweiligen Eintrag beschränken müssen. Aufgrund der Funktionsweise der Adjustierung (vgl. 2.1.2) ist ein vererben des Fehlers auf andere Einträge des jeweiligen Titels wahrscheinlich. Bei genauerer Betrachtung der Adjusted Werte fällt ferne auf, dass auch einige sehr sehr kleine (im Bereich \\(-10^{-6}\\)), wenn auch postive Werte gefunden werden können. Aus diesen Grund werden alle Ticker, welche in ihrer Historie einen Adjusted Preis von weniger als 0.001 aufweisen, ausgeschlossen. Entfernen von Einträgen mit unerwarterer Reihenfolge Die Werte für Open, Low, High und Close implizieren eine klare Reihenfolge. Keine anderer der Werte darf höher als das High oder kleiner als das Low sein. Ist dies der Fall, werden die entsprechenden Werte von der Analyse ausgeschlossen. Erkennen und Ausschluss von Tippfehlern Einzelne Einträge lassen sich als Tippfehler identifizieren. Der Titel “AV.L” weist per 09. August 2019 beispielsweise einen Low-Wert von 3.87 aus, währenddem alle andern Werte des gleichen Tages wie auch der benachbarten Tage bei ca. 380 liegen. Es liegt auf der Hand, dass dieser Wert um einen Faktor 100 falsch erfasst wurde. Solchen Fehlern wird begegnet, indem alle paarweisen Verhältnisse von Open, Low, High und Close Preis kleiner als 8 sein müssen. Andernfalls erfolgt ein Ausschluss der als Tippehler identifizierten Kennzahl. Fehlende Preisbewegungen innerhalb des Tages Der Aktienkurs eines grösseren Unternehens bewegt sich typischerweise auch an ruhigen Börsentagen immer ein wenig. Die vorhandenen Preise liegen mit der Genauigkeit mehrer Nachkommastellen vor. Unterscheiden sich hierbei Tagestiefst- und Tageshöchstpreis nicht, muss von einem Datenfehler ausgegangen werden. Einträge ohne Preisbewegung innerhalb des Tages werden daher von der Analyse ausgeschlossen. Fehlende Kursbewegungen über nacheinanderfolgende Börsentage Ähnlich wie bei den Preisbewegungen innerhalb des Tages verhält es sich auch bei Bewegungen über sich folgende Börsentage hinweg. Es ist zu erwarten, dass sich mindestens einer der fünf betrachteten Preise vom Vortag unterscheidet. Ist dies nicht der Fall, wird der Eintrag ausgeschlossen. Aussergewöhnlich hohe Preisbewegungen Es liegt in der Natur der Sache, dass sich Aktienkurse verändern. Grosse Kurssprünge sind bei Aktien sehr grosser Unternehmen wie sie in dieser Arbeit betrachtet werden aber selten. Die Chance, dass das Verhältnis zwischen adjustiertem Preis des Vortages und adjustiertem Preis des aktuellen Tages (und vice versa) um mehr als einen Faktor 2 unterscheidet erachten wir als kleiner, als dass es sich dabei um einen Datenfehler handelt. Entsprechende Einträge werden deshalb entfernt. Ausschluss von Extremwerten Die der vorliegenden Analyse zugrundliegende Payoff-Funktion ist abhängig von den Preisbewegungen einer Aktie innerhalb des Tages (vgl. 1.2). Speziell dabei ist, dass die Höhe der Bewegung nicht nur linear, sondern gar quadratisch Niederschlag findet. Dies führt dazu, dass die Analyse sehr sensitiv auf einzelne extreme Ausreisser reagiert. Hinzu kommt, dass es Ziel der Arbeit ist, Aussagen über Kursbewegungen innerhalb eines typischen Börsentages machen zu können. Eine Prognose von Werten an aussergewöhnlichen Tagen liegt ausserhalb des Geltungsbereiches der Analyse. Aus diesem Grund werden nach obigen Breinigungen für jede der fünf Preiskennzahlen die jeweils 1% extremsten Werte nach oben wie auch unten ausgeschlossen. Der Ausschluss erfolgt aufgrund der unterschiedlichen Preislevels der Aktien auf einer täglich auf den Eröffnungspreis indexierten Wert. Verfügbarkeit durchgängiger Historie Zur Prognose des zukünftigen Preisverlaufs könnte der direkt vorangegange Verlauf von Bedeutung sein. Ein durchgehend verfügbarer Preisverlauf ist im Datensatz allerdings nicht gewährleistet. Es werden daher alle Datensätze auschgeschlossen, für welche keine durchgängige Historie der vorangegangenen 10 Börsentage verfügbar ist. Zu beachten gilt es hierbei, dass nicht alle nachfolgenden Analyseansätze den ganzen Umfang dieser Historie tatsächlich auch verwenden. Um die Vergleichbarkeit der Analyseansätze zu gewährleisten, gilt dieser Ausschluss allerdings generell. Zusammenfassend lässt sich festhalten, dass der rohe Datensatz aus 9212052 Einträgen besteht. Davon weisen 135267 Zeilen vor Bereinigung einen fehlenden Wert auf. Nach Bereinigung erhöht sich dieser Wert auf 3639965. Für die Analyse bleiben somit 5572087 verwendbare Einträge. 2.2.2 Normalisierung Ein weiteres Problem, das sich beim Vergleich von Preisen verschiedener Aktien ergibt, ist deren unterschiedlichesw Preisniveau. Während eine Aktie bei USD 30 handelt, bewegt sich eine andere auf einem Niveau von USD 1000. Eine Vergleichbarkeit lässt sich dadurch herstellen, indem nicht absolue Preise, sondern relative Returns in der Analyse verwendet werden. Tatsächlich ist dies in der Payoff-Funktion (vgl. Gleichung (1.1)) bereits grösstenteils sichergestellt. Das absolute Preisniveau fliesst allerdings auch in die Berechnung des Gamma Cash (vgl. Gleichung @ref(eq:gamma_cash)) mit ein. Um auch hier eine Vergleichbarkeit der Werte sicherzustellen, wird bei allen nachfolgenden Analysen der Preis auf ein Niveau von 100 per adjustiertem Vortagesendkurs (dies entspricht dem letzten Neutralisierungszeitpunkt des Deltas) standardisiert. Sind all diese Schritte durchgeführt, lässt sich der Payoff berechnen. Da der Payoff - wie bereits erwähnt - quadratisch auf Preisveränderungen reagiert liegt, lohnt sich an dieser Stelle eine Untersuchung der Daten auf (zu) einflussreiche Beobachtungen. Ist der gesamte Payoff über alle Datensätze hinweg durch wenige Einträge dominiert könnten nachfolgende Analysen verfälscht werden. In diesem Fall besteht die Gefahr, dass insbesondere lernende Algorithmen insbesondere diese einzelnen Beobachtungen optimiert. Weniger einflussreiche aber häufiger realsierite Beobachtungen gingen darin unter. Dies ist inbesondere unter dem Gesichtspunkt von Relevanz, als dass vorliegende Analysen auf eine Prognose “normaler” Marktsituationen abzielt. Zur Veranschaulichung einflussreicher Beobachtungen zeigt Abbildung 2.2 die Lorenzkurve des Payoffs über alle für in der Analyse berücksichtigten Datensätze. Hierfür werden die Payoffs der einzelnen Einträge in aufsteigender Weise sortiert. Deren kumulativer Anteil (Y-Achse) wird dem kumulativen Anteil der Datenpunkte (X-Achse) gegenübergestellt. Bei geneu gleichverteiltem Beitrag jedes einzlenen Eintrages zum Payoff würde eine Gerade mit Steiguzng 45° resultieren. Je stärker der einfluss einzelner Beobachtungen, desto stärker konvex die Kurve. Abbildung 2.2: Lorenzkurve des Payoffs Im vorliegenden Fall weist die Lorenzkurve eine deutliche Konvexität auf. Zumal im Datensatz sowohl unterschiedliche Titel als auch ein langer Zeithorizont mit volatileren als auch ruhigeren Marktsituatioen repräsentiert ist, überrascht dieses Ergebnis nicht. Eine gewisse Vielfalt der Daten ist im Hinblick auf die Vorhersage von unterschiedlichen Kaufs- und Verkaufspreisen gar erwünscht. Sollen nochfolgende Algorithmen ja gerade versuchen, die unterschiedlichen Preisschwankungen zu prognostizieren. Andererseits zeigt die Darstellung auch, dass kein extremer Einfluss einzelner Einträge auszumachen ist. In diesen Fällen wäre die Kurve quasi auf dem unteren und rechten Rand der Grafik zu liegen gekommen. Es lässt sich damit der Schluss ziehen, dass die Bereinigung der Daten den erfolgreich war. Die bereinigten Daten zeigen sowohl die gewünschte Variabilität ohne dass dabei wenige Extremwerte das Ergebis verfälschen würden. 2.2.3 Adjustierung unterschiedlicher Volatilitäten Vorliegende Analyse basiert auf einer Vielzahl ganz unterschiedlicher Aktien. Es ist bekannt, dass die Preise unetrschiedlicher Aktien in gleichen Marktsituationen mit unterschiedlich starken Ausschlägen reagieren. Beispielsweise reagieren Unternehmen, welche im Luxusgüterbereich (Bsp. Richemont, Swatch) tätig sind stärker auf gute oder schlechte Marktinformationen als Anbieter im Bereich der Grundversorgung (Bsp. Nestle). Man spricht in diesem Zusammenhang auch von zyklischen und defensiven Werten. Die Adjustierungen der Preisbewegungungen auf eine vergleichbare Ausschlagshöhe / Volatilität könnte den nachfolgenden Algorithmen helfen, grundsätzlich ähnliche Verhaltensmuster unterschiedlicher Aktientypen zu erkennen. Diese Adjustierungbietet selbst auch verschiedenste Freiheitsgrade. Einige davon sind: Welcher Volatiltätsschätzer soll verwendet werden? Über welche Zeitdauer soll die Volatiltät geschätzt werden? Soll ein gleichgewichteter Schätzer verwendet werden, oder ein gewichteter? etc. Wir entscheiden uns für einen Volatiltätsindikator von Yang and Zhang (2000). Dieser ist in der Lage, nicht nur Close-to-Close Preise, sondern alle im Datensatz vorhandenen Preise - namentlich Open, High, Low und Close - zu berücksichtigen. Es handelt sich dabei um eine modifizierte Version des Garman und Klass Schätzers, der auch mit Opening Sprüngen umzugehen weiss. Für die Zeitdauer wählen wir 10 Tage. Dies ist einerseits ein gebräuchliches Standardfenster, zudem erfolgte auch die Datenaufbereitung (vgl. 2.2.1) auf der Bedingung, dass für jeden im Datensatz verbliebenen Eintrag dieses Fenster ohne Unterbruch vorhanden sein muss. Auf eine Gewichtung des Schätzers wird aus Gründen der Einfachheit verzichtet. Bezeichne \\(vol_{i, 10}\\) die beschriebene 10-Tages Volatilität jeden Eintrages, so erfolgt die Adjustierung aller Preisdaten mittels folgender Formel: \\[ P_{i, vol10} = (P_i - 100) / vol10_i + 100 \\] Für ein zweites adjustiertes Datenset wählen wir ein längerfristiges Volatilitätsmass. Die offensichtlichste Variante, dazu das Berechungsfenster von 10 auf 250 oder mehr Tage zu erhöhen greift allerdings zu kurz, zumal im Datensatz sowohl bereinigte wie auch bereits von Beginn weg fehlende Werte vorkommen. Eine Möglichkeit bestünde darin, diese mittels Interpolationsverfahren zu füllen. Wir entscheiden uns aber dafür, als langfristiges Volatilitätsmass den Medianwert aller annualisierten 10-Tages Volatilitäten des jeweiligen Ticker \\(j\\) zu wählen. \\[ P_{i, vol} = (P_i - 100) / median(vol10_i, j) + 100 \\] Nachfolgende Analysen werden teilweise mit und teilweise ohne diese Volatiltätsadjustierung durchgeführt und miteinander verglichen. Wo nicht anders vermerkt erfolgt die Analyse ohne Adjustierung. References "],
["infrastruktur-und-tools.html", "Kapitel 3 Infrastruktur und Tools 3.1 Cloud Setup 3.2 Verwendete Software", " Kapitel 3 Infrastruktur und Tools 3.1 Cloud Setup Gewisse der in dieser Arbeit gemachte Analysen sind sehr rechen- und Speicherintensiv. Sie stellen damit erhöhte Anforderungen an die zur Berechnung eingesetzte Infrastruktur. Für die vorliegenden Analysen stellte sich folgender Setup mit 3 unterschiedlichen virtuellen Maschinentypen (VMs) als geeignet heraus: Datenaufbreitung: Eine Maschine mit 4 Cores und mindestens 32 GB Memory zum Prototyping und die Datenaufbereitung Multi-CPU: Eine Maschine mit mindestens 16 CPUs zur Berechnung paralleliserbarer Aufgaben (z.B. KNN) auf grösseren Datensätzen GPU: Eine Maschine mit mindestens 16 GB RAM, 4 CPUs und eine für kleinere Machine Learning Probleme geigneten GPU (bsp. NVIDIA Tesla K80 oder NVIDIA Tesla M60). Im Rahmen dieser Arbeit wurden deshalb 3 der bekanntesten Cloud-Anbieter ausprobiert. Aus Sicht des Autors unterscheiden sich diese in ihrer Handhabung stärker als dies urspünglich vermutet hätte werden können. Der Aufbau eines geeigneten Infrastruktursetups stellte sich trotz auf den ersten Blick vorgefertigter Varianten als zeitintensiv heraus. Aus Sicht des Autors lohnt sich der Einsatz dieser Zeit allerdings bereits zu Beginn des Projektes. Die eingesetzte Zeit lässt sich später durch Zeiteinsparnisse aufgrund geeigneter Infrastruktur später wieder aufholen. Hinzu kommt, dass eine einmal gefundene und funktionierende Einstellung auch für spätere Projekte wieder einsetzt werden kann. Aus diesem Grund seien die gemachten Erkenntnisse an dieser Stelle festgehalten. Allen beschriebenen Lösungen gemein ist, dass sie ein für Studenten freies Start-Kontingent anbieten. Die Ausführungen beziehen sich auf diese. Ebenfalls allen Anbietern gemein ist, dass sich virtuelle Computer mit wenigen Klicks und dem gewünschten Betriebssystem erstellen lassen. Alle 3 getesteten Dienste bieten ferner neben normalen Instanzen auch sogenannte “Spot” Instanzen an. Diese unterscheiden sich von normalen VMs insofern, als dass es sich dabei um Einmalinstanzen handeln, welche nicht beendet und wieder hochgefahren werden können. Einmal beendet erlöschen Spot-Instanzen. Bei grosser Nachfrage nach Rechenkapazität können Spot Instanzen vom Anbieter zudem ohne Vorwarnung heruntergefahren werden. Sie eignen sich daher nur für nicht Unterbrechungsanfällige Prozesse. Im Gegensatz sind sie deutlich günstiger als reguläre Instanzen. 3.1.1 Microsoft Azure Die Cloud-Computing Dienste von Microsoft nennen sich Azure. Die Plattform bietet verschiedene vordefinierter Maschinentypen, welche sich im Wesentlichen in der Anzahl CPU, RAM und persistentem Speicher unterscheiden. Einige Maschinen bieten zudem Zugriff auf eine oder mehrere GPU. Beim Test zeigte sich hingegen, dass Account für Bildungseinrichtungen oft keine Maschinen verfügbar waren. Die Verfügbarkeit unterscheidet sich zudem je nach Tageszeit. Während am frühen Morgen Mitteleuropäischer Zeit manchmal Maschinen verfügbar waren, war dies weder am Nachmittag noch am Abend der Fall. Die grösste Maschine, welche beim Test über mehrere Tage hinweg erstellt werden konnte war eine NC6 Instanz mit 6 CPUs 56 GB Memory und einer NVIDIA Tesla K80 GPU. Insbesondere für CPU-lastige Analysen stellte sich dieser Setup als zu wenig gut heraus. Wie bei den anderen Anbietern auch unterscheidet sich die Verfügbarkeit der VMs je nach Region. Bei Azure kommt allerdings erschwerend hinzu, dass alle Regionen einzeln durchprobiert werden müssen. Spot Instanzen stehen zudem nur für gewisse Maschinentypen zur Verfügung. Auch für die oben erwähnte Instanz stand beim Test die Spot-Option nicht zur Verfügung. Spot Instanzen lassen sich vor allem dann gut nutzen, wenn Systemabbilder einfach erstellt und davon später wieder neue Instanzen erzeugt werden können. Azure unterscheidet sich hierbei von seinen Konkurrenten, als dass dies nicht einfach auf Knopfdruck erfolgt. Viel mehr muss manuell eine “Generalisierung” der Instanz vorgenommen werden (Vgl. https://docs.microsoft.com/en-us/azure/virtual-machines/windows/capture-image-resource). Die Beurteilung der Qualität der Dokumentation ist subjektiv. Aus Sicht des Autors ist diejenige von Azure weniger ausführlich und selbsterklärend als diejenige der anderen getesteten Kandidaten. 3.1.2 Google Cloud Die Cloud-Computing Dienste von Google nennen sich “Google Cloud”. Neu registrierende Kunden profitieren im Vergleich mit den anderen Anbietern vom höchsten kostenlosen Startguthaben (Azure mit Studenten Account: $100, AWS ($30) mit Github Starter Package ($70): $100, Google Cloud: $300). Die Management Oberfläche wirkt im Vergleich zu den Konkurrenten besser aufgeräumt. Die Dokumentation ist gut. Als einzige der getesteten Anbieter konnten die VMs zudem völlig individuell gestaltet werden (Anzahl CPU, Memory, Anzahl und Art GPU). Als Nachteil entpuppte sich im Test allerdings das komplexe Limitensystem. Fast alle Komponenten unterliegen verschiedenen Limiten. Um eine GPU hinzufügen zu können müssen die beispielsweise gelichzeitig die Limiten für “GPU global”, “GPU der gewählten Region” und “GPU des jeweiligen Typs” (Bsp. NVIDIA Tesla K80) erfüllt sein. Während die Standardeinstellungen Instanzen von bis zu 24 CPUs erlauben, sind noch mehr Einheiten nicht möglich. Die Standardeinstellung für GPUs beträgt gar 0. Eine Erhöhung der Quote kann im Management Portal beantragt werden. Im Test zeigte sich jedoch, dass sowohl Anträge zu Erhöhung der CPU wie auch GPU Limiten innerhalb weniger Minuten mit dem Verweis auf fehlende Zahlungshistorie automatisch abgelehnt wurden. Die Erstellung einer Instanz mit GPU Support gelang so auch über mehrere Tage hinweg nicht. Schriftliche Kontaktaufnahmen mit Bitte und Begründung der GPU Limite auf 1 wurden wiederholt mit Standard-Antworten abgelehnt. Auch nach telefonischer Kontaktaufnahme mit dem Support gelang es nicht, die Quote zu erhöhen. Das Problem stiess beim Mitarbeitenden von Google zwar auf Verständnis, er selbst konnte die Quote allerdings ebenfalls nicht erhöhen. 3.1.3 Amazon Web Services (AWS) Das Cloud Computing Angebot von Amazon nennt sich “Elastic Compute Cloud” (Amazon EC2). Als einziger der getesteten Anbieter fällt dieser Anbieter nicht durch nicht verfügbare Maschinentypen oder unzureichenden Limiten auf. Fernen können auch die meisten Maschinen (im Test bis 40 CPU) als Spot Instanzen ausgeführt werden. Da die Imageerstellung zudem sehr einfach auf Knopfdruck erfolgt, können diese sehr kostengünstig verwendet und wiederhergestellt werden. AWS fällt zudem durch eine sehr gute ausführliche Dokumentation auf. So ist beispielsweise die Erstellung eines von der Instanz unabhängigen persistenten Speichers (in AWS-Lingo: EBS) sowie der Prozess zum Mounten ebendieses Schritt-für-Schritt erklärt (https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html). Damit kann auch bei der Verwendung von Spot Instanzen dauerhaft gespeichert werden. Ferner sind auch die Preise im Vergleich zur Konkurrenz sehr transparent und wettbewerbsfähig. Da auch nach längeren Versuchen nur mit AWS der oben beschriebene Ziel-Infrastruktur einer Basis, Multi-CPU und einer einfachen GPU-Instanz erstellt werden konnte, wurde für die vorliegende Arbeit dieser Anbieter als Cloud Computing Lösung verwendet. 3.2 Verwendete Software Für die Analysen der vorliegenden Arbeit hat sich der Einsatz von R als auch Python bewährt. Es zeigte sich, dass beide Tools / Sprachen ihre Stärken in verschiedenen Bereichen haben. Grundsätzlich lässt sich dies so zusammenfassen, dass Datenaufbereitende Schritte in R durchgeführt wurden. Für das Training der neuronalen Netze stellten sich die entsprechenden Bibliotheken von Python als geeigneter heraus. 3.2.1 R / RStudio Server Die Datenaufbereitung und -bereinigung wie die Analysen ohne neuronale Netze wurden in der Sprache R und der Entwicklungsumgebung RStudio Server durchgeführt. Ersteres kann auf eine sehr breite Community mit entsprechend gut dokumentierten Beispielen in diversen Foren zurückgreifen. Ferner hat sich in den letzten Jahren unter dem Namen “tidyverse” eine Sammlung gut unterhaltener und dokumentierter Packages zum defacto-Standard etabliert. Dieses wird von Mitarbeitern der Firma RStudio unterhalten und steht wie für R üblich Open Source zur freien Benützung zur Verfügung. Gleiche Firma ist es auch, welche unter dem Namen RStudio Server eine hostbare Entwicklungsumgebung (IDE) anbietet. Diese wird in einer frei verfügbaren und einer kostenpflichtigen Variante angeboten. Für die Arbeit wurde die freie Version verwendet und es kam nie der Bedarf zum Upgrade auf die kostenpflichtige Version auf. Verschiedene Builds der gängigsten Linux Distributionen stehen auf der Unternehmenswebseite zur Verfügung (Vgl. https://rstudio.com/products/rstudio/download-server/). Der Zugriff auf die IDE erfolgt über den Browser. Das Look and Feel unterscheidet sich nicht von einer ebenfalls frei verfügbaren lokalen Installation. Auf diese Weise lässt sich Code direkt auf den erstellten Cloud Instanzen entwickeln, ohne auf den Komfort von Entwicklungsumgebungen unterstützen zu müssen. Schwächen bei der Verwendung von R zeigt sich in der inherenten Single-Threadigkeit des Tools. Die Multithreadigkeit kann mit Hilfe zusätzlicher Packages (z.B. parallel, pbapply) erreicht werden und ist insbesondere beim Einsatz auf der oben beschriebenen Multi-CPU Instanz von Relevanz. Zwar stehen mit Keras und Tensorflow auch entsprechende Packages für Deep Learning Ansätze zur Verfügung. Deren Verwendung im Zusammenspiel mit RStudio Server stellte sich im vorliegenden Anwendungsfall allerdings als sehr instabil heraus, was sich in mehrerer Abstürzen der Entwicklungsumgebung manifestierte. Da diese Packages selber lediglich Wrapper auf die gleichnamigen Python Libraries darstellen, stellte sich deren Verwendung direkt in Python als die bevorzugte Variante heraus. Zum Austausch zwischen den beiden Sprachen stellte sich dabei der Weg über zwischengespeicherte “Feather” Files als am geeignetsten heraus. Es handelt sich dabei um ein Format, das beide Sprachen auch für grössere Datenmengen sehr performant laden und speichern können. Tatsächlich kann in R / RStudio mit Hilfe des Packages “reticulate” auch Python Code direkt ausgeführt werden, resp. Objekte beider Sprachen automatisch ausgetauscht werden. Auf diesen Austausch wurde aus Einfachheitsüberlegungen allerdings verzichtet. Gewöhnungsbedürftig ist hingegen die matrix- und verktorbasierte progammierweise in R. Zwar sind gleiche Manipulation auch mit klassischer “loop-Ansätzen” möglich. Diese gehen allerdings mit erheblicher Performanceeinbussen speziell bei Datengrössen wie sie diese Arbeit verwenden einher. Performancekritische Funktionen können aber mit Hilfe des Packages “Rcpp” in C++ geschrieben und einfach mit R verknüpft werden. Dies wurde im Laufe der Arbeit für wenige kritische Funktionen verwendet. Insbesondere bei stark parallelisierbaren Aufgaben, nicht zuletzt auch, um grössere Datenmengen Memory-effizient ausführen zu können. 3.2.2 Python / (Ana)conda Für Python stand keine auf dem Server ausführbare Entwicklungsumgebung zur Verfügung. Da sich der Einsatz von Python im Rahmen des Projektes auf das Training neuronaler Netze beschränkte, reichte das Ausführen eine Jupyter Notebook Servers gut aus. Auf das Notebook lässt sich bei diesem Setup wiederum einfach via Webbrowser zugreifen. Ebenfalls als wertvoll zeigte sich die Verwendung von kapselbaren conda Environments. Diese erlauben projektspezifische Library Installationen sowohl für Python wie auch R-Bibliotheken. Es zeigt sich, dass das erwähnte Problem der Instabilität von Keras und Tensorflow bei direkter Verwendung der Python Libraries nicht auftauchte. Erwähnenswert ist hier aber, dass für die Verwendung der GPU Version auf dem System sowohl passende Grafiktreiber sowie die zur Tensorflow Version passende Version von CUDA installiert sein muss. Erwähnenswert ist dies inbesondere daher, da es auch bei aktuellster Tensorflow Bibliothek nicht die aktuellste CUDA Version sein durfte. Über die zu verwendenden Versionen gibt die Tensorflow Website Auskunft (https://www.tensorflow.org/install/gpu). "],
["analyse.html", "Kapitel 4 Analyse 4.1 Einfache Optimierungen", " Kapitel 4 Analyse Für die Analyse der Daten mit dem Ziel einen Kaufs- sowie einen Verkaufkurs zu prognostizieren, bei dem der Delta-Hedge nachgezogen werden soll, werden nachfolgend verschiedene Techniken eingesetzt. Diese sind: Einfache Optimierungen Klassifikationsverfahren / Nearest Neighbors Neuronale Netzwerke Allen Analysen gemein ist, dass jeweils gefundene Strategien mit der Referenzstrategie verglichen wird, welche keine innertägliche Anpassung des Deltas vorsieht. Eine weitere Gemeinsamkeit liegt darin, dass die verwendeten Daten keine Ausage über den Verlauf des Preises innerhalb des Tages zulassen. Inbesondere kann nicht ermittelt werden, ob zuerst eine obere oder eine untere Grenze Preisgrenze überschritten wurde. Da diese Reihenfolge aber wie in Kapitel 1.2 ausgeführt von Relevanz ist, wird für alle Analysen ein Ansatz verwendet, bei welchem zufällig bestimmt wird, ob am jeweiligen Tag zuerst eine Abwärts- oder eine Aufwärtsbewegung stattgefunden hat.[^Alternative denkbare Vorgehensweisen sind: Immer zuerst Aufwärtsbewegung, immer zuerst Abwärtsbewegung, immer die bezügl. Payoff schlechtere Reihenfolge oder immer die bezügl. Payoff bessere Variante] Auch ein mehrmaliges Erreichen der Kaufs- und Verkaufsschwelle ist innerhalb des Tages bei sehr fluktierenden Preisen in Realität denkbar. Es wären bezüglich Optimierung des Payoffs sogar sehr wünschenswerte Ereignisse. Auf die Berücksichtigung solcher Fälle wird in der Analyse allerdings verzichtet. Das Bewusstsein über deren Exsistenz ist aber bei der Interprätation der Ergebnisse dennoch inneressant, da die Payoffs der gefundenen Strategien diebezüglich als untere Grenzen des Payoffs betrachtet werden können. Eine weitere Gemeinsamkeit aller Analysen ist, dass der bereinigte Datensatz in ein Trainings- (80%) und ein Testdatensatz (20%) aufgeteilt wird. Diese Aufteilung erfolgt zufällig und wird für alle Analysen zwecks Vergleichbarkeit der Ergebnisse beibehalten. 4.1 Einfache Optimierungen Eine erste Möglichkeit, optimale Kaufs- und Verkaufspreise zu finden besteht darin, diese im Testdatensatz mittels einfacher Optimierung zu evaluieren. In einer ersten sehr einfachen Evaluation bietet es sich an, die Kaufs- und Verkaufsmarken als prozentuale Abweichungen vom aktuellen Preis festzulegen. Als Startgrösse bietet sich hierbei der “Open” Kurs des jeweiligen Tages an. Die resultierenden Payoffs bei einer solchen Festlegung lassen sich dann ins Verhältnis zum Referenzpayoff mit Glattstellung der Deltaposition bei Tagesende stellen. Ein Payoff-Verhältnis über 1 kennzeichnet damit eine Strategie, welche der Referenzstrategie überlegen ist. Verhälntisse unter 1 kennzeichnen unterlegene Strategien. Abbildung 4.1: Payoffvergleich bei symmetrischer Abweichung vom Eröffnungspreis Abbildung @ref(fig:symmetric_open_change) veranschaulicht dieses Verhältnis bei variierender symmetrischer Abweichung vom Startpreis. Lesebeispiel: file.exists(variation_factor_open_file_name) "]
]
