## Nearest Neighbor Ansätzue

```{r message=FALSE, warning=FALSE, echo=FALSE}
suppressPackageStartupMessages({
  library(tidyverse)
  library(RANN)
  library(RANN.L1)
  library(digest)
  library(pbmcapply)
  library(parallel)
  library(arrow)
  library(knitr)
  library(kableExtra)
})

source("R/03-analysis.R")
Rcpp::sourceCpp('src/calc_payoff_per_title.cpp')


data_wide_10 <- arrow::read_feather("data/steps/data_wide_10.feather")

set.seed(123456)
train_idx <- sample(x = nrow(data_wide_10), size = floor(0.8 * nrow(data_wide_10)))
test_idx <- setdiff(seq_len(nrow(data_wide_10)), train_idx)

both_first <- c("buy", "sell")[sample(c(1, 2), nrow(data_wide_10), replace = TRUE)]


```


Die ökomische Theorie scheint keinen offensichtlichen Grund zu liefern, weshalb die Volatilität des aktuellen Tages in genau 2 (oder x) Gruppen eingeteilt werden sollte. Auf der andern Seite haben bisherige Analsen gezeigt, dass die aktuelle Volatilität ein erklärender Faktor für die aktuelle Volatilität sein kann. Im vorliegenden Kapitel soll dieser Gedanke weiter verfolgt und ausgebaut werden. 


### Distanzmasse

Die Idee des Ansatzes dieses Kapitels besteht darin, nicht lediglich x vordefinierte Gruppen für symmetrische Abweichungen zu finden, viel mehr soll versucht werden, ähnliche Kursverläufe wie den aktuellen in der Vergangenheit zu finden und individuell darauf zu reagieren. Die zugrundliegende Hypothese dieses Vorgehens ist dabei, dass bei ausreichender Historie in der Vergangenheit ähnliche Muster erkannt werden können und daraus Rückschlüsse auf die Zukunft (genauer: die Kursentwicklung des aktuellen Tages) gezogen werden können. 

Da es sehr unwahrscheinlich ist, die genau gleichen Kursverläufe in der Histore wiederzufinden, muss ein Ähnlichkeitsmass, resp. ein Distanzmass definiert werden, um die Nähe der Kursverläufe zu bestimmen. Um diese zu berechnen formulieren wir für jeden Eintrag $i$ den bisherigen Kursverlauf als Vektor $hist$ in der Form. 

$$ hist_{i, t} = (Open_{i, t}, High_{i, t}, Low_{j, t+1}, Close_{i, t+1}, Open_{i, t+1}, ..., Close_{i, 1}, Open_{i, 0}) $$

Der zweite Index gibt dabei an, wieviele Tage der Vergangenheit hierbei mit einbezogen werden. Ein Index von 0 bezieht sich auf den zu prognostizierenden, aktuellen Tag. Um die Ähnlichkeit zweier Einträge $i$ und $j$ zu berechnen, bieten sich 2 Distanzmasse an:

1) Die Manhattan Distanz (auch L1-Norm)  
$$ 
\Vert hist_{i,t} - hist_{j,t}\rVert_1 =  \lvert Open_{i,t} - Open_{j,t} \rvert + \lvert High_{i,t} - High_{j,t} \rvert + \ldots + \lvert Open_{j,0} - Open_{j,0} \rvert
$$


2) Die Euklidische Distanz (auch L2-Norm)
$$ 
\Vert hist_{i,t} - hist_{j,t}\rVert_2  = \sqrt{(Open_{i,t} - Open_{j,t})^2 + (High_{i,t} - High_{j,t})^2 + \ldots + (Open_{j,0} - Open_{j,0})^2}
$$

Beide Masse sollen nachfolgend verwendet und verglichen werden.

### Setup und Berechnungsdauer

Mithilfe obiger Distanzmasse lassen sich für jeden Kursverlauf, die k ähnlichsten ander Kursverläufe im Datensatz ermitteln. Dazu seien zuerst aber eingige Übelegungen zur Berechnungskomplexität des Problems gemacht: Der vorliegende bereinigte Datensatz weist `r format(nrow(data_wide_10), big.mark = "'")` tägliche Kursverläufe auf. Soll jeder Kursverlauf mit jedem andern verglichen werden, so ergeben sich bei einem Brute-Force Ansatz `r format(nrow(data_wide_10), big.mark = "'")`^2 Distanzberechungen. Unter Berücksichtigung der Tatsache, dass das Problem symmetrisch ist und jeder Kursverlauf nicht mit sich selbst verglichen werden muss halbiert sich die Komplexität zwar um mehr als die Hälfte, bleibt aber so gross, dass es zum Zeitpunkt des Schreibens dieser Arbeit nicht innerhalb weniger Sekunden oder Minuten auf einem handelsüblichen Heim-Computer berechnet werden kann.

Die für die Nearest Neighbors Suche eingesetzten Bibliotheken greifen daher typischerweise auf sophistiziertere Vorgehensweisen zurück. Eine davon sieht die Verwendung von Multidimensionalen Suchbäumen (Search Trees, auch Kd-Trees). Diese gehen auf eine Idee von @bentley zurück. Diese basiert darauf, dass jede Dimension in 2 Teile aufgeteilt wird (z.B. beim Median). Dadruch wird der Raum in viele kleinere Sub-Räume aufgeteilt. Der Algorithmus macht sich dabei zu Nutze, dass er nicht den ganzen Raum absuchen muss, sondern beginnend im eigenen Sub-Raum hin zu den benachtbarten Räumen, bis dass die geforderte Anzahl nächster Nachbarn gefunden ist. Allerdings ist dabei sowohl der Aufbau des Baumes wie auch die Suche im Baum mit Berechnungszeit verbunden. Zeit lässt sich debei einsparen, wenn das Problem auf eine approximative Suche unter der Inkaufname eines Fehlers reduziert wird einsparen.

Die Anzahl der Dimensionen beeinflusst dabei die Berechnungszeit erheblich. Im vorliegenden Fall liegt diese bei der Berücksichtigung einer Historie von 10 Tagen à 4 Werten bei 41, wenn zusätzlich auch der (bekannte) Eröffnungspreis des aktuellen Tages mit einbezogen wird. Tests ergaben, dass die dafür benötigte Rechenzeit nicht praktikabel war. Die zu verwendende Zeitperiode wurde daher auf 3 Tage beschränkt. Dies resultiert in einer deutlichen Reduktion der Suchdimensionen auf 13.

Der vorliegende Fall unterscheidet sich von andern Nearest Neighbor Problemen dadurch, als dass für jeden Eintrag lediglich Kursverläufe der Vergangenheit bertrachtet werden sollten. Im Hinblick auf die Suchstrukur bedeutet dies, dass der KD-Tree nicht nur einmalig aufgebaut und danach für alle Verläufe auf Nachbarn durchsucht werden kann. Viel mehr muss der KD-Tree für jedes Datum mit allen Kursverläufen vor diesem Datum neu aufgebaut werden.[^Natürlich sind auch Mischlösungen denkbar, bei denen nur alle x Daten der Tree neu aufgebaut wird und dafür mehr Nachbarn ermittelt werden, die im Nachgang um nicht vorher realsierte Verläufe gefiltert würden. Dies hat aber das Problem, dass a) mehr Nachbarn ermittelt werden müssen und b) nicht 100% sichergestellt ist, dass die Anzahl "Reservenaachbarn" ausreichen.] Dieser spezielle Setup kommt einer Brute-Force Methode ihrerseits wieder entgegen, da aufgrund des Datums-Filter nicht stehts alle Einträge durchsucht werden müssten.

Beiden Ansätzen gemein ist hingegen, dass sie sich sehr gut parallelisieren lassen und Bibliotheken zur Verfügung stehen, welche diese Methoden effizient implementieren. Wir entscheiden uns um vorliegenden Fall für eine KD-Tree Implementation ohne Approximation. Auf einem Rechner mit 40 Cores (20 physisch, 20 Hyperthreading Cores) dauert die Berechnung von 50 Nachbarn beibe ca. 2  Stunden im Falle des euklidischen Distanzmasses und ca. 3 Stunden im Falle der Manhattan Distanz. Dieser Hohe Anspruch eines KNN-Vorgehens an die Rechenleistung sollte bei der späteren Abwägung verschiedener Algorithmen mit berücksichtigt werden. Erwähnenswert scheint hingegen auch, dass eine allfälligen Anwendung der Methode später nur wenige Titel (resp. nur diejenigen des aktuellen Tages) berechnet werden müssten. Dies ist auch unter der Verwendung einer KNN-Methode in wenigen Sekunden möglich. 


### Bestimmung Kaufs- und Verkaufspreise


```{r, knn-calculation, echo=FALSE}

data_wide_10 <- arrow::read_feather("data/steps/data_wide_10.feather")
data_knn_3 <- select(data_wide_10, c("Date", levels(interaction(c("Open_", "Low_", "High_", "Close_"), 1:3, sep="")), "Open_0")) %>%
  arrange(Date)

nn_3_eucl_path <- c("data/knn/nn_3_eucl_idx.feather", "data/knn/nn_3_eucl_dists.feather")
if (!all(map_lgl(nn_3_eucl_path, ~file.exists(.)))) {
  
  nn_3_eucl <- find_nn(data = data_knn_3, distance = "euclidean", k = 50, mc.cores = ceiling(detectCores() * 0.51))
  write_feather(as_tibble(nn_3_eucl$nn.idx), nn_3_eucl_path[1])
  write_feather(as_tibble(nn_3_eucl$nn.dists), nn_3_eucl_path[2])
  
  nn_3_eucl_idx <- read_feather(nn_3_eucl_path[1])
  nn_3_eucl_idx_hash <- digest(nn_3_eucl$idx)
  
  nn_3_eucl_dists <- read_feather(nn_3_eucl_path[2])
  nn_3_eucl_dists_hash <- digest(nn_3_eucl_dists)
  
} else {
  nn_3_eucl_idx <- read_feather(nn_3_eucl_path[1])
  nn_3_eucl_dists <- read_feather(nn_3_eucl_path[2])
}

nn_3_manh_path <- c("data/knn/nn_3_manh_idx.feather", "data/knn/nn_3_manh_dists.feather")
if (!all(map_lgl(nn_3_manh_path, ~file.exists(.)))) {
  
  nn_3_manh <- find_nn(data = data_knn_3, distance = "manhattan", k = 50, mc.cores = ceiling(detectCores() * 0.51))
  write_feather(as_tibble(nn_3_manh$nn.idx), nn_3_manh_path[1])
  write_feather(as_tibble(nn_3_manh$nn.dists), nn_3_manh_path[2])
  
  nn_3_manh$idx <- read_feather(nn_3_manh_path[1])
  nn_3_manh_idx_hash <- digest(nn_3_manh$idx)
  
  nn_3_manh$dists <- read_feather(nn_3_manh_path[2])
  nn_3_manh_dists_hash <- digest(nn_3_manh$dists)
  
} else {
  nn_3_manh_idx <- read_feather(nn_3_manh_path[1])
  nn_3_manh_dists <- read_feather(nn_3_manh_path[2])
}

```


Sind die Nachbarn eines Eintrages ermittelt lässt sich eine Prognose für den weiteren Kursverlauf des aktuellen Eintrages ableiten. Dazu wird aus den bekannten Kursverläufen der Nachbarn für den kommenden Börstentag mittels geeigneter Metrik ein Prognosewert abgeleitet. Abbildung \@ref(fig:example_nearest_neighbor_outlook) illustriert dieses Vorgehen anhand eines Beispiels. Die Grafiken zeigen die Kursverläufe des aktuellen Eintrages (rot) sowie diejenigen der nächsten 5 Nachbarn für den aktuellen zu prognostizierenden Tag (t) sowie die 3 jeweils vorangegangenen Handelstage (t-1, ... , t-3). Zur Bestimmung der Nähe werden dabei nur die Tage in der Vergangenheit sowie der Open-Preis des aktuellen Tages verwendet. Die gestrichelte rote Linie zeigt den auf Basis der Nachbarn prognostizierten Wert.

``` {r, fig.cap='Exemmplarische Kursprognose auf Bassis näcshter Nachhbarn fig.asp=1, fig.pos = '!H', anef-data, echo=FALSE, message=FALSE, warning=FALSE}
k <- 5
curr_idx <- 432100


cols_3_incl_0 <- as.vector(outer(c("Open_", "Low_", "High_", "Close_"), 0:3, FUN = paste0))

data_knn_3_incl_0 <- select(data_wide_10, c("Date", cols_3_incl_0)) %>%
  arrange(Date)


data_wide_curr <- data_knn_3_incl_0[curr_idx,]
data_wide_nn_eucl <- data_knn_3_incl_0[unlist(nn_3_eucl_idx[curr_idx, ], use.names = FALSE)[seq_len(k)], ]
data_wide_nn_manh <- data_knn_3_incl_0[unlist(nn_3_manh_idx[curr_idx, ], use.names = FALSE)[seq_len(k)], ]


# data_wide_nn - map_df(seq_len(k), ~data_wide_curr)

# plot_nn(data_wide_curr = data_wide_curr, data_wide_nn = data_wide_nn_eucl)
plot_nn(data_wide_curr = data_wide_curr, data_wide_nn = data_wide_nn_manh)

```

Werden die prognostizierten Werte für Höchst- und Tiefstpreise als Kaufs- resp. Verkaufsschranken gewählt, lässt sich der Payoff berechnen und mit demjenigen der Referenzstartegie vergleichen. Wie bereits ausgeführt werden bei der Suche nach Nachbarn nur Einträge der Vergangenheit berücksichtigt. Die Menge an verfügbaren Vergleichsverläufen nimmt damit ständig zu. Stellt man Payoff-Vergleich für das Training Set über die Zeit dar, sollte damit ein gewisser Lerneffekt der Methode sichtbar werden. Wie in Abbildung \@ref(fig:hist-knn) ersichtlich zeigt diese Kurve insbesondere drei Eigenschaften:

1. Hohe Volatilität des Vergleichsfaktor zu Beginn
1. Steigende Faktorhöhe mit fortlaufender Zeitdauer
1. Abflachung im Laufe der Zeit auf ein stabiles Level

Beide Eigenschaften decken sich mit der Intuition. Da zu Beginn der Datenreihe nur sehr wenige Vergleichsverläufe zur Verfügung stehen, reagiert die Kurve sehr sensitiv auf einzelne Beobachtungen und schlägt entsprechend aus. Dies glättet sich im Laufe der Zeit mit dem Vorhandensein von mehr Vergleichsmöglichkeiten. Die zweite Eiegenschaft des steigenden Faktors zeigt, dass der erhoffte Lerneffekt einzutreten scheint. Tatsächlich scheinen damit ähnliche Kursverläufe der Vergangenheit zukünftige Entwicklungen teilweise erklären zu können. Dies deckt sich mit der Erkenntnis des vergangenen Kapitels. Anders als zuvor kann dieser Lernmechanismus aber sehr individuell und nicht beschränkt auf 2 Gruppen erfolgen. Die dritte Eigenschaft zeigt, dass dieser Lerneffekt aber nach gewisser Zeit gesättigt ist.

``` {r, fig.cap='Exemmplarische Kursprognose auf Bassis näcshter Nachhbarn fig.asp=1, fig.pos = '!H', anehist-knndasdasdf, echo=FALSE, message=FALSE, warning=FALSE}
data_train <- select(data_knn_3_incl_0, Date, Close_1, Open_0, Low_0, High_0, Close_0)[train_idx, ]
pred_train <- pred_nn(select(data_train, Open_0, Low_0, High_0), as.matrix(nn_3_eucl_idx[train_idx, seq_len(k)]))

plot_ratio_history(data_train %>% rename(Low = "Low_0", High = "High_0"), data_pred = pred_train, both_first = both_first[train_idx], use_spread = TRUE)

```

Während sich bisherige Ausführungen auf die Analyse mit 5 Nachbarn und euklidischer Distanz beziehen, sind bei der Ermittlung der Prognosewerte auch andere Parametrisierungen denkbar. Neben der Unterscheidung des Distanzmasses sind dies insbesondere die Anzahl der zu berücksichtigenden Nachbarn und die Metrik zur Prognoseermittlung.

- **Anzahl Nachbarn:**  
In der vorliegenden Arbeit werden die 50 nächsten Nachbarn jedes Eintrages ermittelt. Einmal ermittelt lassen sich davon auch weniger verwenden. Damit kann sehr einfach der Einfluss der Anzahl berücksichtigter Nachbarn auf die Prognosequalität ermittelt werden. Denkbar sind dabei grundsätzlich unterschiedliche Ergebnisse. Einerseits lässt sich argumentieren, dass bei der Verwendung nur weniger Nachbarn diejenigen mit effektiv höchster Ähnlichkeit verwendet werden. Insbesondere bei Marktbewegungen die realtiv selten beobachtet werden würden so weitere Nachbarn das Ergebnis nicht verzerren. Umgekehrt lässt sich andererseits argumentieren, dass bei häufigeren Marktsituationen die Berücksichtigung und Mittelung von mehr Nachbarn zu einem unverzerrteren Ergebnis führen könnten. Schliesslich könnte sich als drittes Ergebnis auch eine Konfigration mit "mittlerer" Anzahl Nachbarn als am geeignetsten herausstellen, wenn beide vorherhigen Argumentationen verschmolzen werden. Aus diesem Grund analysiert und vergleicht die vorliegende Arbeit die Ergebnisse bei der VErwenung von 5, 20 und 50 Nachbarn.

- **Metrik der Prognoseermittlung:**  
Eine erste Möglichkeit auf Basis der zukünftigen Kursentwicklungen der nächsten Nachbarn den jenigen des aktuellen Eintrages vorherzusagen liegt darin, deren Tiefst-, Höchst- und Schlusskurs des kommenden Tages zu mitteln. Alternativ zur einfachen Mittelwertbildung sind auch anderer Verfahren denkbar. Beispielsweise wäre auch die Berücksichtigung der Distanz als Gewichtungsfaktor denkbar. Aus Gründen der Einfachheit verzichten wir an dieser Stelle allerdings darauf und verwenden neben dem gleichgewichteten Mittelwert den Median als zweites Prognosemass. Dieses könnte inbesondere bei kleinerer Anzahl Nachbarn weniger sensitiv auf Ausreisser reagieren.  

Die zweite hier betrachtete Möglichkeit besteht darin, die Kaufs- und Verkaufspreise nicht direkt vorauszusagen, sondern als Abweichung vom Eröffnungskurs zu modellieren. Hierzu wird für jeden Nachbarn die Differenz von Eröffnungs- und Tiefstpreis resp. Eröffnungs- und Höchstpreis berechnet. Die entsprechende Metrik (Mittelwert oder Median) wird dann auf diese Werte angewandt und auf den Tatsächlichen Eröffnungspreis zu zu analysierenden Titels appliziert. Dies hat den Vorteil, dass der bekannte Eröffnungspreis keiner Unsicherheit mehr unterliegt.


Tabelle \@ref(tab:knn-payoff-factors) stellt die Ergebnisse aller Parametrisierungen für des Testingset einander gegenüber. Es zeigt sich, dass eine direkte Prognose der Kaufs- und Verkaufspreise derjenigen einer Prognose der Veränderung im Vergleich zum Eröffnungspreis klar unterlegen ist. Bezüglich Distanzmass lässt sich kein eindeutiger Gewinner feststellen, beide Masse weisen ähnliche Performance aus. Ähnliches gilt für die Anzahl der berücksichtigten Nachbarn.

Allen Ergebnissen gemein ist hingegen, dass sie der einfachen Strategie des Vorkapitels nicht überlegen sind. Im Gegenteil fallen die Payoffs im Vergleich trotz deutlich höherem Berechnungsaufwand in der Tendenz schlechter aus, wenn auch der Payoff der Referenzstrategie weiter deutlich geschlagen wird. Die Hoffnung, dass mit der Individualisierung der einzelnen Einträge mehr kursrelevante Information extrahiert werden kann, bestätigt sich nicht. Ein Grund könnte in der durch die Berechnungskomplexität beschränkte Begrenzung auf ein Zeitfenster von 3 vorangegangenen Tagen sein.

```{r} 

factor_config <- expand_grid(aggr = c("mean", "median"), k = c(5, 20, 50)) %>% as.list()
calc_nn_factors <- function(data, factor_config, nn_idx, use_spread, both_first, test_idx, mc.cores = 3) {
  
  factor_names <- paste("payoff_factor", factor_config$aggr, factor_config$k, sep = "_")
  pbmclapply(transpose(factor_config), function(curr_config, nn_idx, use_spread) {
    if (curr_config$aggr == "mean") {
      fct <- function(x) mean(x, na.rm = TRUE)
    } else if (curr_config$aggr == "median") {
      fct <- function(x) median(x, na.rm = TRUE)
    } else {
      stop("Unexpected aggregation")
    }
    
    calc_nn_payoff_factor(
      data = select(data, Close_1, Open_0, Low_0, High_0, Close_0)[test_idx, ],
      nn_idx = nn_idx[test_idx, seq_len(curr_config$k)],
      both_first = both_first[test_idx],
      fct = fct,
      use_spread = use_spread
    )
  }, use_spread = FALSE, nn_idx = nn_idx, mc.cores = mc.cores) %>% set_names(factor_names)
}

nn_factors_eucl <- calc_nn_factors(data_knn_3_incl_0, factor_config, nn_3_eucl_idx, FALSE, both_first, test_idx)
nn_factors_eucl_spread <- calc_nn_factors(data_knn_3_incl_0, factor_config, nn_3_eucl_idx, TRUE, both_first, test_idx)


# payoff_factor_test_eucl_spread <- calc_nn_payoff_factor(
#   data = select(data_knn_3_incl_0, Close_1, Open_0, Low_0, High_0, Close_0)[test_idx, ],
#   nn_idx = nn_3_eucl_idx[test_idx, seq_len(k)],
#   both_first = both_first[test_idx],
#   fct = function(x) mean(x, na.rm = TRUE),
#   use_spread = TRUE
# )
# 
# payoff_factor_test_manh <- calc_nn_payoff_factor(
#   data = select(data_knn_3_incl_0, Close_1, Open_0, Low_0, High_0, Close_0)[test_idx, ],
#   nn_idx = nn_3_manh_idx[test_idx, seq_len(k)],
#   both_first = both_first[test_idx],
#   fct = function(x) mean(x, na.rm = TRUE),
#   use_spread = FALSE
# )
# 
# 
# payoff_factor_test_manh_spread <- calc_nn_payoff_factor(
#   data = select(data_knn_3_incl_0, Close_1, Open_0, Low_0, High_0, Close_0)[test_idx, ],
#   nn_idx = nn_3_manh_idx[test_idx, seq_len(k)],
#   both_first = both_first[test_idx],
#   fct = function(x) mean(x, na.rm = TRUE),
#   use_spread = TRUE
# )
# 
# 
# payoff_factor_test_manh_spread_median <- calc_nn_payoff_factor(
#   data = select(data_knn_3_incl_0, Close_1, Open_0, Low_0, High_0, Close_0)[test_idx, ],
#   nn_idx = nn_3_manh_idx[test_idx, seq_len(k)],
#   both_first = both_first[test_idx],
#   fct = function(x) median(x, na.rm = TRUE),
#   use_spread = TRUE
# )

```

```{r}
long_dt <-rbind(mtcars, mtcars)
kable(long_dt)%>%
  add_header_above(c(" ", "Group 1" = 5, "Group 2" = 6))%>%
  kable_styling(latex_options =c("repeat_header"))
```






