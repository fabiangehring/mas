<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Kapitel 4 Analyse | Die Bestimmung optimaler Kauf- und Verkaufspreise von Basiswerten zur Wahrung der täglichen Delta-Neutralität beim Handel von Aktionoptionen mit Hilfe verschiedener Machine Learning Methoden</title>
  <meta name="description" content="Kapitel 4 Analyse | Die Bestimmung optimaler Kauf- und Verkaufspreise von Basiswerten zur Wahrung der täglichen Delta-Neutralität beim Handel von Aktionoptionen mit Hilfe verschiedener Machine Learning Methoden" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Kapitel 4 Analyse | Die Bestimmung optimaler Kauf- und Verkaufspreise von Basiswerten zur Wahrung der täglichen Delta-Neutralität beim Handel von Aktionoptionen mit Hilfe verschiedener Machine Learning Methoden" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Kapitel 4 Analyse | Die Bestimmung optimaler Kauf- und Verkaufspreise von Basiswerten zur Wahrung der täglichen Delta-Neutralität beim Handel von Aktionoptionen mit Hilfe verschiedener Machine Learning Methoden" />
  
  
  

<meta name="author" content="Fabian Gehring, Zürcher Hochschule für Angewandte Wissenschaften" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="infrastruktur-und-tools.html"/>
<link rel="next" href="summary-and-outlook.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Management Summary</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Einleitung</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#motivation"><i class="fa fa-check"></i><b>1.1</b> Motivation</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#forschungsfrage"><i class="fa fa-check"></i><b>1.2</b> Forschungsfrage</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#daten"><i class="fa fa-check"></i><b>1.3</b> Daten</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#methoden"><i class="fa fa-check"></i><b>1.4</b> Methoden</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#disclaimer"><i class="fa fa-check"></i><b>1.5</b> Disclaimer</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="daten-1.html"><a href="daten-1.html"><i class="fa fa-check"></i><b>2</b> Daten</a><ul>
<li class="chapter" data-level="2.1" data-path="daten-1.html"><a href="daten-1.html#bezug-und-umfang"><i class="fa fa-check"></i><b>2.1</b> Bezug und Umfang</a><ul>
<li class="chapter" data-level="2.1.1" data-path="daten-1.html"><a href="daten-1.html#aktienuniversum"><i class="fa fa-check"></i><b>2.1.1</b> Aktienuniversum</a></li>
<li class="chapter" data-level="2.1.2" data-path="daten-1.html"><a href="daten-1.html#preisinformationen"><i class="fa fa-check"></i><b>2.1.2</b> Preisinformationen</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="daten-1.html"><a href="daten-1.html#aufbereitung"><i class="fa fa-check"></i><b>2.2</b> Aufbereitung</a><ul>
<li class="chapter" data-level="2.2.1" data-path="daten-1.html"><a href="daten-1.html#bereinigung"><i class="fa fa-check"></i><b>2.2.1</b> Bereinigung</a></li>
<li class="chapter" data-level="2.2.2" data-path="daten-1.html"><a href="daten-1.html#normalisierung"><i class="fa fa-check"></i><b>2.2.2</b> Normalisierung</a></li>
<li class="chapter" data-level="2.2.3" data-path="daten-1.html"><a href="daten-1.html#adjustierung-unterschiedlicher-volatilitäten"><i class="fa fa-check"></i><b>2.2.3</b> Adjustierung unterschiedlicher Volatilitäten</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="infrastruktur-und-tools.html"><a href="infrastruktur-und-tools.html"><i class="fa fa-check"></i><b>3</b> Infrastruktur und Tools</a><ul>
<li class="chapter" data-level="3.1" data-path="infrastruktur-und-tools.html"><a href="infrastruktur-und-tools.html#cloud-setup"><i class="fa fa-check"></i><b>3.1</b> Cloud Setup</a><ul>
<li class="chapter" data-level="3.1.1" data-path="infrastruktur-und-tools.html"><a href="infrastruktur-und-tools.html#microsoft-azure"><i class="fa fa-check"></i><b>3.1.1</b> Microsoft Azure</a></li>
<li class="chapter" data-level="3.1.2" data-path="infrastruktur-und-tools.html"><a href="infrastruktur-und-tools.html#google-cloud"><i class="fa fa-check"></i><b>3.1.2</b> Google Cloud</a></li>
<li class="chapter" data-level="3.1.3" data-path="infrastruktur-und-tools.html"><a href="infrastruktur-und-tools.html#amazon-web-services-aws"><i class="fa fa-check"></i><b>3.1.3</b> Amazon Web Services (AWS)</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="infrastruktur-und-tools.html"><a href="infrastruktur-und-tools.html#verwendete-software"><i class="fa fa-check"></i><b>3.2</b> Verwendete Software</a><ul>
<li class="chapter" data-level="3.2.1" data-path="infrastruktur-und-tools.html"><a href="infrastruktur-und-tools.html#r-rstudio-server"><i class="fa fa-check"></i><b>3.2.1</b> R / RStudio Server</a></li>
<li class="chapter" data-level="3.2.2" data-path="infrastruktur-und-tools.html"><a href="infrastruktur-und-tools.html#python-anaconda"><i class="fa fa-check"></i><b>3.2.2</b> Python / (Ana)conda</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="analyse.html"><a href="analyse.html"><i class="fa fa-check"></i><b>4</b> Analyse</a><ul>
<li class="chapter" data-level="4.1" data-path="analyse.html"><a href="analyse.html#einfache-optimierungen"><i class="fa fa-check"></i><b>4.1</b> Einfache Optimierungen</a><ul>
<li class="chapter" data-level="4.1.1" data-path="analyse.html"><a href="analyse.html#ohne-berücksichtigung-der-marktvolatilität"><i class="fa fa-check"></i><b>4.1.1</b> Ohne Berücksichtigung der Marktvolatilität</a></li>
<li class="chapter" data-level="4.1.2" data-path="analyse.html"><a href="analyse.html#mit-berücksichtigung-der-marktvolatilität"><i class="fa fa-check"></i><b>4.1.2</b> Mit Berücksichtigung der Marktvolatilität</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="analyse.html"><a href="analyse.html#nearest-neighbor-ansätze"><i class="fa fa-check"></i><b>4.2</b> Nearest Neighbor Ansätze</a><ul>
<li class="chapter" data-level="4.2.1" data-path="analyse.html"><a href="analyse.html#distanzmasse"><i class="fa fa-check"></i><b>4.2.1</b> Distanzmasse</a></li>
<li class="chapter" data-level="4.2.2" data-path="analyse.html"><a href="analyse.html#setup-und-berechnungsdauer"><i class="fa fa-check"></i><b>4.2.2</b> Setup und Berechnungsdauer</a></li>
<li class="chapter" data-level="4.2.3" data-path="analyse.html"><a href="analyse.html#bestimmung-kaufs--und-verkaufspreise"><i class="fa fa-check"></i><b>4.2.3</b> Bestimmung Kaufs- und Verkaufspreise</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="analyse.html"><a href="analyse.html#neuronale-netzwerke"><i class="fa fa-check"></i><b>4.3</b> Neuronale Netzwerke</a><ul>
<li class="chapter" data-level="4.3.1" data-path="analyse.html"><a href="analyse.html#diskretisierung"><i class="fa fa-check"></i><b>4.3.1</b> Diskretisierung</a></li>
<li class="chapter" data-level="4.3.2" data-path="analyse.html"><a href="analyse.html#architektur"><i class="fa fa-check"></i><b>4.3.2</b> Architektur</a></li>
<li class="chapter" data-level="4.3.3" data-path="analyse.html"><a href="analyse.html#ergebnisse"><i class="fa fa-check"></i><b>4.3.3</b> Ergebnisse</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="summary-and-outlook.html"><a href="summary-and-outlook.html"><i class="fa fa-check"></i><b>5</b> Fazit und Ausblick</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Die Bestimmung optimaler Kauf- und Verkaufspreise von Basiswerten zur Wahrung der täglichen Delta-Neutralität beim Handel von Aktionoptionen mit Hilfe verschiedener Machine Learning Methoden</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="analyse" class="section level1">
<h1><span class="header-section-number">Kapitel 4</span> Analyse</h1>
<p>Für die Analyse der Daten mit dem Ziel einen Kaufs- sowie einen Verkaufkurs zu prognostizieren, bei dem der Delta-Hedge nachgezogen werden soll, werden nachfolgend verschiedene Techniken eingesetzt. Diese sind:</p>
<ul>
<li>Einfache Optimierungen</li>
<li>Nearest Neighbors (KNN)</li>
<li>Neuronale Netzwerke</li>
</ul>
<p>Allen Analysen gemein ist, dass jeweils gefundene Strategien mit der Referenzstrategie verglichen wird, welche keine innertägliche Anpassung des Deltas vorsieht. Eine weitere Gemeinsamkeit liegt darin, dass die verwendeten Daten keine Aussage über den Verlauf des Preises innerhalb des Tages zulassen. Inbesondere kann nicht ermittelt werden, ob zuerst eine obere oder eine untere Preisgrenze überschritten wurde. Da diese Reihenfolge aber wie in Kapitel <a href="intro.html#forschungsfrage">1.2</a> ausgeführt von Relevanz ist, wird für alle Analysen ein Ansatz verwendet, bei welchem zufällig bestimmt wird, ob am jeweiligen Tag zuerst eine Abwärts- oder eine Aufwärtsbewegung stattgefunden hat.[^Alternative denkbare Vorgehensweisen sind: Immer zuerst Aufwärtsbewegung, immer zuerst Abwärtsbewegung, immer die bezügl. Payoff schlechtere Reihenfolge oder immer die bezügl. Payoff bessere Variante.] Auch ein mehrmaliges Erreichen der Kaufs- und Verkaufsschwelle ist innerhalb des Tages bei sehr fluktierenden Preisen in Realität denkbar. Es wären bezüglich Optimierung des Payoffs sogar sehr wünschenswerte Ereignisse. Auf die Berücksichtigung solcher Fälle wird in der Analyse allerdings verzichtet. Das Bewusstsein über deren Exsistenz ist aber bei der Interprätation der Ergebnisse interessant, da die Payoffs der gefundenen Strategien diesbezüglich als untere Grenzen des Payoffs betrachtet werden können.</p>
<p>Eine weitere Gemeinsamkeit aller Analysen ist, dass der bereinigte Datensatz in ein Trainings- (80%) und ein Testdatensatz (20%) aufgeteilt wird. Diese Aufteilung erfolgt zufällig und wird für alle Analysen zwecks Vergleichbarkeit der Ergebnisse beibehalten.</p>
<div id="einfache-optimierungen" class="section level2">
<h2><span class="header-section-number">4.1</span> Einfache Optimierungen</h2>
<div id="ohne-berücksichtigung-der-marktvolatilität" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Ohne Berücksichtigung der Marktvolatilität</h3>
<p>Eine erste Möglichkeit, optimale Kaufs- und Verkaufspreise zu finden, besteht darin, diese im Trainingsdatensatz mittels einfacher Optimierung zu ermitteln. In einer ersten sehr einfachen Evaluation sollen Kaufs- und Verkaufsmarken als prozentuale Abweichungen vom aktuellen Eröffnungspreis festgelegt werden. Die resultierenden Payoffs bei einer solchen Festlegung lassen sich dann ins Verhältnis zum Referenzpayoff mit Ausgleich per Tagesende stellen. Ein Payoff-Verhältnis über 1 kennzeichnet damit eine Strategie, welche der Referenzstrategie überlegen ist. Verhältnisse unter 1 kennzeichnen unterlegene Strategien.</p>
<div class="figure"><span id="fig:symmetric-open-change"></span>
<img src="Masterarbeit_files/figure-html/symmetric-open-change-1.png" alt="Payoffvergleich bei symmetrischer Abweichung vom Eröffnungspreis (ohne Berücksichtigung Marktvolatilität)" width="672" />
<p class="caption">
Abbildung 4.1: Payoffvergleich bei symmetrischer Abweichung vom Eröffnungspreis (ohne Berücksichtigung Marktvolatilität)
</p>
</div>
<p>Abbildung <a href="analyse.html#fig:symmetric-open-change">4.1</a> veranschaulicht dieses Verhältnis bei variierender symmetrischer Abweichung vom Startpreis. Lesebeispiel: Werden die Kaufs- und Verkaufpreise zum Handel untertags 2.5% unter resp. über dem Eröffnungskurs des jeweiligen Tages gesetzt, so resultiert ein Gewinn, welcher rund 8% über demjenigen der Referenzstrategie liegt.</p>
<p>Bei genauerer Betrachtung weist die Kurve einige interessante Eigenschaften auf: Der höchste Payoff wird bei einer Auslenkung der Preise um 0.9% erreicht. Der Payoffüberschüss beträgt an diesem Punkt rund 8.3%. Gleichzeitig zeigt sich, dass ein mehr oder weniger konstanter Paypoff-Überschuss von rund 8% im ganzen Auslenkungsbereich von 0.7 bis rund 4% erreicht werden kann.</p>
<p>Während im Bereich tieferer Auslenkungen viele kleinere Gewinne realisiert werden, sind es beim setzen breiterer Schranken nur noch wenige, dafür grössere. Die Kurve zeigt, dass sich diese beiden Effekte im genannten Bereich in etwa die Waage halten. Dieses Ergebnis ist insofern interessant, als dass die genaue Preisbestimmmung gar nicht von so grosser Relevanz sein könnte. Wichtig dabei zu erwähnen ist auch, dass während der Datenbereinigung tendenziell grosse Auslenkungen aus dem Datensatz entfernt wurden (<a href="daten-1.html#bereinigung">2.2.1</a>). Werden vermehrt auch extreme Marktbewegungen zugelassen, verschiebt sich die optimale Auslenkung der Preisschranken nach oben. In Kombination mit der Erkenntnis, dass auch bei stärkerer Bereinigung gute Payoffs bis 4% Auslenkung erreicht werden, könnte dies eine Motivation sein, die Preise eher breiter zu setzen.</p>
<p>Eine weitere Besonderheit der Kurve zeigt sich mit dem Abwärtsknick bei sehr kleinen Auslenkungen. Erklären lässt sich dieser Knick dadurch, dass bei allen Kursverläufen, bei denen der Eröffnungskurs gleichzeit Höchst- oder Tiefstkurs ist, mindestens eine Schranke nicht mehr erreicht werden kann. Bereits beim Setzen etwas grösserer Schranken wird dieser Effekt wieder mehr als ausgeglichen.</p>
<p>Auffällig ist auch die Tatsache, dass eine Auslenkung von 0 (und damit einem Wiederherstellen der Delta-Neutralität gleich zum Eröffnungskurs) eine deutlich bessere Performance als die Referenzstrategie aufweist. Dies lässt sich damit erklären, dass die Werte im Datensatz offenbar die Tendenz eines “Overshootings” der Eröfnungspreise zeigen. Das beobachtete Bild lässt vermuten, dass sich die Preise im Laufe des Tages in der Tendenz wieder eher Richung Schlusskurs des Vortages entwickeln. Ein Ausgleich der aufgebauten Delta-Position “über Nacht” gleich zu Beginn des Handelstages auszugleichen, scheint daher ebenfalls besser, als bis am Abend zu warten.</p>
<p>Schliesslich stellt sich auch die Frage, inwiefern die gefundenen Ergebnisse als statistisch signifikant bezeichnet werden können. Zur Beurteilung dieser Frage wurde mittels Bootstrapverfahren ein 95%-Konfidenzband der Kurve ermittelt. Dieses ist als grau schraffierte Fläche am Rand der Kurve ersichtlich. Es zeigt sich, dass dieses Band relativ schmal ausfällt. Dies kann als Konsequenz der ausführlichen Datenbereinigung gesehen werden. Diese führt dazu, dass auch über verschiedene Boostrap-Samples hinweg die Payoffs stabil und wenig beeinflusst durch einzelne Beobachtungen ausfallen.</p>
<p>Als zweites Mass zur Beurteilung der Aussagekraft der gefundenen Ergebnisse lassen sich zudem auch die Werte des Testdatensatzes heranziehen. In diesem beträgt der Payoffüberschuss im Vergleich zur Referenzstartegie bei 0.9% Auslenkung ebenfalls rund 8.3% und auch bei einer Auslenkung von 4% kommt der Überschuss bei 7.7 zu liegen. Beide Werte zeigen hohe Ähnlichkeit mit dem Traingsdatensatz und unterstreichen damit auf Signifikanz der Ergebnisse.</p>
</div>
<div id="mit-berücksichtigung-der-marktvolatilität" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Mit Berücksichtigung der Marktvolatilität</h3>
<p>Die bisherige Analyse untersucht die Auslenkung der Kaufs- und Verkaufspreise um den gleichen prozentualen Wert für alle Einträge im Datensatz. Die Bimodalität der Kurve in Abbildung <a href="analyse.html#fig:symmetric-open-change">4.1</a> deutet darauf hin, dass es sich dabei um eine Überlagerung mehrerer Kurven handeln könnte. Gelänge es, diese zu separieren und einzelnen Gruppen von Kursverkäufen im Datensatz zuzuweisen, könnten individuellere Preisschranken gewählt werden. Mit Hilfe dieser könnte der Payoff im Idealfall weiter gesteigert werden.</p>
<p>Als Klassifizierungs sei dazu die Volatilität der vergangenen 10 Handelstage herangezogen. Die Vermutung liegt nahe, dass eine volatile Marktsituation in der kurfristigen Vergangenheit auch am nächsten Tag fortgesetzt werden könnte (bsp. Zeiten mit vielen marktrelevanten Informationen wie Finanzkrise, Corona-Krise, Dividend-Season etc.). Umgekehrt könnten eher ruhig verlaufende Börsentage in den vergangenen Tagen auf eine ruhige Situation auch am aktuellen Tag hinweisen (bsp. Ruhigere Zeiten während Sommerferien, etc).</p>
<p>Um dies zu untersuchen werden alle Einträge des Datensatzes in 2 Gruppen aufgeteilt. Einträge, welche eine aktuelle 10-Tages-Volatilität über demjenigen des Median aufweisen werden in eine Gruppe hoher Volatilität, die andern Einträge einer Gruppe tiefer Volatilität zugeordnet. Zu beachten gilt es hierbei, dass der Medianwert dabei einerseits nur innerhalb des jeweiligen Tickers betrachtet wird und für dessen Berechnung auch nur vergangene Werte mit einbezogen werden.</p>
<p>Für beide Gruppen lassen sich danach im Trainingsset die bereits bekannten Payoffvergleiche zum Referenzszenario durchführen und graphisch darstellen (vgl. Abbildung <a href="analyse.html#fig:symmetric-open-change-vol">4.2</a>).</p>
<div class="figure"><span id="fig:symmetric-open-change-vol"></span>
<img src="Masterarbeit_files/figure-html/symmetric-open-change-vol-1.png" alt="Payoffvergleich bei symmetrischer Abweichung vom Eröffnungspreis (mit Berücksichtigung Marktvolatilität)" width="672" />
<p class="caption">
Abbildung 4.2: Payoffvergleich bei symmetrischer Abweichung vom Eröffnungspreis (mit Berücksichtigung Marktvolatilität)
</p>
</div>
<p>Die Grafiken zeigen das erwartete Bild. Für die Gruppe tieferer vergangener Volatiltäten wird der maximale Payoff bei einer Auslenkung von 0.6% erreicht. Bei der Gruppe höherer Volatilität bei 3.7%. Angewendet auf das Testset resultiert ein Payoffüberschuss von rund 9.6%. Dies ist nocheinmal deutlich höher aus als im ungruppieren Fall.</p>
<p>Insbesondere bei der Gruppe der höheren Volatilitäten weist die Kurve aber weiterhin eine bimodale Form auf. Es scheint, als ob mit der gemachten Gruppierung zwar ein Teil der Varianz des aktuellen Tages erklärt werden konnte, weitere Teile davon aber unerklärt bleiben. Eine Möglichkeit bestünde nun darin, die Anzahl Gruppierungen weiter zu erhöhen, indem beispeilsweise nicht nur der Median, sondern die Quartile, Dezile, Percentile etc. als Klassifikatoren gewählt werden. Darauf sei an dieser Stelle verzichtet und zu andern Ansätzen des maschinellen Lernens übergegangen.</p>
</div>
</div>
<div id="nearest-neighbor-ansätze" class="section level2">
<h2><span class="header-section-number">4.2</span> Nearest Neighbor Ansätze</h2>
<p>Die ökomische Theorie scheint keinen offensichtlichen Grund zu liefern, weshalb die Volatilität des aktuellen Tages in genau 2 (oder x) Gruppen eingeteilt werden sollte. Auf der andern Seite haben bisherige Analysen gezeigt, dass die aktuelle Volatilität ein erklärender Faktor für die aktuelle Volatilität sein kann. Im vorliegenden Kapitel soll dieser Gedanke weiter verfolgt werden.</p>
<div id="distanzmasse" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Distanzmasse</h3>
<p>Die Idee des Ansatzes dieses Kapitels besteht darin, nicht lediglich x vordefinierte Gruppen für symmetrische Abweichungen zu finden, viel mehr soll versucht werden, ähnliche Kursverläufe wie den aktuellen in der Vergangenheit zu finden und individuell darauf zu reagieren. Die zugrund liegende Hypothese ist dabei, dass bei ausreichender Historie in der Vergangenheit ähnliche Muster erkannt und daraus Rückschlüsse auf die Zukunft (genauer: die Kursentwicklung des aktuellen Tages) gemacht werden können.</p>
<p>Da es sehr unwahrscheinlich ist, die genau gleichen Kursverläufe in der Histore wiederzufinden, muss ein Distanzmass definiert werden, welches die Nähe der Kursverläufe quantifiziert. Um diese zu berechnen formulieren wir für jeden Eintrag <span class="math inline">\(i\)</span> den bisherigen Kursverlauf als Vektor <span class="math inline">\(hist\)</span>.</p>
<p><span class="math display">\[ hist_{i, t} = (Open_{i, t}, High_{i, t}, Low_{j, t+1}, Close_{i, t+1}, Open_{i, t+1}, ..., Close_{i, 1}, Open_{i, 0}) \]</span></p>
<p>Der zweite Index gibt dabei an, wieviele Tage der Vergangenheit mit einbezogen werden. Ein Index von 0 bezieht sich auf den zu prognostizierenden, aktuellen Tag. Um die Ähnlichkeit zweier Einträge <span class="math inline">\(i\)</span> und <span class="math inline">\(j\)</span> zu berechnen, bieten sich 2 Distanzmasse an:</p>
<ol style="list-style-type: decimal">
<li><p>Die Manhattan Distanz (auch L1-Norm)<br />
<span class="math display">\[ 
\Vert hist_{i,t} - hist_{j,t}\rVert_1 =  \lvert Open_{i,t} - Open_{j,t} \rvert + \lvert High_{i,t} - High_{j,t} \rvert + \ldots + \lvert Open_{j,0} - Open_{j,0} \rvert
\]</span></p></li>
<li><p>Die Euklidische Distanz (auch L2-Norm)
<span class="math display">\[ 
\Vert hist_{i,t} - hist_{j,t}\rVert_2  = \sqrt{(Open_{i,t} - Open_{j,t})^2 + (High_{i,t} - High_{j,t})^2 + \ldots + (Open_{j,0} - Open_{j,0})^2}
\]</span></p></li>
</ol>
<p>Die Ergebnisse beider Masse sollen anschliessend miteinander verglichen werden.</p>
</div>
<div id="setup-und-berechnungsdauer" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Setup und Berechnungsdauer</h3>
<p>Mithilfe obiger Distanzmasse lassen sich für jeden Kursverlauf, die k ähnlichsten Kursverläufe der Vergangenheit ermitteln. Zuvor seien an dieser Stelle aber eingige Überlegungen zur Berechnungskomplexität des Problems gemacht: Der vorliegende bereinigte Datensatz weist 5’572’087 tägliche Kursverläufe auf. Soll jeder Kursverlauf mit jedem andern verglichen werden, so ergeben sich bei einem Brute-Force Ansatz 5’572’087^2 Distanzberechungen. Unter Berücksichtigung der Tatsache, dass das Problem symmetrisch ist und jeder Kursverlauf nicht mit sich selbst verglichen werden muss, halbiert sich die Komplexität zwar um mehr als die Hälfte, bleibt aber so gross, dass es zum Zeitpunkt des Schreibens dieser Arbeit nicht innerhalb weniger Sekunden oder Minuten auf einem handelsüblichen Heim-Computer berechnet werden kann.</p>
<p>Die für die Nearest Neighbors Suche eingesetzten Bibliotheken greifen daher typischerweise auf sophistiziertere Vorgehensweisen zurück. Eine davon sieht die Verwendung von Multidimensionalen Suchbäumen (Search Trees, auch Kd-Trees) vor. Diese gehen auf eine Idee von <span class="citation">Bentley (<a href="#ref-bentley">1975</a>)</span> zurück. Sie basiert darauf, dass jede Dimension in 2 Bereiche aufgeteilt wird (z.B. beim Median). Dadurch wird der Raum in viele kleinere Sub-Räume aufgeteilt. Der Algorithmus macht sich danach zu Nutze, dass er nicht den ganzen Raum absuchen muss. Beginnend im aktuellen Sub-Raum werden schrittweise alle benachtbarten Räum abgesucht, bis die geforderte Anzahl nächster Nachbarn gefunden ist. Allerdings benötigt dabei sowohl der Aufbau des Baumes wie auch die Suche im Baum Berechnungszeit.</p>
<p>Die Anzahl der Dimensionen beeinflusst dabei die benötigte Zeit erheblich. Im vorliegenden Fall liegt diese bei Historie von 10 Tagen à 4 Werten bei 41, wenn zusätzlich auch der (bekannte) Eröffnungspreis des aktuellen Tages mit einbezogen wird. Tests ergaben, dass die dafür benötigte Rechenzeit zu hoch war. Die zu verwendende Zeitperiode wurde daher auf 3 Tage beschränkt. Dies resultiert in einer deutlichen Reduktion der Suchdimensionen auf 13.</p>
<p>Der vorliegende Fall unterscheidet sich von andern Nearest Neighbor Problemen ferner dadurch, als dass für jeden Eintrag lediglich Kursverläufe der Vergangenheit bertrachtet werden sollten. Im Hinblick auf die Suchstrukur bedeutet dies, dass der KD-Tree nicht nur einmalig aufgebaut und danach für alle Verläufe auf Nachbarn durchsucht werden kann. Vielmehr muss der KD-Tree für jedes Datum mit allen Kursverläufen vor diesem Datum neu aufgebaut werden.[^Es sind auch Mischlösungen denkbar, bei denen nur alle x Daten der Tree neu aufgebaut wird und dafür mehr Nachbarn ermittelt werden, die im Nachgang um nicht vorher realsierte Verläufe gefiltert würden. Dies hat aber das Problem, dass a) mehr Nachbarn ermittelt werden müssen und b) nicht 100% sichergestellt ist, dass die Anzahl “Reservenaachbarn” ausreichen.] Dieser spezielle Setup kommt einer Brute-Force Methode ihrerseits wieder entgegen, da aufgrund des Datums-Filter nicht stets alle Einträge durchsucht werden müssten.</p>
<p>Beiden Ansätzen gemein ist hingegen, dass sie sich sehr gut parallelisieren lassen und Bibliotheken zur Verfügung stehen, welche diese Methoden effizient implementieren. Wir entscheiden uns im vorliegenden Fall für eine KD-Tree Implementation ohne Approximation. Auf einem Rechner mit 40 Cores (20 physisch, 20 Hyperthreading Cores) dauert die Berechnung von 50 Nachbarn ca. 2 Stunden im Falle des euklidischen Distanzmasses und ca. 3 Stunden im Falle der Manhattan Distanz. Dieser Hohe Anspruch eines KNN-Vorgehens an die Rechenleistung sollte bei der späteren Abwägung verschiedener Algorithmen mit berücksichtigt werden. Erwähnt sei an dieser Stelle aber auch, dass eine allfälligen Anwendung der Methode später nur wenige Titel (resp. nur diejenigen des aktuellen Tages) umfasst. Dies ist auch unter der Verwendung einer KNN-Methode in wenigen Sekunden möglich.</p>
</div>
<div id="bestimmung-kaufs--und-verkaufspreise" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Bestimmung Kaufs- und Verkaufspreise</h3>
<pre><code>## Warning in ans * length(l) + if1: Länge des längeren Objektes
##       ist kein Vielfaches der Länge des kürzeren Objektes</code></pre>
<p>Sind die Nachbarn eines Eintrages ermittelt lässt sich eine Prognose für den weiteren Kursverlauf des aktuellen Eintrages ableiten. Dazu wird aus den bekannten Kursverläufen der Nachbarn mittels geeigneter Metrik ein Prognosewert für den Folgetag abgeleitet. Abbildung @ref(fig:example_nearest_neighbor_outlook) illustriert dieses Vorgehen anhand eines Beispiels. Die Grafiken zeigen die Kursverläufe des aktuellen Eintrages (rot) sowie diejenigen der nächsten 5 Nachbarn für den aktuellen zu prognostizierenden Tag (t) sowie die 3 jeweils vorangegangenen Handelstage (t-1, … , t-3). Die gestrichelte rote Linie zeigt den auf Basis der Nachbarn prognostizierten Wert.</p>
<div class="figure">
<img src="Masterarbeit_files/figure-html/example_nearest_neighbor_outlook-1.png" alt="Exemplarische Kursprognose auf Basis nächster Nachhbarn" width="672" />
<p class="caption">
(#fig:example_nearest_neighbor_outlook)Exemplarische Kursprognose auf Basis nächster Nachhbarn
</p>
</div>
<p>Werden die prognostizierten Werte für Höchst- und Tiefstpreise als Kaufs- resp. Verkaufsschranken gewählt, lässt sich der Payoff berechnen und mit demjenigen der Referenzstartegie vergleichen. Wie bereits ausgeführt werden bei der Suche nach Nachbarn nur Einträge der Vergangenheit berücksichtigt. Die Menge an verfügbaren Vergleichsverläufen nimmt damit mit fortlaufender Zeit zu. Stellt man den Payoff-Vergleich für das Training Set über die Zeit dar (vgl. Abbildung <a href="analyse.html#fig:knn-learning-history">4.3</a>), wird der Lerneffekt der Methode sichtbar. Insbesondere 3 Eigenschaften der Kurve lassen sich ausmachen:</p>
<ol style="list-style-type: decimal">
<li>Hohe Volatilität des Vergleichsfaktor zu Beginn</li>
<li>Steigende Faktorhöhe mit fortlaufender Zeitdauer</li>
<li>Abflachung im Laufe der Zeit auf ein stabiles Level</li>
</ol>
<p>Beide Eigenschaften decken sich mit der Intuition. Da zu Beginn der Datenreihe nur sehr wenige Vergleichsverläufe zur Verfügung stehen, reagiert die Kurve sehr sensitiv und schlägt entsprechend aus. Dies glättet sich im Laufe der Zeit mit dem Vorhandensein von mehr Vergleichsmöglichkeiten. Die zweite Eiegenschaft des steigenden Faktors zeigt, dass der erhoffte Lerneffekt einzutreten scheint. Tatsächlich scheinen ähnliche Kursverläufe in der Vergangenheit zukünftige Entwicklungen teilweise erklären zu können. Dies deckt sich mit der Erkenntnis des vergangenen Kapitels. Anders als zuvor kann dieser Lernmechanismus hier aber sehr individuell und nicht beschränkt auf 2 Gruppen erfolgen. Die dritte Eigenschaft zeigt, dass dieser Lerneffekt nach gewisser Zeit gesättigt scheint.</p>
<div class="figure"><span id="fig:knn-learning-history"></span>
<img src="Masterarbeit_files/figure-html/knn-learning-history-1.png" alt="Entwicklung der Modellperformance über die Zeit" width="672" />
<p class="caption">
Abbildung 4.3: Entwicklung der Modellperformance über die Zeit
</p>
</div>
<p>Während sich bisherige Ausführungen auf die Analyse mit 5 Nachbarn ermittelt auf Basis der euklidischen Distanz beziehen, sind bei der Ermittlung der Prognosewerte auch andere Parametrisierungen denkbar. Neben der Unterscheidung des Distanzmasses sind dies insbesondere die Anzahl der zu berücksichtigenden Nachbarn und die Metrik zur Prognoseermittlung.</p>
<ul>
<li><p><strong>Anzahl Nachbarn:</strong><br />
In der vorliegenden Arbeit werden die 50 nächsten Nachbarn jedes Eintrages ermittelt. Einmal ermittelt lassen sich davon auch weniger verwenden. Damit kann sehr einfach der Einfluss der Anzahl Nachbarn auf die Prognosequalität ermittelt werden. Denkbar sind dabei grundsätzlich unterschiedliche Ergebnisse. So lässt sich argumentieren, dass bei der Verwendung weniger Nachbarn auch diejenigen mit grösster Ähnlichkeit verwendet werden. Insbesondere bei Marktbewegungen die relativ selten sind, könnten fernere, weniger gut passende Nachbarn das Ergebnis hier nicht verzerren. Umgekehrt lässt sich argumentieren, dass bei häufigeren Marktsituationen die Berücksichtigung und Mittelung von mehr Nachbarn zu einem unverzerrteren Ergebnis führen könnte. Schliesslich könnte sich als drittes Ergebnis auch eine Konfiguration mit “mittlerer” Anzahl Nachbarn beähren, wenn beide vorherhigen Argumentationen verschmolzen werden. Aus diesem Grund analysiert und vergleicht die vorliegende Arbeit die Ergebnisse bei der Verwendung von 5, 20 und 50 Nachbarn.</p></li>
<li><p><strong>Metrik der Prognoseermittlung:</strong><br />
Eine erste Möglichkeit zur Prognose von Tiefst-, Höchst- und Schlusskurs besteht darin, den Mittelwert der jeweiligen Kursfortsetzungen der Nachbarn zu wählen. Alternativ zur einfachen Mittelwertbildung sind auch andere Verfahren denkbar. Beispielsweise wäre auch die Berücksichtigung der Distanz als Gewichtungsfaktor denkbar. Aus Gründen der Einfachheit verzichten wir an dieser Stelle allerdings darauf und verwenden neben dem gleichgewichteten Mittelwert den Median als zweites Prognosemass. Dieses reagiert weniger sensitiv auf Ausreisser innerhalb der Nachbarn.</p></li>
</ul>
<p>Die zweite hier betrachtete Möglichkeit besteht darin, die Kaufs- und Verkaufspreise nicht direkt, sondern als Abweichung vom Eröffnungskurs zu modellieren. Hierzu wird für jeden Nachbarn die Differenz von Eröffnungs- und Tiefstpreis resp. Eröffnungs- und Höchstpreis berechnet. Die entsprechende Metrik (Mittelwert oder Median) wird dann auf diese Werte angewandt und auf den tatsächlichen Eröffnungspreis des zu analysierenden Titels appliziert. Dies hat den Vorteil, dass der bekannte Eröffnungspreis keiner Unsicherheit mehr unterliegt.</p>
<p>Tabelle <a href="#tab:knn-payoff-factors"><strong>??</strong></a> stellt die Ergebnisse aller Parametrisierungen für des Testingset einander gegenüber. Es zeigt sich, dass eine direkte Prognose der Kaufs- und Verkaufspreise derjenigen einer Prognose der Veränderung im Vergleich zum Eröffnungspreis klar unterlegen ist. Bezüglich Distanzmass lässt sich kein eindeutiger Gewinner feststellen, beide Masse weisen ähnliche Performance aus. Ähnliches gilt für die Anzahl der berücksichtigten Nachbarn.</p>
<p>Allen Ergebnissen gemein ist hingegen, dass sie der einfachen Strategie des Vorkapitels nicht überlegen sind. Im Gegenteil fallen die Payoffs im Vergleich trotz deutlich höherem Berechnungsaufwand in der Tendenz schlechter aus, wenn auch der Payoff der Referenzstrategie weiter deutlich geschlagen wird. Die Hoffnung, dass mit der Individualisierung der einzelnen Einträge mehr kursrelevante Information extrahiert werden kann, bestätigt sich nicht. Ein Grund könnte in der durch die Berechnungskomplexität beschränkte Begrenzung auf ein Zeitfenster von 3 vorangegangenen Tagen sein.</p>
<table>
<thead>
<tr>
<th style="border-bottom:hidden" colspan="1">
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Euklidisch
</div>
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Manhattan
</div>
</th>
</tr>
<tr>
<th style="text-align:right;">
</th>
<th style="text-align:left;">
Mittelwert
</th>
<th style="text-align:left;">
Median
</th>
<th style="text-align:left;">
Mittelwert
</th>
<th style="text-align:left;">
Median
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;font-weight: bold;">
5
</td>
<td style="text-align:left;">
Error in curr_config$aggr : $ operator is invalid for atomic vectors
</td>
<td style="text-align:left;">
Error in curr_config$aggr : $ operator is invalid for atomic vectors
</td>
<td style="text-align:left;">
Error in curr_config$aggr : $ operator is invalid for atomic vectors
</td>
<td style="text-align:left;">
Error in curr_config$aggr : $ operator is invalid for atomic vectors
</td>
</tr>
<tr>
<td style="text-align:right;font-weight: bold;">
20
</td>
<td style="text-align:left;">
Error in curr_config$aggr : $ operator is invalid for atomic vectors
</td>
<td style="text-align:left;">
Error in curr_config$aggr : $ operator is invalid for atomic vectors
</td>
<td style="text-align:left;">
Error in curr_config$aggr : $ operator is invalid for atomic vectors
</td>
<td style="text-align:left;">
Error in curr_config$aggr : $ operator is invalid for atomic vectors
</td>
</tr>
<tr>
<td style="text-align:right;font-weight: bold;">
50
</td>
<td style="text-align:left;">
Error in curr_config$aggr : $ operator is invalid for atomic vectors
</td>
<td style="text-align:left;">
Error in curr_config$aggr : $ operator is invalid for atomic vectors
</td>
<td style="text-align:left;">
Error in curr_config$aggr : $ operator is invalid for atomic vectors
</td>
<td style="text-align:left;">
Error in curr_config$aggr : $ operator is invalid for atomic vectors
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="neuronale-netzwerke" class="section level2">
<h2><span class="header-section-number">4.3</span> Neuronale Netzwerke</h2>
<p>Als dritte Analysemethode sollen im vorliegenden Kapitel neuronale Netzwerke verwendet werden. Diese Methoden erlauben den Umgang mit sehr grossen Datenmengen, weshalb die Beschränkung auf ein sehr kurzes Zeitfenster hier nicht nötig ist. Für die vorliegenden Analysen verwenden wir eine einfache Architektur mehrerer aufeinanderfolgender Dense Layer. Das Ziel der Analyse bleibt hingegen das gleiche wie in den Kapiteln zuvor: Es soll basierend auf vergangenen Kursverläufen möglich optimale Kaufs- und Verkaufskurse für den laufenden Tag prognositiert werden. Wie bereits früher ausgeführt ist der resultierende Payoff dabei abhängig von der Höhe der Preisbewegung und der der Wahrscheinlichkeit der Ausführung. Da der Payoff weiter in quadratischer Form von der Höhe der Kursbewegung abhängt haben frühere Überlegungen bereits gezeigt, dass es allenfalls lukrativ sein könnte eher breite Preisschranken zu setzen, welche zwar weniger oft erreicht werden, in diesem Fall aber einen umso höheren Payoff abwerfen. Als erste der betrachteten Methoden sollen nachfolgend daher nicht nur der kommende Höchst- und Tiefstwerte pronostiziert werden, sondern auch deren Verteilung. Das Vorgehen zum Finden optimaler Kaufs- und Verkaufspreise wird dadurch zweistufig. In einem ersten Schritt wird die Verteilung der prognostizierten Tiefst-, Höchst- und Schlusspreise geschätzt.</p>
<div id="diskretisierung" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Diskretisierung</h3>
<p>Zur Modellierung der Verteilung werden die zu prognostizierenden zukünftigen Preis (Tiefst-, Höchst- und Schlusspreis) in einem ersten Schritt diskretisiert. Für jeden Preisbereich [Bucket] lässt sich mit dem Modell später eine Eintretenswahrscheinlichkeit berechnen. Wichtig für die ein solches Klassifikationsproblem ist dabei, dass die einzelnen Buckets möglichst gleich viele Werte enthalten. Ist dies nicht der Fall bestehen für den Algorithmus Anreize, ein Element dem grössten Bucket zuzuordnen. Im vorliegenden Fall werden dazu zwei Strategien verfolgt:</p>
<p><strong>1. Unabhängige Modellierung von Low, High und Close Preisen</strong><br />
Eine erste Möglichkeit besteht darin, die Diskretisierung von Tiefts-, Höchst- und Schlusspreisen unabhängig voneinander zu gestalten. Dies hat den Nachteil, dass allenfalls wertvolle Information verloren geht. Dies zeigt sich inbesondere auch dadurch, dass Bucketkombinationen enstehen, welche in Realität nicht möglich sind. Beispielsweise sind dies Kombinationen, bei denen der Tiefstpreis höher vorausgesagt wird als der Tiefstpreis. Gelöst wird dieses Problem in der vorliegenden Analyse so, dass für solche Fälle keine Kaufs- und Verkaufspreise gestellt werden. All dies Fälle werden damit gleich wie im Referenzfall behandelt - namentlich wird damit aufgelaufenes Delta erst zum Tagesendkurs ausgeglichen. Der Vorteil dieses Vorgehens liegt darin, dass die Ermittlung gleich grosser Buckets sehr einfach gelingt. Zudem lässt sich das Problem unter der Annahme der Unabhängigkeit in 3 kleinere Klassifikationsprobleme aufteilen. Werden beispielsweise sowohl Tiefts-, Höchst- und Schlusspreise mit jeweils 30 Buckets modelliert, resultiert dies in 3 Klassifikationsproblemen mit 30 Klassen. Kombiniert man diese, resultieren <span class="math inline">\(27&#39;000 \ (= 30^3)\)</span> Preisszenarien, wobei sich die Wahrscheinlichkeit für jedes diese Szeanrien als Multipliaktion der einzelnen Preiswahrscheinlichkeiten ermitteln lässt.</p>
<p><strong>2. Abhängige Modellierung von Low, High und Close Preisen</strong>
Eine zweite Möglichkeit besteht darin, die Diskretisierung ohne Annahme der Unabhängigkeit der einzelnen Tagespreise zu gestalten. Die Diskretisierung unter dem Ziel möglich gleich grosser Buckets gestaltet sich dabei etwas schwieriger. Wir verwenden dazu den Ansatz, welcher zuerst die Tiefstpreise in gleich grosse Buckets aufteilt. Für jedes dieser Bucket werden danach die Höchstkurse in gleich grosse Buckets aufgeteilt. Die resultierenden Buckets werden danach wiederum in möglichst gleich grosse Buckets bezüglich Schlusspreise aufgeteilt. Anders als im Unabhängigen Fall sind die Bucket-Grenzen damit nicht immer gleich. Auch das Klassifikationsproblem lässt sich damit nicht mehr auf kleinere Modelle aufteilen. Im obogen Beispiel müssen damit alle 27’000 Szeanrien auf einmal bearbeitet werden. Dies erhöht die Komplexität der Bearbeitung. Der Vorteil dieser Methode liegt darin, dass eine Abhängikeit der Preise ökonomisch plausibler ist und auch unmögliche Szenarien nicht mehr vorkommen.</p>
</div>
<div id="architektur" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Architektur</h3>
<p>Eine zweite Fragestellung die sich bei der Verwebdung von neuronalen Netzen ergibt, ist diejenige nach der geeigneten Architektur. Neuronale Netze haben sich inbesondere im Bereich der Computer Vision - beispielsweise zur Klassifikation des Bildinhaltes - als sehr leistungsfähig herausgestellt. Diese Probleme zeichenen sich dadurch aus, dass mit der Verwendung von Pixel-Daten sehr grosse Datenmengen zur Verfügung stehen und verarbeitet werden müssen. Im Vorliegenden Fall sind die Datenmengen in Relation zu Bilddaten bedeutend kleiner. Die positiven Erfahrungen, welche bei der Bilderkennung mit Convolutional Layers gemacht wurden lassen sich beim vorliegenden Sachverahlt auch nicht direkt übertragem. Aus diesem Grund und aus Überlegungen der Einfachheit entscheiden wir uns daher für eine einfache Architektur mit 2 hidden Dense Layers mit Grösse 512. Diverse Tests zur Erhöhung der Anzahl Knoten oder Gestskltung des Output-Layer dem Beifügen weiterer Layer haben zu keinen wesentlichen Verbesserungen geführt. Neben der Anzahl Layers stelltsich zudem die Frage nach der geeigneten Abbildung des Output Layers. Hierbei wurden zwei Vorgehensweisen untersucht.</p>
<p><strong>1. Klassische Klassifikation</strong><br />
Eine erste Möglichkeit besteht darin, die Ordinalität der Klassen zu vernachlässigen. Eine solche Vernachlässigung macht insbesondere dann Sinn, wenn die einzlenen Klassen in keiner Abhängigkeit zueinander stehen. Wiederum bietet sich dabei der Vergleich mit der Bildklassifikation an - beispielsweise der Klassifikation des Bildes einer Katze. Es lässt sich je nach Verwendungszweck argumentieren, dass es bei einer Fehlklassifikation keine Rolle spielt, ob das Bild als Auto oder als Apfel erkannt wurde. Keines der beiden ist weniger oder mehr falsch, beide sind einfach falsch. Für diese Art der Klassifikation bietet sich eine Verlustfunktion der Art “Categorical Crossentropy” mit Aktivierung “softmax” des Outputlayers an.</p>
<p><strong>2. Ordinale Klassifikation</strong><br />
Im vorliegenden Fall - insbesondere bei unabhängiger Klassifikation von Tiefst-, Höchst- und Schlusspreisen - lassen sich die Buckets in eine logische Reihenfolge bringen. Wird ein Wert von 100 anstatt Klasse 10 fälschlicherweise Klasse 1 zugeordnet, so scheint dieser Fehler grösser als wenn die Fehlklassifikation in Klasse 9 erfolgt wäre. Eine Möglichkeit, diese Art von Klassifikationen in neuronalen Netzen zu modellieren zeigen <span class="citation">Frank and Hall (<a href="#ref-frank_hall">2001</a>)</span>. Grob besteht die Idee darin, nicht die Wahrscheinlichkeit der aktuellen Klasse zu modellieren, sondern die kumulierte Wahrscheinlichkeit der aktuellen und der darunterliegenden. Die Wahrscheinlichkeit der Zugehörigkeit zu einer spezifischen Klasse lässt sich dann einfach als Differnz der kumulierten Wahrscheinlichkeiten benachbater Klassen ableiten. Zu berücksichtigen gilt es hierbei, dass die kumulative Wahrscheinlichkeit nicht sinken darf, dies durch das geschätzte Modell aber nicht unbedingt garantiert ist. In der praktischen Umsetzung unterscheidet sich die Architektur zur Schätzung dieser Art von Modellen nicht gross von derjenigen der klassischen Klassifikation. Die Unterscheidungen beziehen sich auf eine andere Verlustfunktion (Binary Crossentropy), andere Aktivierung (Sigmoid) des Output Layers sowie eine etwas anderen Kodierung der Labels.</p>
</div>
<div id="ergebnisse" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Ergebnisse</h3>
<p>Alle der nachfolgenden Modelle wurden mit 30 Buckets im Falle unabhängig modellierter Preise, respektive 27’000 Preisszenarien im Falle abhängiger Preise gerechnet. Ebenfalls allen Modellen gemein ist, dass eine Batch-Grösse von 512 und ein Training über 10 Epochen gewählt wurde. Ferner werden als Features alle 4 Preiskennzahlen der letzten 10 Tage plus der Eröffnungskurs des aktuellen Tages verwendet. Jedes dieser Features wird vor dem Training skaliert.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> Das eigentliche Training erfolgt auf 80% des Trainingsets, 20% der Trainingsdaten dienen der Validierung.</p>
<div id="unabhängige-modelle" class="section level4">
<h4><span class="header-section-number">4.3.3.1</span> Unabhängige Modelle</h4>
<pre><code>## [1] &quot;col_id: 1&quot;
## [1] &quot;curr_group: 0&quot;
## [1] &quot;col_id: 1&quot;
## [1] &quot;curr_group: 0&quot;
## [1] &quot;col_id: 1&quot;
## [1] &quot;curr_group: 0&quot;</code></pre>
<p>Während des Trainings der Netze für Tiefst-, Höchst und Schlusskurs zeigt sich der in Abbildung <a href="analyse.html#fig:ind-train-progress">4.4</a> visualisierte Lernfortschritt. Als Genauigkeit wird hierbei der Anteil welcher dem richtigen Bucket zugeordnet wurde angegeben. Bei der Betrachtung des Lernprozesses lassen sich folgende Erkenntnisse gewinnen:</p>
<ul>
<li>Die Genauigkeit richtig zugeordner Klassen liegt deutlich über demjenigen, welcher bei zufälliger Zuteilung erreicht werden könnte. Im vorliegenden Fall von 30 Buckets bei ungefähr gleichverteilter Anzahl Datenbunkte wäre dies lediglich 3.3%.</li>
<li>Die Genauigkeit der Prognose für Tiefst- und Höchstkurs ist deutlich höher als diejenige des Schlusskurses.</li>
<li>Über den Traingsverlauf nimmt die Genauigkeit zu. Dies gilt sowohl für die Trainings- wie auch die Validierungsdaten. Ein Overfitting ist damit im vorliegenden Fall nicht auszumachen.</li>
</ul>
<div class="figure"><span id="fig:ind-train-progress"></span>
<img src="Masterarbeit_files/figure-html/ind-train-progress-1.png" alt="Trainingsfortschritte im Falle unabhängiger Modelle" width="672" />
<p class="caption">
Abbildung 4.4: Trainingsfortschritte im Falle unabhängiger Modelle
</p>
</div>
<p>Mit Hilfe der trainierten Modelle lassen sich nun Voraussagen über die Wahrscheinlichkeitsverteilungen der Daten im Testset machen. Für einzelne Beobachtungen lassen sich diese gar visualisieren. Abbildung <a href="analyse.html#fig:plot-binary-histogram">4.6</a> zeigt die prognostizierten Verteilungen zweier exemplarischen Einträge im Testset. Diese unterscheiden sich deutlich. Während für ersteren Eintrag ein wenig volatiler Kursverlauf prognostiziert ist, erwartet das Modell im zweiten Fall eine deutlich volatilere Entwicklung.</p>
<div class="figure"><span id="fig:plot-categorical-histogram"></span>
<img src="Masterarbeit_files/figure-html/plot-categorical-histogram-1.png" alt="Exemplarische Verteilung der vorausgesagten Preise" width="672" />
<p class="caption">
Abbildung 4.5: Exemplarische Verteilung der vorausgesagten Preise
</p>
</div>
<p>Tatsächlich unterscheiden sich die realisierten Tiefts- und Schlusskurse, wenn auch nicht so deutlich wie dies vom Modell prognostiziert wird. Die entsprechenden Realisierungen für Low und High sind 99.37 und 100.83 für das erste und 98.75 und 101.5 für das zweite Beispiel.</p>
<p>Zur Überprüfung der Modelleignung soll erneut der Payoff des Testsets ermittelt und mit demjenigen der Referenzstrategie verglichen werden. Eine erste Möglichkeit besteht darin, die Kaufs- resp. Verkaufsschranken als Tiefst- respektive den Höchstpreis desjenigen Buckets vorauszusagen, welchem das Modell die höchste Wahrscheinlichkeit vorhersagt.[^Da ein Bucket durch untere und obere Grenze bestimmt ist, verwenden wir den Mittelwert von oberer und unterer Grenze als Vorhersagewert. Ist eine der Grenzen nicht finit, wird die andere Grenze als Vorhersagewert verwendet.]</p>
<p>Es resultiert ein Überschusspayoff von 9.8%. Bereits in dieser einfachen Form ist das gelernte Modell damit gleich gut gut, wie das Model mit Kaufs- und Verkaufsschranken als symmetrischer Abweichungen vom aktuellen Eröffnungskurs bei Unterscheidung von Hoch- und Tiefvolatilitätsphasen. Bei der Konzeption des Modelles war es gerade das Ziel, nicht nur eine Klassifikation in das wahrscheinlichste Bucket zu machen, sondern auch Voraussagen über die Verteilung der prognostizierten Preise machen zu können. Lediglich das Bucket mit höchster Wahrscheinlichkeit als Prognosewert zu verwenden ging damit zuwenig weit. Viel können nun verschiedene Kaufs- und Verkaufskombinationen für alle Preisszenarien berechnet und mit der jeweiligen Wahrscheinlichkeit gewichtet werden. Die optimale Strategie lässt sich dann diejenige mit dem höchsten erwarteten Payoff auswählen. Hierbei lohnt wiederum ein Blick auf die Berchnungskomplexität des Problems. In der vorliegenden Analyse wurden je Preistyp 30 Buckets verwendet. Dies resultiert in 27’000 möglichen Preisszenarien. Diese können für jeden Eintag des Testsets (rund 1’000’000 Einträge) ermittelt werden. Für jedes dieser Preisszenarien sollen wiederum verschiedene Paare von Kaufs- und Verkaufspreisen getestet werden. Orientiert man sich dabei an einer ähnlichen Granularität wir für die Preise resultieren 900 Szenarien für die Schranken (je 30 Werte für die Kaufs- und Verkaufsschranke). Damit müssen zur kompletten Evaluation <span class="math inline">\(27&#39;000 \times 1&#39;000&#39;000 \times 900\)</span> Payoffs berechnet werden. Dies ist mit Hilfe der in Kapitel @ref(infrastruktur_und_tools) vorgestellen Cloud-Infrastruktur und Tools nicht innerhalb kurzer Zeit möglich. Zur Reduktion der Komplexität lässt sich dabei ausnützen, dass nicht alle Szenarien von gleicher Bedeutung sind. Während die Verwendung des häufigsten Szenarios und die Verwendung aller Szenarios die beiden Extrempositionen einnehmen, ist auch die Verwendgung einiger wichtiger Preis- und Schrankenszenarien denkbar. Konkret werden diehjenigen Szenarien ausgewählt, welche die höchsten Eintretenswahrscheinlichkeiten aufweisen. Dabei hat es sich bewährt, diese Grenze als Quantil der jeweiligen Wahrscheinlichkeiten zu wählen. Damit werden nur sehr wenige, dafür wichtige Szenarien berücksichtigt. Die Berechungszeits lässt sich damit sehr deutlich senken. Diese Vorgehensweise hat zudem den Vorteil, dass mit einer Reduzierung des Quantil sehr einfach mehr Werte einbezogen werden könnnen. Ferner wird auch die Schrankenszenarien sehr stark reduziert, indem als Kaufs- und Verkaufspreise nur die Tiefs- und Höchstwerte der betrachteten Preisszenarien berechnet werden.</p>
<p>Das beschriebene Vorgehen sowohl für unabhängige wie auch abhängige Modelle möglich. Bei den unabhängigen Modellen gibt es ferner zusätzlich zu beachten, dass nicht mögliche Szenarien (bsp. prognostizierter Höchstpreis &lt; prognostizierter Tieftspreis) entfernt, resp. deren prognostizierte Wahrscheinlichkeiten auf 0 gesetzt werden. Bei den Modellen mit voneinander abhängigen Preisen treten solche Szenarien nicht auf.</p>
<p>Bezieht man für die Pronose der Preisschranken alle Buckets mit 0.1% höchster Wahrscheinlichkeit mit ein (99.9% Quantil) resultiert für den Testdatensatz ein ÜBershusspayoff von 12% gegenüber der Referenzstartegie. Durch Berücksichtigung (eines Teils) der geschätzen Verteilung kann dieser damit noch einmal um 2.2 Prozentpunkte gesteiegert werden und übertrifft damit die Performance einfacherer früherer Modelle.</p>
<p>Wie in Abbildung @(fig:plot-binary-histogram) gezeigt schien das Modell auch ohne explizite Modellierung der Ordinalität der Klassen diese implizit gelernt zu haben. Dies zeigt sich dadurch, dass benachbarte Klassen typsischerweise auch ähnliche Wahrscheinlichkeiten aufweisen. Eine explizite Modelierung der Ordinalität scheint damit im vorliegenden Fall nicht unbedingt nötig. Dies zeigt sich auch beim Vergleich der Ergebnisse mit ebendiesem Modell.</p>
<div class="figure"><span id="fig:plot-binary-histogram"></span>
<img src="Masterarbeit_files/figure-html/plot-binary-histogram-1.png" alt="Exemplarische Verteilung der vorausgesagten Preise" width="672" />
<p class="caption">
Abbildung 4.6: Exemplarische Verteilung der vorausgesagten Preise
</p>
</div>
</div>
<div id="abhängige-modelle" class="section level4">
<h4><span class="header-section-number">4.3.3.2</span> Abhängige Modelle</h4>

</div>
</div>
</div>
</div>
<h3> Fazit und Ausblick</h3>
<div id="refs" class="references">
<div id="ref-bentley">
<p>Bentley, J.L. 1975. “Multidimensional Binary Search Trees Used for Associative Searching.” <em>Communications of the ACM</em> 18 (9): 509–17.</p>
</div>
<div id="ref-frank_hall">
<p>Frank, Eibe, and Mark Hall. 2001. “A Simple Approach to Ordinal Classification.” In <em>Lecture Notes in Computer Science</em>, 2167:145–56. <a href="https://doi.org/10.1007/3-540-44795-4_13">https://doi.org/10.1007/3-540-44795-4_13</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>Im Detail erfolgt die Skalierung auf Basis des Mittelwertes und Standardabweichung jedes Features des Trainingsets.<a href="analyse.html#fnref3" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="infrastruktur-und-tools.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary-and-outlook.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
