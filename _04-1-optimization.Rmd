## Einfache Optimierungen


### Ohne Berücksichtigung der Marktvolatilität
```{r message=FALSE, warning=FALSE, echo=FALSE}
suppressPackageStartupMessages({
  library(dplyr)
  library(stringr)
  library(purrr)
  library(pbmcapply)
  library(tidyr)
  library(ggplot2)
  library(data.table)
  library(dtplyr)
  library(arrow)
  library(microbenchmark)
  library(profvis)
  library(dqrng)
  library(keras)
  library(magrittr)
  library(parallel)
  library(cumstats)
})

source("R/02-data.R")
source("R/03-analysis.R")
Rcpp::sourceCpp('src/calc_payoff_per_title.cpp')

data_wide_10 <- arrow::read_feather("data/steps/data_wide_10.feather")

set.seed(123456)
train_idx <- sample(x = nrow(data_wide_10), size = floor(0.8 * nrow(data_wide_10)))
test_idx <- setdiff(seq_len(nrow(data_wide_10)), train_idx)

both_first <- c("buy", "sell")[sample(c(1, 2), nrow(data_wide_10), replace = TRUE)]

```


Eine erste Möglichkeit, optimale Kaufs- und Verkaufspreise zu finden besteht darin, diese im Testdatensatz mittels einfacher Optimierung zu ermitteln. In einer ersten sehr einfachen Evaluation sollen Kaufs- und Verkaufsmarken als prozentuale Abweichungen vom aktuellen Preis festgelegt werden. Als Startgrösse bietet sich hierbei der "Open" Kurs des jeweiligen Tages an. Die resultierenden Payoffs bei einer solchen Festlegung lassen sich dann ins Verhältnis zum Referenzpayoff mit Glattstellung der Deltaposition bei Tagesende stellen. Ein Payoff-Verhältnis über 1 kennzeichnet damit eine Strategie, welche der Referenzstrategie überlegen ist. Verhältnisse unter 1 kennzeichnen unterlegene Strategien. 


``` {r symmetric-open-change, fig.cap='Payoffvergleich bei symmetrischer Abweichung vom Eröffnungspreis (ohne Berücksichtigung Marktvolatilität)', fig.asp=0.6, fig.pos = '!H', echo=FALSE, message=FALSE, warning=FALSE}

variation_factor_open_file_name <- "data/optimizations/variation_factor_open.feather"
data_variation <- select(data_wide_10, Close_1, Open = "Open_0", Low = "Low_0", High = "High_0", Close_0)
opt_payoff_sym <- function(move, data, col, both_first, scale_fct = 1) {
  sum(calc_payoff_const_gamma(data, buy = (1 - move) * data[[col]], sell = (1 + move) * data[[col]], both_first = both_first), na.rm = TRUE) / scale_fct
}

if (file.exists(variation_factor_open_file_name)) {
  bootstraped_variation_factor_open <- arrow::read_feather(variation_factor_open_file_name)
} else {
  scale_fct_train <- sum(calc_payoff_const_gamma(data_variation[train_idx, ], both_first = both_first[train_idx]))
  bootstraped_variation_factor_open <- bootstrap_variation_factor(
    col = "Open",
    move = seq(0, 0.1, 0.001),
    data = data_variation[train_idx, ],
    both_first = both_first[train_idx],
    R = 100,
    mc.cores = ceiling(detectCores() * 0.51)
  )
  
  arrow::write_feather(bootstraped_variation_factor_open, variation_factor_open_file_name)
}

plot_variation_factor(bootstraped_variation_factor_open)

```

Abbildung \@ref(fig:symmetric-open-change) veranschaulicht dieses Verhältnis bei variierender symmetrischer Abweichung vom Startpreis. Lesebeispiel: Werden die Kaufs- und Verkaufpreise zum Handel untertags 2.5% unter resp. über dem Eröffnungskurs des jeweiligen Tages gesetzt, so resultiert ein Gewinn, welcher rund 8% über demjenigen der Referenz bei alleinigem Ausgleich am Abend liegt. 

Bei genauerer Betrachtung weist die Kurve einige interessante Eigenschaften auf: Der höchste Payoff wird bei einer symmetrischen Auslenkung des Preises um `r bootstraped_variation_factor_open$move[which.max(bootstraped_variation_factor_open$original)] * 100`% erreicht. Der Payoffüberschüss beträgt an diesem Punkt rund `r round((bootstraped_variation_factor_open$original[which.max(bootstraped_variation_factor_open$original)] - 1) * 100, 1)`%. Gleichzeitig zeigt sich, dass ein mehr oder weniger konstanter Paypoff-Überschuss von rund 8% im ganzen Auslenkungsbereich von 0.7 bis rund 4% erreicht werden kann. Während im Bereich tieferer Auslenkungen viele kleinere Gewinne realisiert werden, sind es beim setzen breiterer Schranken nur noch wenige, dafür grössere. Die Kurve zeigt, dass sich diese beiden Effekte im genannten Bereich in etwa die Waage halten. Dieses Ergebnis ist für einen Optionshändler insofern interessant, als dass die genaue Preisbestimmmung gar nicht von so grosser Relevanz sein könnte. Wichtig dabei zu erwähnen ist auch, dass während der Datenbereinigung tendenziell eher grosse Auslenkungen aus dem Datensatz entfernt wurden (\@ref(bereinigung)). Werden vermehrt auch extreme Marktbewegungen zugelassen, verschiebt sich die optimale Auslenkung der Preisschranken nach oben. In Kombination mit der Erkenntnis, dass auch bei stärkerer Bereinigung gute Payoffs bis 4% Auslenkung vom Eröffnungspreis erreicht werden, könnte dies auch eine Motivation sein, die Preise eher breiter zu setzen.

Eine weitere Besonderheit der Kurve zeigt sich mit dem Abwärtsknick bei einer Auslenkung bei sehr kleinen Auslenkungen. Erklären lässt sich dieser Knick dadurch, dass bei allen Kursverläufen, bei denen der Eröffnungskurs gleichzeit dem Höchst-, resp. Tiefstkurs des jeweiligen Tages entspricht mindestens eine Marke nicht mehr erreicht werden kann. Bereits beim setzen etwas grösserer Schranken wird dieser Effekt allerdings wieder mehr als ausgeglichen.

Auffällig ist auch die Tatsache, dass bei einer Auslenkung von 0 (und damit einem Wiederherstellen der Delta-Neutralität gleich zum Eröffnungskurs) eine deutlich bessere Performance als die Referenzstrategie aufweist. Dies lässt sich damit erklären, als dass die Werte im Datensatz offenbar eine Tendenz des "Overshootings" der Eröfnungspreise zeigen. Das beobachtete Bild lässt vermuten, dass sich die Preise im Laufe des Tages in der Tendenz wieder eher Richung Schlusskurs des Vortages entwickeln. Ein Ausgleich der aufgebauten Delta-Position "über Nacht" gleich zu Beginn des Handelstages scheint daher ebenfalls besser Ergebnisse zu liefern als die Referenzstartegie.

Schliesslich stellt sich auch die Frage, inwiefern die gefundenen Ergebnisse als statistisch signifikant bezeichnet werden können. Zur Beurteilung dieser Frage wurden mittels Bootsprapverfahren ein 95%-Konfidenzband der Kurve ermittelt. Dieses ist als grau schraffierte Fläche am Rand der Kurve ersichtlich. Es zeigt sich, dass dieses Bank relativ schmal ausfällt. Auch dies kann als direkte Konsequenz der asuführlichen Datenbereinigung gesehen werden. Diese führt dazu, dass auch über verschiedene Boostrap-Samples hinweg die Payoffs stabil und wenig beeinflusst durch einzelne Beobachtungen ausfallen.


``` {r, echo=FALSE}

scale_fct_test <- sum(calc_payoff_const_gamma(data_variation[test_idx, ], both_first = both_first[test_idx]))
payoff_factor_test_max <- opt_payoff_sym(
  move = 0.009, 
  data = data_variation[test_idx, ], 
  col = "Open",
  both_first = both_first[test_idx],
  scale_fct = scale_fct_test
)

payoff_factor_test_4 <- opt_payoff_sym(
  move = 0.04, 
  data = data_variation[test_idx, ], 
  col = "Open",
  both_first = both_first[test_idx],
  scale_fct = scale_fct_test
)

```

Als zweites Mass zur Beurteilung der Aussagekraft der gefundenen Ergebnisse lassen sich zudem auch die Werte des Testdatensatzes heranziehen. In diesem beträgt der Payoffüberschuss im Vergleich zur Referenzstartegie bei 0.9% Auslenkung ebenfalls rund `r round((payoff_factor_test_max - 1) * 100, 1)`% und auch bei einer Auslenkung von 4% kommt der Überschuss bei `r round((payoff_factor_test_4 - 1) * 100, 1)` zu liegen. Beide Werte zeigen hohe Ähnlichkeit mit dem Traingsdatensatz und unterstreichen damit auf Signifikanz der Ergebnisse.


### Mit Berücksichtigung der Marktvolatilität

Die bisherige Analyse untersucht die Auslenkung der Kaufs- und Verkaufspreise um den geleichen prozentualen Wert für alle Einträge im Datensatz. Die Form der Kurve in Abbildung \@ref(fig:symmetric-open-change) insbesondere deren Bimodalität deutet darauf hin, dass es sich dabei um eine Überlagerung mehrerer Kurven handeln könnte. Gelänge es, diese zu separieren und einzelnen Gruppen von Kursverkäufen im Datensatz zuzuweisen, könnten individuellere Preisschranken gewählt werden. Mit Hilfe dieser könnte der Payoff im Idealfall weiter gesteigert werden. 

Als Klassifizierungs sei dazu die Volatilität der vergangenen 10 Handelstage herangezogen. Die Vermutung liegt nahe, dass eine volatile Marktsituation in der kurfristigen Vergangenheit auch am nächsten Tag fortgesetzt werden könnte (bsp. Zeiten mit vielen marktrelevanten Informationen wie Finanzkrise, Corona-Krise, Dividend-Season etc.). Umgekehrt könnten eher ruhig verlaufende Börsentage in den vergangenen Tagen auf eine ruhige Situation auch am aktuellen Tag hinweisen (bsp. Ruhigere Zeiten während Sommerferien, etc). 

Um anfänglich zwei Gruppen zu unterscheiden, werden alle Einträge des Datensatzes in 2 Gruppen aufgeteilt. Einträge, welche eine aktuelle 10-Tages-Volatilität über demjenigen des Median aufweisen, werden in eine Gruppe hoher hoher Volatilität, die andern Einträge einer Gruppe tiefer Volatilität zugeordnet. Zu beachten gilt es hierbei, dass der Medianwert dabei einerseits nur innerhalb des jeweiligen Tickers betrachtet wird und für dessen Berechnung auch nur vergangene Werte mit einbezogen werden. 

Für beide Gruppen lassen sich danach im Trainingsset die bereits bekannten Payoffvergleiche zum Referenzszenario durchführen und graphisch darstellen (vgl. Abbildung \@ref(fig:symmetric-open-change-vol)).


``` {r symmetric-open-change-vol, fig.cap='Payoffvergleich bei symmetrischer Abweichung vom Eröffnungspreis (mit Berücksichtigung Marktvolatilität)', fig.asp=0.6, fig.pos = '!H', echo=FALSE, message=FALSE, warning=FALSE}

data_variation_vol_paths <- c(
  "data/steps/data_variation_vol.feather",
  "data/optimizations/bootstraped_variation_factor_open_low.feather", 
  "data/optimizations/bootstraped_variation_factor_open_high.feather"
)

data_wide_10_vol <- arrow::read_feather("data/steps/data_wide_10_vol.feather")

if (!all(map_lgl(data_variation_vol_paths, ~file.exists(.)))) {
  
  # split data into low and high varioance groups
  vol_10 <- data_wide_10_vol$Vol_10
  data_variation_vol <- data_wide_10 %>% select(Ticker, Date, Close_1, Open = "Open_0", Low = "Low_0", High = "High_0", Close_0) %>% mutate(Vol_10 = vol_10)
  
  vol_10_median <- data_variation_vol %>%
    mutate(ID = seq_len(nrow(.))) %>%
    arrange(Ticker, Date) %>%
    group_by(Ticker) %>%
    select(ID, Ticker, Vol_10) %>%
    group_split() %>%
    pbmclapply(function(x) list(ID = x$ID, Vol_10_med = cummedian(x$Vol_10)), mc.cores = 3) %>%
    bind_rows()
  
  data_variation_vol <- data_variation_vol %>% 
    mutate(ID = seq_len(nrow(.))) %>% 
    left_join(vol_10_median, by = "ID") %>% 
    select(-ID)
  
  data_variation_vol <- data_variation_vol %>% mutate(Vol_Group = factor(if_else(Vol_10 <= Vol_10_med, "Low", "High"), levels = c("Low", "High")))
  write_feather(data_variation_vol, data_variation_vol_paths[1])
  
  
  # find opimal move
  train_idx_low <- intersect(train_idx, which(data_variation_vol$Vol_Group == "Low"))
  bootstraped_variation_factor_open_low <- bootstrap_variation_factor(
    col = "Open",
    move = seq(0, 0.1, 0.001),
    data = data_variation_vol[train_idx_low, ],
    both_first = both_first[train_idx_low],
    R = 0,
    mc.cores = ceiling(detectCores() * 0.51)
  )
  write_feather(bootstraped_variation_factor_open_low, data_variation_vol_paths[2])
  
  train_idx_high <- intersect(train_idx, which(data_variation_vol$Vol_Group == "High"))
  bootstraped_variation_factor_open_high <- bootstrap_variation_factor(
    col = "Open",
    move = seq(0, 0.1, 0.001),
    data = data_variation_vol[train_idx_high, ],
    both_first = both_first[train_idx_high],
    R = 0,
    mc.cores = ceiling(detectCores() * 0.51)
  )
  write_feather(bootstraped_variation_factor_open_high, data_variation_vol_paths[3])
  
  
} else {
  data_variation_vol <- read_feather(data_variation_vol_paths[1])
  bootstraped_variation_factor_open_low <- read_feather(data_variation_vol_paths[2])
  bootstraped_variation_factor_open_high <- read_feather(data_variation_vol_paths[3])
}

bind_rows(
  mutate(bootstraped_variation_factor_open_low, group = factor("Tiefe Vol_10", c("Tiefe Vol_10", "Hohe Vol_10"))),
  mutate(bootstraped_variation_factor_open_high, group = factor("Hohe Vol_10", c("Tiefe Vol_10", "Hohe Vol_10")))
) %>% plot_variation_factor()

```



```{r}

opt_move_low <- bootstraped_variation_factor_open_low$move[which.max(bootstraped_variation_factor_open_low$original)]
opt_move_high <- bootstraped_variation_factor_open_high$move[which.max(bootstraped_variation_factor_open_high$original)]

test_idx_low <- intersect(test_idx, which(data_variation_vol$Vol_Group == "Low"))
test_idx_high <- intersect(test_idx, which(data_variation_vol$Vol_Group == "High"))

scale_fct_test <- sum(calc_payoff_const_gamma(data_variation[test_idx, ], both_first = both_first[test_idx]))
payoff_test_low <- opt_payoff_sym(move = opt_move_low, data = data_variation_vol[test_idx_low, ], both_first = both_first[test_idx_low], col = "Open")
payoff_test_high <- opt_payoff_sym(move = opt_move_high, data = data_variation_vol[test_idx_high, ], both_first = both_first[test_idx_high], col = "Open")

payoff_factor_test_vol <- (payoff_test_low + payoff_test_high) / scale_fct_test

```

Die Grafiken zeigen das erwartete Bild. Für die Gruppe tiefere vergangener Volatiltät wird wird der maximale Payoff bei einer Auslenkung von `r round(opt_move_low * 100, 1)`% erreicht. Bei der Gruppe höherer Volatilität bei `r round(opt_move_high * 100, 1)`%. Angewendet auf das Testset resultiert mit dieser Strategie ein Payoffüberschuss von rund `r round((payoff_factor_test_vol - 1) * 100, 1)`% nocheinmal deutlich höher aus als im ungruppieren Fall. 

Insbesondere bei der Gruppe der höheren Volatilitäten weist die Kurve aber weiterhin eine bimodale Form auf. Es scheint als ob mit der gemachten Gruppierung zwar ein Teil der Varianz des aktuellen Tages erklärt werden konnte, weitere Teile davon aber unerklärt bleiben. Eine Möglichkeit bestünde nun darin, die Anzahl Gruppierungen weiter zu erhöhen, indem beispeilsweise nicht nur der Median, sondern die Quartile, Dezile, Percentile etc. als Klassifikatoren gewählt werden. Darauf sei an dieser Stelle verzichtet und zu andern Ansätzen des maschinellen Lernens übergegangen.







