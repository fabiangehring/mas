## Neuronale Netzwerke


```{r, echo=FALSE, results="hide"}
library(tidyverse)
library(magrittr)
library(keras)
library(arrow)
library(pbmcapply)
Rcpp::sourceCpp('src/calc_payoff_per_title.cpp')
source("R/03-analysis.R")

# Rcpp::sourceCpp('src/find_best_buy_sell_ind.cpp')
# Rcpp::sourceCpp('src/find_best_buy_sell_dep.cpp')

data_wide_10 <- arrow::read_feather("data/steps/data_wide_10.feather")

set.seed(123456)
train_idx <- sample(x = nrow(data_wide_10), size = floor(0.8 * nrow(data_wide_10)))
test_idx <- setdiff(seq_len(nrow(data_wide_10)), train_idx)
both_first <- c("buy", "sell")[sample(c(1, 2), nrow(data_wide_10), replace = TRUE)]

sample_idx <- seq_len(500)

```


Als dritte Methode werden im vorliegenden Kapitel neuronale Netzwerke analysiert. Diese Methode erlaubt den Umgang mit sehr grossen Datenmengen, weshalb die Beschränkung auf ein sehr kurzes Zeitfenster hier nicht nötig ist. Die Architektur wird sehr einfach gehalten und besteht aus mehreren vollständig miteinander verknüpfter Schichten (Dense Layer). Das Ziel der Analyse bleibt das Gleiche wie in den Kapiteln zuvor: Es sollen basierend auf vergangenen Kursverläufen möglichst optimale Kauf- und Verkaufskurse für den laufenden Tag prognositiert werden. 

Wie bereits ausgeführt, ist der Payoff abhängig von der Höhe der Preisbewegung und der Wahrscheinlichkeit der Ausführung. Da der Payoff in quadratischer Form von der Höhe der Kursbewegung abhängt, haben frühere Überlegungen bereits gezeigt, dass es allenfalls vorteilhaft sein könnte, eher breite Preisschranken zu setzen. Diese werden zwar weniger oft erreicht, werfen im Falle der Ausführung aber einen umso höheren Payoff ab. Ziel ist es daher, nicht nur Punktprognosen für die Preisextreme des Tages zu machen, sondern auch Erkenntnisse über deren Verteilungen zu gewinnen. Sind diese bekannt, können unter Berücksichtigung der Eintretenswahrscheinlichkeiten der jeweiligen Preisentwicklungen die optimalen Kauf- und Verkaufsschranken eruiert werden.


### Diskretisierung
Zur Modellierung der Verteilung werden die Tiefst-, Höchst- und Schlusspreise in einem ersten Schritt diskretisiert. Für jeden Preisbereich [Bucket] lässt sich mit dem Modell später eine Eintretenswahrscheinlichkeit ermitteln. Zu beachten gilt es hierbei, dass die einzelnen Buckets möglichst gleich viele Werte enthalten. Ist dies nicht der Fall, bestehen für den Algorithmus Anreize, ein Element dem grössten Bucket zuzuordnen. Es gibt verschiedene Oversampling und Undersampling Techniken, um mit solchen Ungleichgewichten umzugehen. Im vorliegenden Fall lassen sich die Buckets aber gut so wählen, dass sie in häufigen Datenbereichen enger sind als in Bereichen mit weniger Daten. Wir verwenden dazu zwei unterschiedliche Einteilungsverfahren:

1. **Unabhängige Modellierung von Low, High und Close Preisen**  
Eine erste Möglichkeit besteht darin, die Diskretisierung von Tiefst-, Höchst- und Schlusspreisen unabhängig voneinander zu gestalten. Dies hat den Nachteil, dass allenfalls wertvolle Information verloren geht. Dies zeigt sich insbesondere dadurch, dass Bucketkombinationen entstehen, welche in Realität nicht möglich sind. Beispielsweise sind dies Kombinationen, bei denen der Tiefstpreis höher ausfällt als der Höchstpreis. 
Der Vorteil dieses Vorgehens liegt andererseits darin, dass die Ermittlung gleich grosser Buckets mittels Quantilbildung sehr einfach möglich ist. Zudem lässt sich das Problem bei Annahme von Unabhängigkeit in drei kleinere Klassifikationsprobleme aufteilen. Werden beispielsweise sowohl Tiefst-, Höchst- und Schlusspreise mit jeweils 30 Buckets modelliert, resultieren drei Klassifikationsprobleme à 30 Klassen. Kombiniert man diese, resultieren $27'000 \ (= 30^3)$ Preisszenarien, wobei sich die Wahrscheinlichkeit für jedes dieser Szenarien als Multiplikation der einzelnen Preiswahrscheinlichkeiten ergibt.

2. **Abhängige Modellierung von Low, High und Close Preisen**  
Eine zweite Möglichkeit besteht darin, die Diskretisierung mit Annahme einer Abhängigkeit der einzelnen Tagespreise vorzunehmen. Die Diskretisierung unter dem Ziel möglichst gleich grosser Buckets gestaltet sich dabei etwas schwieriger. Wir verwenden dazu den Ansatz, welcher zuerst die Tiefstpreise in gleich grosse Buckets aufteilt. Für jedes dieser Buckets werden danach die darin enthaltenen Höchstkurse in gleich grosse Buckets aufgeteilt. Die resultierenden Buckets werden danach wiederum in möglichst gleich grosse Buckets bezüglich Schlusspreisen aufgeteilt. Anders als im unabhängigen Fall sind die Bucket-Grenzen damit nicht immer gleich. Das Klassifikationsproblem lässt auch nicht mehr auf kleinere Modelle aufteilen. Alle 27'000 Szenarien müssen damit in einem grösseren Modell auf einmal bearbeitet werden. Der Vorteil dieser Methode liegt darin, dass eine Abhängigkeit der Preise ökonomisch plausibler ist und auch unmögliche Szenarien nicht mehr vorkommen.


### Architektur
Neuronale Netze haben sich insbesondere im Bereich der Computer Vision - beispielsweise zur Klassifikation von Bildern - als sehr leistungsfähig herausgestellt [vgl. @krizhevsky_sutskever_hinton]. Diese Probleme zeichnen sich dadurch aus, dass mit der Verwendung von Pixel-Daten sehr grosse Datenmengen verarbeitet werden müssen. Im vorliegenden Fall sind die Datenmengen deutlich kleiner. Aus diesem Grund und aus Überlegungen der Einfachheit entscheiden wir uns daher für eine einfache Architektur mit zwei versteckten Dense Layer mit jeweils 512 Knoten. Diverse Tests zur Erhöhung der Knotenanzahl oder dem Beifügen weiterer Layer haben nicht zu wesentlich andern Ergebnissen geführt. Neben der Anzahl Layer stellt sich zudem die Frage nach der geeigneten Abbildung des Output Layers. Hierbei werden zwei Vorgehensweisen untersucht.

1. **Klassische Klassifikation**  
Eine erste Möglichkeit besteht darin, die Ordinalität der Klassen zu vernachlässigen. Eine solche Vernachlässigung macht insbesondere dann Sinn, wenn die einzelnen Klassen in keiner Abhängigkeit zueinander stehen. Wiederum bietet sich der Vergleich mit der Bildklassifikation an - beispielsweise der Klassifikation des Bildes einer Katze. Es lässt sich argumentieren, dass es bei einer Fehlklassifikation keine Rolle spielt, ob das Bild als Auto oder als Apfel erkannt wurde. Keines der beiden ist "weniger" oder "mehr" richtig, beide sind schlicht falsch. Für diese Art der Klassifikation bietet sich eine Verlustfunktion der Art "Categorical Crossentropy" und eine Aktivierung des Outputlayers mittels "Softmax"-Funktion an. 

2. **Ordinale Klassifikation**  
Im vorliegenden Fall - insbesondere bei unabhängiger Klassifikation von Tiefst-, Höchst- und Schlusspreisen - lassen sich die Buckets in eine logische Reihenfolge bringen. Wird nun ein Wert anstatt in Bucket 10 fälschlicherweise in Bucket 9 klassifiziert, scheint dies der kleinere Fehler zu sein, als wenn die Klassifikation in Klasse 1 erfolgt wäre. Eine Möglichkeit, diese Art von Klassifikationen in neuronalen Netzen zu modellieren, zeigen @frank_hall. Grob besteht die Idee darin, die Wahrscheinlichkeit der aktuellen Klasse nicht direkt, sondern als kumulierte Wahrscheinlichkeit zu modellieren. Die Wahrscheinlichkeit einer spezifischen Klasse lässt sich dann als Differenz der kumulierten Wahrscheinlichkeiten benachbarter Klassen berechnen. Zu berücksichtigen gilt es hierbei, dass die kumulative Wahrscheinlichkeit in der Theorie nicht sinken kann. Dies ist durch das Modell aber nicht garantiert. Wir lösen dieses Problem, indem für die Auswertung jeweils das Minimum der aktuellen kumulierten Wahrscheinlichkeit und der Vorhergehenden verwendet wird. In ihrer Architektur unterscheiden sich diese Art der Modelle durch eine andere Verlustfunktion (Binary Crossentropy), eine andere Aktivierungsfunktion (Sigmoid) des Output Layers sowie eine etwas anderen Kodierung der Labels.


### Ergebnisse

Wie bereits erwähnt werden alle Modelle mit jeweils 30 Buckets im Falle unabhängig modellierter Preise, respektive 27'000 Preisszenarien im Falle abhängiger Preise berechnet. Ebenfalls allen Modellen gemein ist, dass eine Batch-Grösse von 512 und ein Training über 30 Epochen gewählt wird. Ferner werden als Features alle vier Preiskennzahlen der letzten 10 Tage plus der Eröffnungskurs des aktuellen Tages verwendet. Jedes dieser Features wird vor dem Training skaliert.^[Im Detail erfolgt die Skalierung auf Basis des Mittelwertes und Standardabweichung jedes Features des Trainingssets.] Das eigentliche Training erfolgt auf 80% des Trainingssets, 20% der Trainingsdaten dienen der Validierung.

#### Unabhängige Modelle
```{r, echo=FALSE, results='hide', message=FALSE}
n_groups_per_col <- 30

data <- select(data_wide_10, c(as.vector(outer(c("Open", "High", "Low", "Close"), 10:1, function(x, y) paste(x, y, sep = "_"))), "Open_0"))
scale <- map_dbl(data[train_idx, ], sd)
center <- map_dbl(data[train_idx, ], mean)

x <- scale(data, center, scale)
rm(data)

calc_mid_prices <- function(lower, upper) {
  out <- (lower + upper) / 2
  out[!is.finite(out)] <- if_else(is.finite(lower[!is.finite(out)]), lower[!is.finite(out)], upper[!is.finite(out)])
  return(out)
}
```


```{r, echo=FALSE, results='hide', message=FALSE}
ind_categorical <-  NULL
ind_categorical$discretization <- list(
  low = multivariate_discretization(data_wide_10, train_idx, test_idx, "Low_0", n_groups_per_col),
  high = multivariate_discretization(data_wide_10, train_idx, test_idx, "High_0", n_groups_per_col),
  close = multivariate_discretization(data_wide_10, train_idx, test_idx, "Close_0", n_groups_per_col)
)

y_low <- ind_categorical$discretization$low$groups
y_high <- ind_categorical$discretization$high$groups
y_close <- ind_categorical$discretization$close$groups

fit_model_cat <- function(x, y, epochs = 10, n_classes = 30, callbacks = NULL) {
  model <- keras_model_sequential() 
  model %>% 
    layer_dense(units = 512, activation = 'relu', input_shape = ncol(data)) %>% 
    layer_dense(units = 512, activation = 'relu') %>%
    layer_dense(units = n_classes, activation = 'softmax')
  
  model %>% compile(
    loss = 'sparse_categorical_crossentropy',
    optimizer = optimizer_adam(),
    metrics = c('accuracy')
  )
  
  history <- model %>% fit(
    x = x, 
    y = y, 
    epochs = epochs, batch_size = 512, 
    validation_split = 0.2,
    callbacks = callbacks
  )
  list(model = model, history = history)
}

model_ind_low_path  <- "data/models/ind/categorical/2_dense_512/model_low.h5"
history_ind_low_path  <- "data/models/ind/categorical/2_dense_512/history_low.rds"
pred_ind_low_path  <- "data/models/ind/categorical/2_dense_512/pred_low.rds"
if (file.exists(model_ind_low_path) && file.exists(pred_ind_low_path) && file.exists(history_ind_low_path)) {
  model_cat_low <- load_model_hdf5(model_ind_low_path)
  history_cat_low <- readRDS(history_ind_low_path)
  pred_cat_low <- readRDS(pred_ind_low_path)
} else {
  cp_callback_low <- callback_model_checkpoint(
    filepath = file.path("data/models/ind/categorical/2_dense_512", "low.{epoch:02d}-{val_loss:.2f}.hdf5")
  )
  model_cat_low <- fit_model_cat(x[train_idx, ], y_low[train_idx], 30, callbacks = list(cp_callback_low))
  save_model_hdf5(model_cat_low$model, model_ind_low_path)
  saveRDS(model_cat_low$history, history_ind_low_path)
  pred_cat_low <- predict_proba(model_cat_low$model, x[test_idx, ], batch_size = 512)
  saveRDS(pred_cat_low, pred_ind_low_path)
}

model_ind_high_path  <- "data/models/ind/categorical/2_dense_512/model_high.h5"
history_ind_high_path  <- "data/models/ind/categorical/2_dense_512/history_high.rds"
pred_ind_high_path  <- "data/models/ind/categorical/2_dense_512/pred_high.rds"
if (file.exists(model_ind_high_path) && file.exists(pred_ind_high_path) && file.exists(history_ind_low_path)) {
  model_cat_high <- load_model_hdf5(model_ind_high_path)
  history_cat_high <- readRDS(history_ind_high_path)
  pred_cat_high <- readRDS(pred_ind_high_path)
} else {
  cp_callback_high <- callback_model_checkpoint(
    filepath = file.path("data/models/ind/categorical/2_dense_512", "high.{epoch:02d}-{val_loss:.2f}.hdf5")
  )
  model_cat_high <- fit_model_cat(x[train_idx, ], y_high[train_idx], 30, callbacks = list(cp_callback_high))
  save_model_hdf5(model_cat_high$model, model_ind_high_path)
  saveRDS(model_cat_high$history, history_ind_high_path)
  pred_cat_high <- predict_proba(model_cat_high$model, x[test_idx, ], batch_size = 512)
  saveRDS(pred_cat_high, pred_ind_high_path)
}

model_ind_close_path  <- "data/models/ind/categorical/2_dense_512/model_close.h5"
history_ind_close_path  <- "data/models/ind/categorical/2_dense_512/history_close.rds"
pred_ind_close_path  <- "data/models/ind/categorical/2_dense_512/pred_close.rds"
if (file.exists(model_ind_close_path) && file.exists(history_ind_close_path) && file.exists(pred_ind_close_path)) {
  model_cat_close <- load_model_hdf5(model_ind_close_path)
  history_cat_close <- readRDS(history_ind_close_path)
  pred_cat_close <- readRDS(pred_ind_close_path)
} else {
  cp_callback_close <- callback_model_checkpoint(
    filepath = file.path("data/models/ind/categorical/2_dense_512", "close.{epoch:02d}-{val_loss:.2f}.hdf5")
  )
  model_cat_close <- fit_model_cat(x[train_idx, ], y_close[train_idx], 30, callbacks = list(cp_callback_close))
  save_model_hdf5(model_cat_close$model, model_ind_close_path)
  saveRDS(model_cat_close$history, history_ind_close_path)
  pred_cat_close <- predict_proba(model_cat_close$model, x[test_idx, ], batch_size = 512)
  saveRDS(pred_cat_close, pred_ind_close_path)
}
```

Während des Trainings der Netze unabhängiger Preise zeigt sich der in Abbildung \@ref(fig:ind-train-progress) dargestellte Lernfortschritt. Als Genauigkeit wird hierbei der Anteil richtiger Klassifizierungen angegeben. Folgende Aussagen lassen sich aus der Darstellung gewinnen:

- Die Genauigkeit richtig zugeordneter Klassen liegt deutlich über derjenigen einer zufälligen Zuteilung von 3.3% $(1/30)$.
- Die Genauigkeit der Prognose für Tiefst- und Höchstkurs ist deutlich höher als diejenige des Schlusskurses.
- Über den Trainingsverlauf nimmt die Genauigkeit mit abnehmender Geschwindigkeit zu. Dies gilt sowohl für die Trainings- wie auch die Validierungsdaten. Ein Overfitting ist im vorliegenden Fall nicht auszumachen.


```{r, ind-train-progress, fig.cap='Trainingsfortschritt im Falle unabhängiger Preise', fig.height=3, echo=FALSE, message=FALSE, warning = FALSE} 
train_history <- as_tibble(history_cat_low) %>% mutate(price = "low") %>%
  bind_rows(as_tibble(history_cat_high) %>% mutate(price = "high")) %>%
  bind_rows(as_tibble(history_cat_close) %>% mutate(price = "close")) %>%
  mutate(
    price = factor(price, levels = c("low", "high", "close")),
    data = do.call(recode, list(.x = data, validation = "Validierung", training = "Training"))
  )

train_history %>% 
  filter(metric == "accuracy") %>%
  ggplot(mapping = aes(x = epoch, y = value, color = data)) +
  geom_line() + 
  facet_wrap(vars(price), ncol = 3,  scales = "free_y") +
  scale_x_continuous(labels = scales::number_format(accuracy = 1)) +
  ylab("Genauigkeit") + 
  xlab("Epoche") +
  labs(color = "Daten") + 
  theme_bw()
```


Die trainierten Modelle lassen sich danach zur Prognose der Wahrscheinlichkeitsverteilungen der Daten im Testset heranziehen. Für einzelne Beobachtungen lassen sich diese einfach visualisieren. Abbildung \@ref(fig:plot-categorical-histogram) zeigt die prognostizierten Verteilungen zweier exemplarischer Einträge. Diese unterscheiden sich deutlich. Während für ersteren Eintrag ein wenig volatiler Kursverlauf prognostiziert wird, erwartet das Modell im zweiten Fall eine deutlich volatilere Entwicklung.

```{r, plot-categorical-histogram, fig.cap='Exemplarische Verteilung der vorausgesagten Preise ohne explizite Modellierung der Ordinalität', fig.height=3,  echo=FALSE, message=FALSE, warning = FALSE}
eval_idx <- c(52, 12)
ind_categorical$pred <- list(low = pred_cat_low, high = pred_cat_high, close = pred_cat_close)
plot_neural_sample_histogram(eval_idx, ind_categorical)
rm(list = c("pred_cat_low", "pred_cat_high", "pred_cat_close"))
```

Tatsächlich unterscheiden sich die realisierten Tiefts- und Höchstkurse, wenn auch nicht ganz so deutlich wie dies vom Modell prognostiziert. Die entsprechenden Realisierungen sind `r round(data_wide_10$Low_0[test_idx][eval_idx[1]], 2)` und `r round(data_wide_10$High_0[test_idx][eval_idx[1]], 2)` für das erste und `r round(data_wide_10$Low_0[test_idx][eval_idx[2]], 2)` und `r round(data_wide_10$High_0[test_idx][eval_idx[2]], 2)` für das zweite Beispiel.


```{r}

mid_low <- calc_mid_prices(ind_categorical$discretization$low$borders$lower, ind_categorical$discretization$low$borders$upper)
mid_high <- calc_mid_prices(ind_categorical$discretization$high$borders$lower, ind_categorical$discretization$high$borders$upper)
mid_close <- calc_mid_prices(ind_categorical$discretization$close$borders$lower, ind_categorical$discretization$close$borders$upper)
rm(ind_categorical)

glob_scen <- expand_grid(Close = mid_close, High = mid_high, Low = mid_low) %>% mutate(ID = seq_len(nrow(.))) %>% filter(Low <= Close, High >= Close)

data_test <- select(data_wide_10[test_idx, ], Close_1, High = "High_0", Low = "Low_0", Close_0)

```

Um den Payoff des ganzen Testsets berechnen zu können, müssen aus den geschätzten Verteilungen die optimalen Kauf- und Verkaufsschranken ermittelt werden. Eine erste Möglichkeit besteht darin, die Kauf- und Verkaufsschranken als Tiefst- respektive Höchstpreis desjenigen Buckets vorauszusagen, für welches das Modell die höchste Eintretenswahrscheinlichkeit prognostiziert.^[Da ein Bucket durch untere und obere Grenze bestimmt ist, verwenden wir den Mittelwert von oberer und unterer Grenze als Vorhersagewert. Ist eine der Grenzen nicht finit, wird die andere Grenze als Vorhersagewert verwendet.] 

```{r}

ind_buy_sell_max_prob_path <- "data/models/ind/categorical/2_dense_512/ind_buy_sell_max_prob.rds"
if (file.exists(ind_buy_sell_max_prob_path)) {
  ind_buy_sell_max_prob <- read_feather(ind_buy_sell_max_prob_path)
} else {
  n_eval <- length(test_idx)
  ind_buy_sell_max_prob <- pbmclapply(seq_len(n_eval), function(i) {
    probs <- as.vector(pred_cat_low[i, ] %o% pred_cat_high[i, ] %o% pred_cat_close[i, ])[glob_scen$ID]
    threshold <- quantile(probs, q)
    calc_best_buy_sell(probs, max(probs), glob_scen$Low, glob_scen$High, glob_scen$Close, both_first = both_first[test_idx[i]])
  }, mc.cores = 3) %>% bind_rows()
  write_feather(ind_buy_sell_max_prob, ind_buy_sell_max_prob_path)
}

factor_ind_max_prob <- sum(calc_payoff_const_gamma(data_test, buy = ind_buy_sell_max_prob$buy, sell = ind_buy_sell_max_prob$sell, both_first = both_first[test_idx])) / sum(calc_payoff_const_gamma(data_test, both_first = both_first[test_idx]))
saveRDS(factor_ind_max_prob, "data/models/ind/categorical/2_dense_512/factor_ind_max_prob.rds")

```


Mit Hilfe der so ermittelten Schranken lässt sich wie bei den vorangegangenen Methoden der Payoff des Testsets ermitteln und ins Verhältnis zur Referenzstategie setzen. Es resultiert ein Überschusspayoff von `r round((factor_ind_max_prob - 1) * 100, 1)`%. Bereits in dieser einfachen Form ist das gelernte Modell damit ähnlich gut, wie das Modell basierend auf symmetrischen Abweichungen vom aktuellen Eröffnungskurs. 

Während der Konzeption des Modelles wurde grosser Wert darauf gelegt, auch die Verteilung der Preise zu prognostizieren. Lediglich das wahrscheinlichste Szenario für die Prognose der Preisschranken zu verwenden, greift damit etwas kurz. Tatsächlich erlauben es die vorhandenen Werte nun auch, den Payoff verschiedener Kauf- und Verkaufsschranken für jedes der Preisszenarien zu berechnen und mit der jeweiligen Wahrscheinlichkeit zu gewichten. Als optimale Strategie lässt sich dann diejenige mit dem höchsten erwarteten Payoff auswählen.

Wiederum lohnt sich dabei erst ein Blick auf die Berechnungskomplexität des Problems: In der vorliegenden Analyse wurden je Preistyp 30 Buckets verwendet. Dies resultiert in 27'000 möglichen Preisszenarien. Diese können für jeden Eintrag des Testsets (rund 1'000'000 Einträge) ermittelt werden. Für jedes dieser Preisszenarien sollen wiederum verschiedene Paare von Kauf- und Verkaufsschranken getestet werden. Orientiert man sich dabei an einer ähnlichen Granularität wie bei den Preisen, resultieren 900 Szenarien für die Schranken (je 30 Werte für die Kauf- und Verkaufsschranke). Damit müssen zur kompletten Evaluation $27'000 \times 1'000'000  \times 900$ Payoffs berechnet werden. Dies ist mit Hilfe der vorgestellten Cloud-Infrastruktur und Tools nicht innerhalb kurzer Zeit möglich. 

Zur Reduktion der Komplexität lässt sich aber ausnützen, dass nicht alle Szenarien von gleicher Bedeutung sind. Während die Verwendung des häufigsten Szenarios und die Verwendung aller Szenarien die beiden Extrempositionen bilden, ist auch die Verwendung einiger wichtiger Preis- und Schrankenszenarien denkbar. Konkret lohnt es sich, diejenigen Szenarien auszuwählen, welche die höchsten Eintretenswahrscheinlichkeiten aufweisen. Dabei hat es sich bewährt, diese Grenze als Quantil der jeweiligen Wahrscheinlichkeiten zu wählen. Damit können nur sehr wenige, dafür wichtige Szenarien berücksichtigt werden. Die Berechnungskomplexität lässt sich damit deutlich senken. Diese Vorgehensweise hat zudem den Vorteil, dass mit einer Reduzierung des Quantils auch sehr einfach mehr Werte einbezogen werden können, sollte dies gewünscht werden. Ferner werden auch die Schrankenszenarien sehr stark reduziert, indem als Kauf- und Verkaufspreise nur die Tiefst- und Höchstwerte der betrachteten Preisszenarien berechnet werden. 

Das beschriebene Vorgehen ist sowohl für unabhängige wie später auch für abhängige Modelle möglich. Bei den unabhängigen Modellen gibt es ferner zu beachten, dass nicht mögliche Szenarien (beispielsweise prognostizierter Höchstpreis < prognostizierter Tiefstpreis) entfernt, respektive deren prognostizierte Wahrscheinlichkeiten vor der Auswertung auf 0 gesetzt werden. Bei den Modellen mit voneinander abhängigen Preisen treten solche Szenarien nicht auf.


```{r}

ind_buy_sell_0_999_prob_path <- "data/models/ind/categorical/2_dense_512/ind_buy_sell_0_999_prob.rds"
if (file.exists(ind_buy_sell_0_999_prob_path)) {
  ind_buy_sell_0_999_prob <- read_feather(ind_buy_sell_0_999_prob_path)
} else {
  n_eval <- length(test_idx)
  q <- 0.999
  ind_buy_sell_0_999_prob <- pbmclapply(seq_len(n_eval), function(i) {
    probs <- as.vector(pred_cat_low[i, ] %o% pred_cat_high[i, ] %o% pred_cat_close[i, ])[glob_scen$ID]
    threshold <- quantile(probs, q)
    calc_best_buy_sell(probs, threshold, glob_scen$Low, glob_scen$High, glob_scen$Close, both_first = both_first[test_idx[i]])
  }, mc.cores = 4) %>% bind_rows()
  write_feather(ind_buy_sell_0_999_prob, ind_buy_sell_0_999_prob_path)
}

factor_ind_buy_sell_0_999_prob <- sum(calc_payoff_const_gamma(data_test, buy = ind_buy_sell_0_999_prob$buy, sell = ind_buy_sell_0_999_prob$sell, both_first = both_first[test_idx])) / sum(calc_payoff_const_gamma(data_test, both_first = both_first[test_idx]))
saveRDS(factor_ind_buy_sell_0_999_prob, "data/models/ind/categorical/2_dense_512/factor_ind_buy_sell_0_999_prob.rds")

```


Bezieht man die Buckets mit den 0.1% höchsten Eintretenswahrscheinlichkeiten (99.9% Quantil) in die Auswertung mit ein, resultiert ein Überschusspayoff von `r round((factor_ind_buy_sell_0_999_prob - 1) * 100, 1)`% gegenüber der Referenzstrategie. Durch Berücksichtigung (eines Teils) der geschätzten Verteilung kann die Modell-Performance damit noch einmal um `r round((factor_ind_buy_sell_0_999_prob - factor_ind_max_prob) * 100, 1)` Prozentpunkte gesteigert werden und übertrifft damit die Performance früherer Modelle.

``` {r, echo=FALSE, results='hide'}
ind_binary <-  NULL
ind_binary$discretization <- list(
  low = multivariate_discretization(data_wide_10, train_idx, test_idx, "Low_0", n_groups_per_col),
  high = multivariate_discretization(data_wide_10, train_idx, test_idx, "High_0", n_groups_per_col),
  close = multivariate_discretization(data_wide_10, train_idx, test_idx, "Close_0", n_groups_per_col)
)

y_low <- ind_binary$discretization$low$groups
y_high <- ind_binary$discretization$high$groups
y_close <- ind_binary$discretization$close$groups

fit_model_bin <- function(x, y, n_classes, epochs = 30, callbacks = NULL) {
  
  y <- as.matrix(purrr::map_dfc(seq_len(n_classes) - 1, ~as.integer(.<=y))) %>% unname()
  
  model <- keras_model_sequential()
  model %>%
    layer_dense(units = 512, activation = 'relu', input_shape = ncol(x)) %>%
    layer_dense(units = 512, activation = 'relu') %>%
    layer_dense(units = n_classes, activation = 'sigmoid')
  
  model %>% compile(
    loss = 'binary_crossentropy',
    optimizer = optimizer_rmsprop(),
    metrics = c('accuracy')
  )
  
  history <- model %>% fit(
    x = x,
    y = y,
    epochs = epochs,
    batch_size = 512,
    validation_split = 0.2,
    callbacks = callbacks
  )
  list(model = model, history = history)
}


model_ind_bin_low_path <- "data/models/ind/binary/2_dense_512/model_low.h5"
history_ind_bin_low_path <- "data/models/ind/binary/2_dense_512/history_low.rds"
if (file.exists(model_ind_bin_low_path) && file.exists(history_ind_bin_low_path)) {
  model_ind_bin_low <- load_model_hdf5(model_ind_bin_low_path)
  history_ind_bin_low <- readRDS(history_ind_bin_low_path)
} else {
  cp_callback_bin_low <- callback_model_checkpoint(
    filepath = file.path("data/models/ind/binary/2_dense_512", "low.{epoch:02d}-{val_loss:.2f}.hdf5")
  )
  model_cat_bin_low <- fit_model_bin(x[train_idx, ], y_low[train_idx], n_classes = 30, callbacks = list(cp_callback_bin_low))
  save_model_hdf5(model_cat_bin_low$model, model_ind_bin_low_path)
  saveRDS(model_cat_bin_low$history, history_ind_bin_low_path)
}

model_ind_bin_high_path <- "data/models/ind/binary/2_dense_512/model_high.h5"
history_ind_bin_high_path <- "data/models/ind/binary/2_dense_512/history_high.rds"
if (file.exists(model_ind_bin_high_path) && file.exists(history_ind_bin_high_path)) {
  model_ind_bin_high <- load_model_hdf5(model_ind_bin_high_path)
  history_ind_bin_high <- readRDS(history_ind_bin_high_path)
} else {
  cp_callback_bin_high <- callback_model_checkpoint(
    filepath = file.path("data/models/ind/binary/2_dense_512", "high.{epoch:02d}-{val_loss:.2f}.hdf5")
  )
  model_ind_bin_high <- fit_model_bin(x[train_idx, ], y_high[train_idx], n_classes = 30, callbacks = list(cp_callback_bin_high))
  save_model_hdf5(model_cat_bin_high$model, model_ind_bin_high_path)
  saveRDS(model_cat_bin_high$history, history_ind_bin_high_path)
}

model_ind_bin_close_path <- "data/models/ind/binary/2_dense_512/model_close.h5"
history_ind_bin_close_path <- "data/models/ind/binary/2_dense_512/history_close.rds"
if (file.exists(model_ind_bin_close_path) && file.exists(history_ind_bin_close_path)) {
  model_ind_bin_close <- load_model_hdf5(model_ind_bin_close_path)
  history_ind_bin_close <- readRDS(history_ind_bin_close_path)
} else {
  cp_callback_bin_close <- callback_model_checkpoint(
    filepath = file.path("data/models/ind/binary/2_dense_512", "close.{epoch:02d}-{val_loss:.2f}.hdf5")
  )
  model_ind_bin_close <- fit_model_bin(x[train_idx, ], y_close[train_idx], n_classes = 30, callbacks = list(cp_callback_bin_close))
  save_model_hdf5(model_cat_bin_close$model, model_ind_bin_close_path)
  saveRDS(model_cat_bin_close$history, history_ind_bin_close_path)
}
```


```{r, make-ind-binary-predictions}

translate_cum_prob <- function(cum_prob) {
  n_col <- ncol(cum_prob)
  n_row <- nrow(cum_prob)
  for (i in tail(seq_len(n_col), -1)) {
    cum_prob[, i] <- pmin(cum_prob[, i], cum_prob[, i-1])
  }
  cum_prob - cbind(cum_prob[, head(1 + seq_len(n_col), -1)],rep(0, n_row))
}

pred_bin_low <- predict_proba(model_ind_bin_low, x[test_idx, ], batch_size = 512) %>% translate_cum_prob()
pred_bin_high <- predict_proba(model_ind_bin_high, x[test_idx, ], batch_size = 512) %>% translate_cum_prob()
pred_bin_close <- predict_proba(model_ind_bin_close, x[test_idx, ], batch_size = 512) %>% translate_cum_prob()

```

```{r, plot-binary-histogram, fig.cap='Exemplarische Verteilung der vorausgesagten Preise mit expliziter Modellierung der Oridinalität', fig.height=3, echo=FALSE, message=FALSE, warning = FALSE}
eval_idx <- c(52, 12)
ind_binary$pred <- list(low = pred_bin_low, high = pred_bin_high, close = pred_bin_close)
plot_neural_sample_histogram(eval_idx, ind_binary)
rm(list = c("pred_bin_low", "pred_bin_high", "pred_bin_close"))
```


Führt man die gleichen Analysen auch für das Modell durch, welches die Ordinalität der Klassen explizit modelliert, ergeben sich ähnliche Ergebnisse. Wie in Abbildung \@ref(fig:plot-binary-histogram) ersichtlich, weisen die beiden exemplarischen Einträge im Testdatensatz auch bei diesem Modell vergleichbare Preisverteilungen auf. Es scheint, als ob eine explizite Modellierung der Ordinalität nicht unbedingt nötig ist. Die Verteilung scheint auch ohne explizite Modellierung richtig gelernt worden zu sein.  



```{r, echo=FALSE, results='hide'}

ind_bin_buy_sell_max_prob_path <- "data/models/ind/binary/2_dense_512/ind_buy_sell_max_prob.rds"
if (file.exists(ind_bin_buy_sell_max_prob_path)) {
  ind_bin_buy_sell_max_prob <- read_feather(ind_bin_buy_sell_max_prob_path)
} else {
  n_eval <- length(test_idx)
  ind_bin_buy_sell_max_prob <- pbmclapply(seq_len(n_eval), function(i) {
    probs <- as.vector(pred_bin_low[i, ] %o% pred_bin_high[i, ] %o% pred_bin_close[i, ])[glob_scen$ID]
    calc_best_buy_sell(probs, max(probs), glob_scen$Low, glob_scen$High, glob_scen$Close, both_first = both_first[test_idx[i]])
  }, mc.cores = 3) %>% bind_rows()
  write_feather(ind_bin_buy_sell_max_prob, ind_bin_buy_sell_max_prob_path)
}

factor_ind_bin_max_prob <- sum(calc_payoff_const_gamma(data_test, buy = ind_bin_buy_sell_max_prob$buy, sell = ind_bin_buy_sell_max_prob$sell, both_first = both_first[test_idx])) / sum(calc_payoff_const_gamma(data_test, both_first = both_first[test_idx]))
saveRDS(factor_ind_bin_max_prob, "data/models/ind/binary/2_dense_512/factor_ind_bin_max_prob.rds")


```

```{r, echo=FALSE, results='hide'}

ind_bin_buy_sell_0_999_prob_path <- "data/models/ind/binary/2_dense_512/ind_bin_buy_sell_0_999_prob.rds"
if (file.exists(ind_bin_buy_sell_0_999_prob_path)) {
  ind_bin_buy_sell_0_999_prob <- read_feather(ind_bin_buy_sell_0_999_prob_path)
} else {
  n_eval <- length(test_idx)
  q <- 0.999
  ind_bin_buy_sell_0_999_prob <- pbmclapply(seq_len(n_eval), function(i) {
    probs <- as.vector(pred_bin_low[i, ] %o% pred_bin_high[i, ] %o% pred_bin_close[i, ])[glob_scen$ID]
    threshold <- quantile(probs, q)
    calc_best_buy_sell(probs, threshold, glob_scen$Low, glob_scen$High, glob_scen$Close, both_first = both_first[test_idx[i]])
  }, mc.cores = 2) %>% bind_rows()
  write_feather(ind_bin_buy_sell_0_999_prob, ind_bin_buy_sell_0_999_prob_path)
}

factor_ind_bin_buy_sell_0_999_prob <- sum(calc_payoff_const_gamma(data_test, buy = ind_bin_buy_sell_0_999_prob$buy, sell = ind_bin_buy_sell_0_999_prob$sell, both_first = both_first[test_idx])) / sum(calc_payoff_const_gamma(data_test, both_first = both_first[test_idx]))
saveRDS(factor_ind_bin_buy_sell_0_999_prob, "data/models/ind/binary/2_dense_512/factor_ind_bin_buy_sell_0_999_prob.rds")


```


Der Komplettheit halber seien die Überschusspayoffs des ordinalen Modelles an dieser Stelle dennoch aufgeführt. Diese betragen bei der Berücksichtigung des Buckets mit höchster Wahrscheinlichkeit `r round((factor_ind_bin_max_prob - 1) * 100, 1)`% und bei Berücksichtigung der 0.1% grössten Wahrscheinlichkeiten `r round((factor_ind_bin_buy_sell_0_999_prob - 1) * 100, 1)`%. Beide Werte sind damit etwas besser als bei der Modellierung ohne Ordinalität.


#### Abhängige Modelle

Die meisten Erkenntnisse aus der Analyse der unabhängigen Modelle lassen sich auch auf die abhängigen Modelle übertragen. Ein Unterschied liegt allerdings darin, dass sich jedes der 27'000 Buckets im abhängigen Fall nicht nur auf den Bereich eines Preises, sondern auf den Bereich aller der drei Preise für Tiefst-. Höchst- und Schlusskurs gleichzeitig bezieht. Die Bildung einer Reihenfolge der Buckets für eine ordinale Modellierung sind damit nicht mehr gegeben. Ein weiterer Unterschied ergibt sich bei der Berechnungsdauer des Modelles. Diese erhöht sich im Falle des abhängigen Modelles deutlich und es empfiehlt sich die Berechnung auf einer oder mehrerer GPUs, um die Berechnungsdauer zu verkürzen.


```{r, echo=FALSE, results='hide'}
n_groups_per_col <- 30

dep_categorical <-  NULL
dep_categorical$discretization <- multivariate_discretization(data_wide_10, train_idx, test_idx, c("Low_0", "High_0", "Close_0"), n_groups_per_col)

y <- dep_categorical$discretization$groups

model_dep_path  <- "data/models/dep/categorical/2_dense_512/model.h5"
history_dep_path  <- "data/models/dep/categorical/2_dense_512/history.rds"
if (file.exists(model_dep_path) && file.exists(history_dep_path)) {
  # model_dep <- load_model_hdf5(model_dep_path)
  model_dep <- load_model_hdf5("data/models/dep/categorical/2_dense_512/dep.04-9.31.hdf5")
  history_dep <- readRDS(history_dep_path)
} else {
  cp_callback_dep <- callback_model_checkpoint(
    filepath = file.path("data/models/dep/categorical/2_dense_512", "dep.{epoch:02d}-{val_loss:.2f}.hdf5")
  )
  model_dep <- fit_model_cat(x[train_idx, ], y[train_idx], callbacks = list(cp_callback_dep), epochs = 30, n_groups = 27000)
  save_model_hdf5(model_dep$model, model_dep_path)
  saveRDS(model_dep$history, history_dep_path)
}
```



```{r, dep-train-progress, fig.cap='Trainingsfortschritt im Falle abhängiger Preise', fig.height=3, echo=FALSE, message=FALSE, warning = FALSE} 
train_history <- as_tibble(history_dep) %>%
  mutate(data = do.call(recode, list(.x = data, validation = "Validierung", training = "Training")))

  train_history %>% 
  filter(metric == "loss") %>%
  ggplot(mapping = aes(x = epoch, y = value, color = data)) +
  geom_line() + 
  # facet_wrap(vars(price), ncol = 3,  scales = "free_y") +
  scale_x_continuous(labels = scales::number_format(accuracy = 1)) +
  ylab("Verlust") + 
  xlab("Epoche") +
  labs(color = "Daten") + 
  theme_bw()
```


Wie in Abbildung \@ref(fig:dep-train-progress) ersichtlich, zeigt sich ein weiterer Unterschied darin, dass es während des Trainings des abhängigen Modelles innerhalb der verwendeten 30 Epochen zu einem Overfitting kommt. Dies zeigt sich dadurch, dass sich sich der Verlust des Validierungsset nach Epoche 5 wieder deutlich erhöht. Wir entscheiden uns darum dafür, das Modell aus Epoche 5 zu verwenden.

```{r, echo=FALSE, results='hide'}
dep_categorical$discretization$borders
mid_dep_low <- calc_mid_prices(dep_categorical$discretization$borders$Low_0_lower, dep_categorical$discretization$borders$Low_0_upper)
mid_dep_high <- calc_mid_prices(dep_categorical$discretization$borders$High_0_lower, dep_categorical$discretization$borders$High_0_upper)
mid_dep_close <- calc_mid_prices(dep_categorical$discretization$borders$Close_0_lower, dep_categorical$discretization$borders$Close_0_upper)

n_eval <- length(test_idx)
dep_buy_sell_max_prob_path <- "data/models/dep/categorical/2_dense_512/dep_buy_sell_max_prob.rds"
if (file.exists(dep_buy_sell_max_prob_path)) {
  dep_buy_sell_max_prob <- read_feather(dep_buy_sell_max_prob_path)
} else {
  chunk_size <- 4000
  chunks <- chunk2(seq_len(length(test_idx)), ceiling(length(test_idx)/chunk_size))
  dep_buy_sell_max_prob <- map_df(seq_along(chunks), function(curr_chunk_id) {
    print(curr_chunk_id)
    curr_chunk <- chunks[[curr_chunk_id]]
    probs <- predict_proba(model_dep, x[test_idx[curr_chunk], ], batch_size = 512)
    pbmclapply(seq_along(curr_chunk), function(i) {
      calc_best_buy_sell(probs[i, ], max(probs[i, ]), mid_dep_low, mid_dep_high, mid_dep_close, both_first = both_first[test_idx[curr_chunk[i]]])
    }, mc.cores = 4) %>% bind_rows()
  })
  
  write_feather(dep_buy_sell_max_prob, dep_buy_sell_max_prob_path)
}

factor_dep_max_prob <- sum(calc_payoff_const_gamma(data_test, buy = dep_buy_sell_max_prob$buy, sell = dep_buy_sell_max_prob$sell, both_first = both_first[test_idx[seq_len(n_eval)]])) / sum(calc_payoff_const_gamma(data_test, both_first = both_first[test_idx[seq_len(n_eval)]]))
saveRDS(factor_dep_max_prob, "data/models/dep/categorical/2_dense_512/factor_dep_max_prob.rds")


```

Für dieses lassen sich die gleichen Auswertungen wie zuvor durchführen. Wiederum lässt sich zum Vergleich mit den andern Modellen der Überschusspayoff im Vergleich mit der Referenzstrategie ermitteln. Bei Verwendung des Buckets mit höchster Eintretenswahrscheinlichkeit liegt dieser bei `r round((factor_dep_max_prob - 1) * 100, 1)`% und vermag vorherige Modelle zu übertreffen. 

```{r, echo=FALSE, results='hide'}
n_eval <- length(test_idx)
dep_buy_sell_0_999_prob_path <- "data/models/dep/categorical/2_dense_512/dep_buy_sell_0_999_prob.rds"
if (file.exists(dep_buy_sell_0_999_prob_path)) {
  dep_buy_sell_0_999_prob <- read_feather(dep_buy_sell_0_999_prob_path)
} else {
  chunk_size <- 4000
  chunks <- chunk2(seq_len(length(test_idx)), ceiling(length(test_idx)/chunk_size))
  q <- 0.999
  dep_buy_sell_0_999_prob <- map_df(seq_along(chunks), function(curr_chunk_id) {
    print(curr_chunk_id)
    curr_chunk <- chunks[[curr_chunk_id]]
    probs <- predict_proba(model_dep, x[test_idx[curr_chunk], ], batch_size = 512)
    pbmclapply(seq_along(curr_chunk), function(i) {
      threshold <- quantile(probs[i, ], q)
      calc_best_buy_sell(probs[i, ], threshold, mid_dep_low, mid_dep_high, mid_dep_close, both_first = both_first[test_idx[curr_chunk[i]]])
    }, mc.cores = 4) %>% bind_rows()
  })
  
  write_feather(dep_buy_sell_0_999_prob, dep_buy_sell_0_999_prob_path)
}


factor_dep_0_999_prob <- sum(calc_payoff_const_gamma(data_test, buy = dep_buy_sell_0_999_prob$buy, sell = dep_buy_sell_0_999_prob$sell, both_first = both_first[test_idx[seq_len(n_eval)]])) / sum(calc_payoff_const_gamma(data_test, both_first = both_first[test_idx[seq_len(n_eval)]]))
saveRDS(factor_dep_0_999_prob, "data/models/dep/categorical/2_dense_512/factor_dep_0_999_prob.rds")


```

Werden wiederum gar die Buckets mit höchsten 0.1% Eintretenswahrscheinlichkeiten beigezogen, resultiert gar ein Überschusspayoff von  `r round((factor_dep_0_999_prob - 1) * 100, 1)`%. Es handelt sich damit innerhalb der analysierten Modelle um dasjenige Modell mit der besten Performance.

```{r, echo=FALSE, results='hide'}
rm(list = ls())
gc()
```