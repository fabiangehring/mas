<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Kapitel 5 Neuronale Netzwerke | Die Bestimmung optimaler Kauf- und Verkaufspreise von Basiswerten zur Wahrung der täglichen Delta-Neutralität beim Handel von Aktionoptionen mit Hilfe verschiedener Machine Learning Methoden</title>
  <meta name="description" content="Kapitel 5 Neuronale Netzwerke | Die Bestimmung optimaler Kauf- und Verkaufspreise von Basiswerten zur Wahrung der täglichen Delta-Neutralität beim Handel von Aktionoptionen mit Hilfe verschiedener Machine Learning Methoden" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Kapitel 5 Neuronale Netzwerke | Die Bestimmung optimaler Kauf- und Verkaufspreise von Basiswerten zur Wahrung der täglichen Delta-Neutralität beim Handel von Aktionoptionen mit Hilfe verschiedener Machine Learning Methoden" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Kapitel 5 Neuronale Netzwerke | Die Bestimmung optimaler Kauf- und Verkaufspreise von Basiswerten zur Wahrung der täglichen Delta-Neutralität beim Handel von Aktionoptionen mit Hilfe verschiedener Machine Learning Methoden" />
  
  
  

<meta name="author" content="Fabian Gehring, Zürcher Hochschule für Angewandte Wissenschaften" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="analyse.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> - </a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Einleitung</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#motivation"><i class="fa fa-check"></i><b>2.1</b> Motivation</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#forschungsfrage"><i class="fa fa-check"></i><b>2.2</b> Forschungsfrage</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#daten"><i class="fa fa-check"></i><b>2.3</b> Daten</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#methoden"><i class="fa fa-check"></i><b>2.4</b> Methoden</a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#disclaimer"><i class="fa fa-check"></i><b>2.5</b> Disclaimer</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="daten-1.html"><a href="daten-1.html"><i class="fa fa-check"></i><b>3</b> Daten</a><ul>
<li class="chapter" data-level="3.1" data-path="daten-1.html"><a href="daten-1.html#bezug-und-umfang"><i class="fa fa-check"></i><b>3.1</b> Bezug und Umfang</a><ul>
<li class="chapter" data-level="3.1.1" data-path="daten-1.html"><a href="daten-1.html#aktienuniversum"><i class="fa fa-check"></i><b>3.1.1</b> Aktienuniversum</a></li>
<li class="chapter" data-level="3.1.2" data-path="daten-1.html"><a href="daten-1.html#preisinformationen"><i class="fa fa-check"></i><b>3.1.2</b> Preisinformationen</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="daten-1.html"><a href="daten-1.html#aufbereitung"><i class="fa fa-check"></i><b>3.2</b> Aufbereitung</a><ul>
<li class="chapter" data-level="3.2.1" data-path="daten-1.html"><a href="daten-1.html#bereinigung"><i class="fa fa-check"></i><b>3.2.1</b> Bereinigung</a></li>
<li class="chapter" data-level="3.2.2" data-path="daten-1.html"><a href="daten-1.html#normalisierung"><i class="fa fa-check"></i><b>3.2.2</b> Normalisierung</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="analyse.html"><a href="analyse.html"><i class="fa fa-check"></i><b>4</b> Analyse</a><ul>
<li class="chapter" data-level="4.1" data-path="analyse.html"><a href="analyse.html#einfache-optimierungen"><i class="fa fa-check"></i><b>4.1</b> Einfache Optimierungen</a></li>
<li class="chapter" data-level="4.2" data-path="analyse.html"><a href="analyse.html#klassifikationsverfahren"><i class="fa fa-check"></i><b>4.2</b> Klassifikationsverfahren</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="neuronale-netzwerke.html"><a href="neuronale-netzwerke.html"><i class="fa fa-check"></i><b>5</b> Neuronale Netzwerke</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Die Bestimmung optimaler Kauf- und Verkaufspreise von Basiswerten zur Wahrung der täglichen Delta-Neutralität beim Handel von Aktionoptionen mit Hilfe verschiedener Machine Learning Methoden</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="neuronale-netzwerke" class="section level1">
<h1><span class="header-section-number">Kapitel 5</span> Neuronale Netzwerke</h1>
<!-- ```{r, keras-setup, echo=FALSE} -->
<!-- data_wide_3 <- arrow::read_feather("data/data_wide_3.feather") -->
<!-- data_wide_3_all <- arrow::read_feather("data/data_wide_3_all.feather") -->
<!-- set.seed(321654) -->
<!-- train_idx <- sample(x = nrow(data_wide_3), size = floor(0.8 * nrow(data_wide_3))) -->
<!-- test_idx <- setdiff(seq_len(nrow(data_wide_3)), train_idx) -->
<!-- ``` -->
<!-- ```{r, keras-individual-categorial, echo=FALSE} -->
<!-- fit_categorical_model <- function(data_all, data, train_idx, test_idx, cols, n_groups_per_col) { -->
<!--   all_data <- as.matrix(dplyr::select(data_wide_3, -Ticker, -Date)) -->
<!--   train_data <- all_data[train_idx, ] -->
<!--   test_data <- all_data[test_idx, ] -->
<!--   all_labels <- multivariate_discretization(data_all, train_idx, test_idx, cols, n_groups_per_col) %$% groups -->
<!--   train_labels <- all_labels[train_idx] -->
<!--   test_labels <- all_labels[test_idx] -->
<!--   model <- keras::keras_model_sequential() %>% -->
<!--     keras::layer_dense(units = 512, activation = "relu",  input_shape = dim(train_data)[2]) %>% -->
<!--     keras::layer_dense(units = 512, activation = "relu") %>% -->
<!--     keras::layer_dense(units = n_groups_per_col^length(cols), activation = "softmax") -->
<!--   model %>% keras::compile( -->
<!--     optimizer = 'adam',  -->
<!--     loss = 'sparse_categorical_crossentropy', -->
<!--     metrics = c('accuracy') -->
<!--   ) -->
<!--   history <- model %>% keras::fit( -->
<!--     train_data, -->
<!--     train_labels, -->
<!--     epochs = 10, -->
<!--     batch_size = 512,  -->
<!--     validation_split = 0.2 -->
<!--   ) -->
<!--   return(list(model = model, history = history)) -->
<!-- } -->
<!-- low_win_3_bin_30 <- fit_categorical_model(data_wide_3_all, data_wide_3, train_idx, test_idx, "Low_0", 30) -->
<!-- save_model_hdf5(low_win_3_bin_30$model, "data/low_win_3_bin_30_model.hdf5") -->
<!-- saveRDS(low_win_3_bin_30$history, "data/low_win_3_bin_30_history.rds") -->
<!-- # low_win_3_bin_30$model <- load_model_hdf5("data/low_win_3_bin_30_model.hdf5") -->
<!-- high_win_3_bin_30 <- fit_categorical_model(data_wide_3_all, data_wide_3, train_idx, test_idx, "High_0", 30) -->
<!-- save_model_hdf5(high_win_3_bin_30$model, "data/high_win_3_bin_30_model.hdf5") -->
<!-- saveRDS(high_win_3_bin_30$history, "data/high_win_3_bin_30_history.rds") -->
<!-- # high_win_3_bin_30$model <- load_model_hdf5("data/high_win_3_bin_30_model.hdf5") -->
<!-- close_win_3_bin_30 <- fit_categorical_model(data_wide_3_all, data_wide_3, train_idx, test_idx, "Close_0", 30) -->
<!-- save_model_hdf5(close_win_3_bin_30$model, "data/close_win_3_bin_30_model.hdf5") -->
<!-- saveRDS(close_win_3_bin_30$history, "data/close_win_3_bin_30_history.rds") -->
<!-- # close_win_3_bin_30$model <- load_model_hdf5("data/close_win_3_bin_30_model.hdf5") -->
<!--  low_binary_win_3_bin_30 <- fit_binary_model(data_wide_3_all, data_wide_3, train_idx, test_idx, "Low_0", 30) -->
<!--   save_model_hdf5(low_binary_win_3_bin_30$model, "data/low_binary_win_3_bin_30_model.hdf5") -->
<!--   saveRDS(low_binary_win_3_bin_30$history, "data/low_binary_win_3_bin_30_history.rds") -->
<!--   high_binary_win_3_bin_30 <- fit_binary_model(data_wide_3_all, data_wide_3, train_idx, test_idx, "High_0", 30) -->
<!--   save_model_hdf5(high_binary_win_3_bin_30$model, "data/high_binary_win_3_bin_30_model.hdf5") -->
<!--   saveRDS(high_binary_win_3_bin_30$history, "data/high_binary_win_3_bin_30_history.rds") -->
<!--   close_binary_win_3_bin_30 <- fit_binary_model(data_wide_3_all, data_wide_3, train_idx, test_idx, "Close_0", 30) -->
<!--   save_model_hdf5(close_binary_win_3_bin_30$model, "data/close_binary_win_3_bin_30_model.hdf5") -->
<!--   saveRDS(close_binary_win_3_bin_30$history, "data/close_binary_win_3_bin_30_history.rds") -->
<!-- ``` -->
<!-- ```{python} -->
<!-- import numpy as np -->
<!-- my_python_array = np.array([2,4,6,8]) -->
<!-- for item in my_python_array: -->
<!--     print(item) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- py$my_python_array -->
<!-- ``` -->
<!-- ```{r, keras-individual-categorial-evaluation, echo=FALSE} -->
<!-- get_class_prices <- function(borders) { -->
<!--   class_borders <- borders %>% set_names(c("Bucket", "Lower_Border", "Upper_Border")) -->
<!--   class_borders[1, "Lower_Border"] <- class_borders[1, "Upper_Border"] -->
<!--   class_borders[nrow(class_borders), "Upper_Border"] <- class_borders[nrow(class_borders), "Lower_Border"] -->
<!--   (class_borders$Lower_Border + class_borders$Upper_Border) / 2 -->
<!-- } -->
<!-- test_data <- as.matrix(dplyr::select(data_wide_3, -Ticker, -Date))[test_idx, ] -->
<!-- n_groups_per_col <- 100 -->
<!-- curr_eval_id <- 781 -->
<!-- # buy prices -->
<!-- model_low <- load_model_hdf5("models/model_low_100.hdf5") -->
<!-- pred_prob_low <- predict_proba(model_low, test_data[curr_eval_id, , drop = FALSE], batch_size = 512) -->
<!-- # pred_classes_low <- predict_classes(model_low, test_data, batch_size = 512) -->
<!-- discretization_low <- multivariate_discretization(data_wide_3_all, train_idx, test_idx, "Low_0", n_groups_per_col) -->
<!-- hist_data_low <- discretization_low$borders %>%  -->
<!--   mutate(prob = as.vector(pred_prob_low)) %>%  -->
<!--   rename_at(dplyr::vars(tidyselect::ends_with("lower")), ~"lower") %>% -->
<!--   rename_at(dplyr::vars(tidyselect::ends_with("upper")), ~"upper") %>% -->
<!--   select("lower", "upper", "prob") %>% -->
<!--   mutate(group = "Low") -->
<!-- # sell prices -->
<!-- model_high <- load_model_hdf5("models/model_high_100.hdf5") -->
<!-- pred_prob_high <- predict_proba(model_high, test_data[curr_eval_id, , drop = FALSE], batch_size = 512) -->
<!-- # pred_classes_high <- predict_classes(model_high, test_data, batch_size = 512) -->
<!-- discretization_high <- multivariate_discretization(data_wide_3_all, train_idx, test_idx, "High_0", n_groups_per_col) -->
<!-- hist_data_high <- discretization_high$borders %>%  -->
<!--   mutate(prob = as.vector(pred_prob_high)) %>%  -->
<!--   rename_at(dplyr::vars(tidyselect::ends_with("lower")), ~"lower") %>% -->
<!--   rename_at(dplyr::vars(tidyselect::ends_with("upper")), ~"upper") %>% -->
<!--   select("lower", "upper", "prob") %>% -->
<!--   mutate(group = "High") -->
<!-- plot_price_histogram(bind_rows(hist_data_low, hist_data_high), "Some test") -->
<!-- test <- expand.grid( -->
<!--   Close_1 = 100, -->
<!--   Low = discretization_low$borders$Low_0_lower, -->
<!--   High = discretization_high$borders$High_0_lower, -->
<!--   Close_0 = (discretization_low$borders$Low_0_lower + discretization_high$borders$High_0_lower) / 2 -->
<!-- ) -->
<!-- test$Low <- ifelse(!is.finite(test$Low ), 97, test$Low) -->
<!-- test$High <- ifelse(!is.finite(test$Low ), 103, test$Low) -->
<!-- test$Close_0 <- ifelse(!is.finite(test$Low ), 100, test$Low) -->
<!-- set.seed(654951) -->
<!-- both_first <- c("buy", "sell")[sample(c(1, 2), nrow(test), replace = TRUE)] -->
<!-- calc_payoff_const_gamma(tibble(Close_1 = 100, Low = 102, High = 105, Close_0 = 103), buy = -Inf, sell = Inf, both_first = "buy") -->
<!-- microbenchmark::microbenchmark({ -->
<!-- sum(calc_payoff_const_gamma(test, buy = 97, sell = 103, both_first = both_first)) -->
<!-- }) -->
<!--   sell <- get_class_prices(discretization_high$borders, pred_classes_high) -->
<!--   eval_data <- data_wide_3_all[test_idx, ] %>% select(Close_1, Open = Open_0, Low = Low_0, High = High_0, Close_0) -->
<!--   data_wide_3_all[test_idx, ][1, ] -->
<!--   buy[1] -->
<!--   sell[1] -->
<!--   scale_fct <- sum(calc_payoff_const_gamma(eval_data, both_first = both_first)) -->
<!--   sum(calc_payoff_const_gamma(eval_data, buy = buy, sell = sell, both_first = both_first)) / scale_fct  -->
<!--   # max -->
<!--   sum(calc_payoff_const_gamma(eval_data, buy = eval_data$Low, sell = eval_data$High, both_first = both_first)) / scale_fct  -->
<!--   # open -->
<!--   sum(calc_payoff_const_gamma(eval_data, buy = eval_data$Open * (1 - 0.045), sell = eval_data$Open * (1 + 0.045), both_first = both_first)) / scale_fct  -->
<!--   ``` -->
<!--   ```{r, keras-combined-individual-binary, echo=FALSE} -->
<!--   fit_binary_model <- function(data_all, data, train_idx, test_idx, cols, n_groups_per_col) { -->
<!--     # https://www.cs.waikato.ac.nz/~eibe/pubs/ordinal_tech_report.pdf -->
<!--     # http://orca.st.usm.edu/~zwang/files/rank.pdf -->
<!--     # https://machinelearningmastery.com/how-to-prepare-categorical-data-for-deep-learning-in-python/ -->
<!--     all_data <- as.matrix(dplyr::select(data_wide_3, -Ticker, -Date)) -->
<!--     train_data <- all_data[train_idx, ] -->
<!--     all_labels <- multivariate_discretization(data_all, train_idx, test_idx, cols, n_groups_per_col) %$% groups -->
<!--     train_labels <- all_labels[train_idx] -->
<!--     # make them "ordinal" -->
<!--     train_labels <- purrr::map(seq_len(n_groups_per_col) - 1, ~as.integer(.<=train_labels)) %>% unlist() %>% matrix(., ncol = n_groups_per_col) -->
<!--     model <- keras::keras_model_sequential() %>% -->
<!--       keras::layer_dense(units = 512, activation = "relu",  input_shape = dim(train_data)[2]) %>% -->
<!--       keras::layer_dense(units = 512, activation = "relu") %>% -->
<!--       keras::layer_dense(units = n_groups_per_col^length(cols), activation = "sigmoid") -->
<!--     model %>% keras::compile( -->
<!--       optimizer = 'adam',  -->
<!--       loss = 'binary_crossentropy', -->
<!--       metrics = c('accuracy') -->
<!--     ) -->
<!--     history <- model %>% keras::fit( -->
<!--       train_data, -->
<!--       train_labels, -->
<!--       epochs = 10, -->
<!--       batch_size = 512,  -->
<!--       validation_split = 0.2 -->
<!--     ) -->
<!--     return(list(model = model, history = history)) -->
<!--   } -->
<!--  low_binary_win_3_bin_30 <- fit_binary_model(data_wide_3_all, data_wide_3, train_idx, test_idx, "Low_0", 30) -->
<!--   save_model_hdf5(low_binary_win_3_bin_30$model, "data/low_binary_win_3_bin_30_model.hdf5") -->
<!--   saveRDS(low_binary_win_3_bin_30$history, "data/low_binary_win_3_bin_30_history.rds") -->
<!--   high_binary_win_3_bin_30 <- fit_binary_model(data_wide_3_all, data_wide_3, train_idx, test_idx, "High_0", 30) -->
<!--   save_model_hdf5(high_binary_win_3_bin_30$model, "data/high_binary_win_3_bin_30_model.hdf5") -->
<!--   saveRDS(high_binary_win_3_bin_30$history, "data/high_binary_win_3_bin_30_history.rds") -->
<!--   close_binary_win_3_bin_30 <- fit_binary_model(data_wide_3_all, data_wide_3, train_idx, test_idx, "Close_0", 30) -->
<!--   save_model_hdf5(close_binary_win_3_bin_30$model, "data/close_binary_win_3_bin_30_model.hdf5") -->
<!--   saveRDS(close_binary_win_3_bin_30$history, "data/close_binary_win_3_bin_30_history.rds") -->
<!-- ``` -->
<!--   ```{r, keras-combined-softmax-no-saampling, echo=FALSE} -->
<!--   n_groups_per_col <- 10 -->
<!--   cols <- c("Low_0", "High_0", "Close_0") -->
<!--   all_labels <- multivariate_discretization(data_wide_3_all, train_idx, test_idx, cols, n_groups_per_col) %$% groups -->
<!--   train_labels <- all_labels[train_idx] -->
<!--   test_labels <- all_labels[test_idx] -->
<!--   model <- keras::keras_model_sequential() %>% -->
<!--     keras::layer_dense(units = 512, activation = "relu",  input_shape = dim(train_data)[2]) %>% -->
<!--     keras::layer_dense(units = 512, activation = "relu") %>% -->
<!--     keras::layer_dense(units = n_groups_per_col^length(cols), activation = "softmax") -->
<!--   model %>% keras::compile( -->
<!--     optimizer = 'adam',  -->
<!--     loss = 'sparse_categorical_crossentropy', -->
<!--     metrics = c('accuracy') -->
<!--   ) -->
<!--   history <- model %>% keras::fit( -->
<!--     train_data, -->
<!--     train_labels, -->
<!--     epochs = 2, -->
<!--     batch_size = 512,  -->
<!--     validation_split = 0.2 -->
<!--   ) -->
<!--   summary(model) -->
<!--   keras::save_model_hdf5(model, "data/2_dense_512_window_3_epoch_50_batch_128_val_split_20.hdf5") -->
<!--   print("done") -->
<!--   ``` -->
<!--   ##### Old -->
<!--   ```{r, knn-calculation, echo=FALSE} -->
<!--   # data_bkp <- data -->
<!--   # data <- data_bkp -->
<!--   orig_order <- order(desc(data$Date)) -->
<!--   data <- data %>% select(-Ticker) %>% arrange(desc(Date)) -->
<!--   counts <- data %>% select(Date) %>% group_by(Date) %>% summarise(cnt = n()) %>% ungroup() %>% mutate(cum_sum = cumsum(cnt)) -->
<!--   n_chunks <- 100 -->
<!--   breaks <- nrow(data) / n_chunks * seq_len(n_chunks) -->
<!--   split_dates <- map(breaks, ~counts$Date[[min(which(counts$cum_sum>=.))]]) -->
<!--   nn <- function(i, split_dates, data, dates){ -->
<!--     split_date <- split_dates[[i]] -->
<!--     curr_data <- data[dates <= split_date, ] -->
<!--     curr_dates <- dates[dates <= split_date] -->
<!--     curr_query <- data[dates <= split_date & dates > ifelse(i == 1, -Inf, split_dates[[i-1]]), ] -->
<!--     RANN2::nn2_cpp2(data = curr_data, query = curr_query, group = as.integer(curr_dates), k = 50, k_internal = 1.2*50) -->
<!--   } -->
<!--   knn_eucl_list <- pbmcapply::pbmclapply( -->
<!--     X = rev(seq_len(n_chunks)), -->
<!--     FUN = nn, -->
<!--     split_dates = split_dates, -->
<!--     data = as.matrix(select(data, -Date)), -->
<!--     dates = data$Date, -->
<!--     mc.cores = parallel::detectCores() -->
<!--   ) -->
<!--   knn_eucl_list_hash <- digest::digest(knn_eucl_list) -->
<!--   # saveRDS(knn_eucl_list_hash, paste0("tmp/knn_eucl_list_", knn_eucl_list_hash, ".rds")) -->
<!--   knn_eucl <- purrr::transpose(knn_eucl_list) %>% map(~do.call(rbind, .[rev(seq_along(.))])) -->
<!--   knn_eucl_hash <- digest::digest(knn_eucl) -->
<!--   # arrow::write_feather(as_tibble(knn_eucl$nn.idx), paste0("tmp/knn_eucl_idx_", knn_eucl_hash, ".feather")) -->
<!--   # arrow::write_feather(as_tibble(knn_eucl$nn.dists), paste0("tmp/knn_eucl_dists_", knn_eucl_hash, ".feather")) -->
<!--   ``` -->
<!--   ```{r, knn-single-plot, echo=FALSE} -->
<!--   knn_eucl_idx <- arrow::read_feather("tmp/knn_eucl_idx_7d3356cf64a5f81081840f6b1b370d77.feather") -->
<!--   knn_eucl_dists <- arrow::read_feather("tmp/knn_eucl_dists_7d3356cf64a5f81081840f6b1b370d77.feather") -->
<!--   idx <- 123456  -->
<!--   knn_eucl_idx[idx, ] -->
<!--   knn_eucl_dists[idx, ] -->
<!--   select(data[idx, ], -c("Ticker", "Date")) -->
<!--   select(data[head(as.integer(knn_eucl_idx[idx, ]), 1), ], -c("Ticker", "Date")) -->
<!--   sqrt(sum((select(data[idx, ], -c("Ticker", "Date")) - select(data[head(as.integer(knn_eucl_idx[idx, ]), 1), ], -c("Ticker", "Date")))^2)) -->
<!--   id_cols <- c("Ticker", "Date") -->
<!--   knn <- RANN::nn2( -->
<!--     data = select(data, -id_cols), -->
<!--     query = select(data[idx, ], -id_cols), -->
<!--     k = 10 -->
<!--   ) -->
<!--   plot_nn( -->
<!--     data_wide_curr = data_all[idx, ], -->
<!--     data_wide_nn = data_all[c(3660437, 2876442, 1411227), ] -->
<!--   ) -->
<!--   # id <- 280000 -->
<!--   # valid_idx <- seq_len(n)[rowSums(is.na(nn$nn.idx)) == 0] -->
<!--   #  -->
<!--   # k <- 10 -->
<!--   # plot_nn(data_wide_0[valid_idx[id], ], data_wide_0[nn$nn.idx[valid_idx[id], seq_len(k)],]) -->
<!--   ``` -->
<!--   ```{r, knn-prediction-power, echo=FALSE} -->
<!--   # k <- 20 -->
<!--   #  -->
<!--   # nn_idx <- as.matrix(arrow::read_arrow("data/nn_idx_eucl_olhc_w3_38a896430298c738055505dc89e042ac.feather")) -->
<!--   #  -->
<!--   # nn_pred <- pred_nn(select(data_wide_0, c("Low_0", "High_0")), nn_idx = nn_idx[, seq_len(k)]) -->
<!--   # na_row_bool <- rowSums(is.na(nn_pred)) > 0 -->
<!--   #  -->
<!--   # nn_pred_sample <- nn_pred[!na_row_bool, ] %>% rename(Buy = Low_0, Sell = High_0) -->
<!--   # quotes_line_sample <- quotes_line[!na_row_bool, ] -->
<!--   #  -->
<!--   # plot_ratio_history(quotes_line = quotes_line_sample, data_pred = nn_pred_sample) -->
<!--   #  -->
<!--   #  -->
<!--   # ### perform bootstraping -->
<!--   # size_map <- quotes_line %>% -->
<!--   #   select(Date) %>% -->
<!--   #   group_by(Date) %>%  -->
<!--   #   summarize(count = n()) %>%  -->
<!--   #   ungroup() %>%  -->
<!--   #   mutate(size = cumsum(count) - count) %>%  -->
<!--   #   select(-count) -->
<!--   # size <- quotes_line %>% select(Date) %>% left_join(size_map, by = "Date") %>% .[["size"]] -->
<!--   # ### -->
<!--   #  -->
<!--   # boot_nn_idx_1 <- bootstrap_nn_idx(nn_idx, size, 20, 123456) -->
<!--   #  -->
<!--   # nn_pred_boot <- pred_nn(select(data_wide_0, c("Low_0", "High_0")), nn_idx = boot_nn_idx_1) -->
<!--   # na_row_bool_boot <- rowSums(is.na(nn_pred_boot)) > 0 -->
<!--   #  -->
<!--   # nn_pred_boot_sample <- nn_pred_boot[!na_row_bool_boot, ] %>% rename(Buy = Low_0, Sell = High_0) -->
<!--   # quotes_line_boot_sample <- quotes_line[!na_row_bool_boot, ] -->
<!--   #  -->
<!--   # plot_ratio_history(quotes_line = quotes_line_boot_sample, data_pred = nn_pred_boot_sample) -->
<!--   #  -->
<!--   #  -->
<!--   ``` -->
<!--   ```{r, knn-bootstraping, echo=FALSE} -->
<!--   # k <- 20 -->
<!--   # quotes_line <- readRDS("tmp/quotes_line.rds") -->
<!--   # quotes_line_sorted <- quotes_line %>% arrange(Date) -->
<!--   #  -->
<!--   # nn_idx <- arrow::read_arrow("data/nn_idx_eucl_olhc_w3_38a896430298c738055505dc89e042ac.feather") -->
<!--   #  -->
<!--   # nn_idx_sorted <- readRDS("data/nn_eucl_olhc_w3_38a896430298c738055505dc89e042ac.rds") %>% .[["nn.idx"]] %>% sort_nn_idx(quotes_line$Date) -->
<!--   #  -->
<!--   #  -->
<!--   # set.seed(123456) -->
<!--   # boot_pred <- map(seq_len(100), function(i) { -->
<!--   #   print(i) -->
<!--   #   curr_nn_idx_boot <- bootstrap_nn(sort(quotes_line$Date), sort_nn_idx(nn_idx_sorted), k = k) -->
<!--   #   pred_nn(select(quotes_line_sorted, c("Low", "High")), nn_idx = curr_nn_idx_boot) -->
<!--   # }) -->
<!--   # # saveRDS(boot_pred, "tmp/boot_pred.rds") -->
<!--   #  -->
<!--   # boot_pred <- readRDS("tmp/boot_pred.rds") -->
<!--   #  -->
<!--   #  -->
<!--   # all_complete <- rep(TRUE, nrow(quotes_line_sorted)) -->
<!--   # for(i in seq_along(boot_pred)) { -->
<!--   #   print(i) -->
<!--   #   all_complete <- all_complete * rowSums(is.na(boot_pred[[i]])) == 0 -->
<!--   #   gc() -->
<!--   # } -->
<!--   #  -->
<!--   # na_row_bool <- rowSums(is.na(nn_idx_sorted)) + rowSums(is.na()) -->
<!--   #  -->
<!--   # test <- map(boot_pred, ~sum(calc_payoff_const_gamma(quotes_line_sorted[all_complete], buy = .$Low, sell = .$High, both_first = 234567))) -->
<!--   #  -->
<!--   # na_row_bool <- rowSums(is.na(nn$nn.idx)) == 0 -->
<!--   # nn_idx[4418184, ] -->
<!--   # quotes_line[4418184, ] -->
<!--   ``` -->
<!--   ## Neuronale Netzwerke -->
<!--   ## Autoregressive Modelle -->

</div>
            </section>

          </div>
        </div>
      </div>
<a href="analyse.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
