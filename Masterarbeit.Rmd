---
title: "Die Bestimmung optimaler Kauf- und Verkaufspreise von Basiswerten zur Wahrung der täglichen Delta-Neutralität beim Handel von Aktionoptionen mit Hilfe verschiedener Machine Learning Methoden"
author: "Fabian Gehring, Zürcher Hochschule für Angewandte Wissenschaften"
site: bookdown::bookdown_site
documentclass: scrartcl
bibliography: ["mas.bib"]
biblio-style: apalike
link-citations: yes
header-includes:
  - \usepackage[ngerman]{babel}
  - \usepackage{float}
#  - \floatplacement{figure}{H} 

---


```{r setup_index, include = FALSE}
packages <- c(
  "bookdown",
  "dplyr",
  "here",
  "jsonlite",
  "magrittr",
  "purrr",
  "quantmod",
  "qrmtools",
  "renv",
  "roxygen2",
  "stringr",
  "tibble"
)

# load packages
suppressMessages({
  for (pkg in packages) {
    library(pkg, character.only = TRUE)
  }
})

# load sources
for (file in list.files(here("R"), full.names = TRUE)) source(file)
```


# Management Summary {-#summary}

TODO


<!--chapter:end:index.Rmd-->

# Einleitung {#intro}

## Motivation
Aktienoptionen sind derivative Finanzinstrumente, welche dem Inhaber das Recht (nicht aber die Pflicht) geben, die zugrunde liegende Aktie (auch Basiswert oder Underlying) zu einem im Voraus festgelegten Ausübungspreis (auch Strike) zu kaufen oder zu verkaufen. Wie sich der Preis einer Option entwickelt, ist dabei von verschiedenen Faktoren abhängig. Dazu gehören unter anderem der Preis des Basiswertes, dessen Volatilität oder die Restlaufzeit der Option.

Der Zusammenhang zwischen Preis des Underlyings und Preis der Option ist nicht linear, sondern nimmt bei steigendem Preis zu (Kaufoption), resp. ab (Verkaufsoption). Diese nicht-Linearität wird mit Hilfe der Kennzahl Gamma ($\Gamma$) beschrieben. Im profesionellen Optionenhandel ist es üblich, den linearen Teil der Veränderung (Delta, auch $\Delta$) durch Kauf oder Verkauf des Underlyings abzusichern. Damit lassen sich Strategien verfolgen, welche von der Richtung der Preisbewegung unabhängig sind (Delta-Neutral Trading). Der Nicht-lineare Zusammenhang führt allerdings dazu, dass die Delta-Neutralität mit sich ändernden Preisen wieder verloren geht. Dies sein an einem Beispiel demonstriert. 


Sei dafür angenommen, dass es eine Aktie zum Preis von 10 gehandelt wird, welche als Basiswert sowohl für eine Kaufoption (Call) wie auch eine Verkaufsoption (Put) dient. Die Laufzeit betrage 6 Monate, der Strike ebenfalls 10. Mit dem Kauf der Option erwirbt der Besitzer damit das Recht, die Aktie in 6 Monaten zu 10 zu kaufen (im Fall des Calls) oder zu verkaufen (im Falle des Puts). In diesem Beispiel entspricht der Aktienkurs dem Strike. Solche Optionen bezeichnet man als "at-the-money (atm)". Bei Veränderung des Basispreises werden diese zu "in-the-money (itm)" oder "out-of-the-money (otm)" Optionen.


``` {r, setup_intro, echo=FALSE, message=FALSE, warning=FALSE}
suppressPackageStartupMessages({
  source("R/01-intro.R")
})
```

```{r call-put, fig.cap='Zusammenhang zwischen Preis der Underlyings und Preis der Option.', fig.asp=0.6, fig.pos = '!H', echo=FALSE, warning=FALSE, message=FALSE}
t <- 0
S <- seq(5, 15, 0.01)
r <- 0.01
sigma = 0.5
K = 10
T = 0.5
move = 0.2

delta_atm_call <- qrmtools::Black_Scholes(t, K, r, sigma, K, T, "call")
delta_atm_put <- qrmtools::Black_Scholes(t, K, r, sigma, K, T, "put")

par(mfrow = c(1, 2))
plot_call_price(t, S, r, sigma, K, T, move, "call")
plot_call_price(t, S, r, sigma, K, T, move, "put")
par(mfrow = c(1, 1))
```

Den Zusammenhang zwischen Preis der Option und Preis des Underlyings ist in \@ref(fig:call-put) abgebildet. Die gestrichelten Linien bilden die Tangenten am  Strikepreis der Optionen. Deren Steigungen ($\Delta$) betragen `r round(delta_atm_call, 2)` und im Falle des Calls, und `r round(-delta_atm_put, 2)` im Falle des Puts. Dies bedeutet, dass sich der Besitzer der Option sich gegen Preisbewegungen absichern (hegden) kann, indem zusätzlich zum Besitz der Option diese Anzahl an Aktien verkauft (Call), resp. kauft (Put). Wertänderungen der Option werden dann durch die entgegenläufige Wertenwicklung in der Aktienposition ausgeglichen. Eine solche Position wird "deltaneutral" genannt. Bei grösseren Preisbewegungen führt die Konvexität des Optionspreises allerdings dazu, dass es trotz Absicherung zu einer Abweichung kommt. Diese Fehler sind in Abbildung \@ref(fig:call-put) mit roten Linien dargestellt. Es mag auf den ersten Blick erstaunen, dass diese Abweichung immer zugunsten des Optionsbesitzers ausfällt (sowohl für Call- als auch Put-Option verlaufen die Tangenten unterhalb des Optionspreises). Dass es sich dabei aber nicht eine Geldmaschine handelt liegt daran, dass die beschriebenen Preisbewegungen erst im Laufe der Zeit erfolgen und die Restlaufzeit der Option kleiner wird (Theta, auch $\Theta$). Mit sinkender Restlaufzeit auch der Wert der Option ab. In effizienten Märkten gleichen sich diese Effekte im Durchschnitt aus.

Wenn die zukünftigen Bewegungen des Preises aber grösser sind, als dies durch den Markt mit dem Wertverlust aufgrund der Zeit einpreist, kann die Verfolgung einer solchen Handelsstrategie allerdings durchaus lukrativ sein. Die Schwierigkeit liegt hier natürlich bei der Auswahl der richtigen Titel. Diese Strategie wird als Gamma-Scalping bezeichnet. Sie verdankt ihren Namen der zweiten Ableitung des Optionspreises nach dem Underlyingpreis - dem Gamma ($\Gamma$) - welches die Nichtlinearität der Beziehung beschreibt.

Der Einfluss des Gammas ist aber nicht nur zu Spekulationszwecken interessant. Dessen Bewirtschaftung ist auch für die Steuerung des eingegangenen Risikos von Relevanz. Soll nämlich die Delta-Neutralität nach Preisbewegungen wieder hergestellt werden, müssen laufend neue Titel des Underlyings gekauft oder verkauft werden. Grafisch entspricht dies der Einnahme einer Aktienposition, welche wieder der Steigung der Tangente beim nun vorherrschenden Preis entspricht. Typischerweise ist es so, dass diese Anpassungen abends (zur Reduktion des Risikos über Nacht) oder je nach Entscheidung des Händlers auch untertags erfolgen. Der Händler wird dabei versuchen, den Ausgleich nach möglichst grosser Preisbewegung zu machen, d.h. negatives Delta am Tiefstpunkt zu kaufen resp. positives Delta am Höchstpunkt zu verkaufen. Verpasst er diesen Zeitpunkt und die Preise entwickeln sich wieder Richtung Ausgangspunkt, so entgehen ihm mögliche Gewinne.


## Forschungsfrage
Als Forschungsfrage der vorliegenden Arbeit ergibt sich damit folgende Forschungsfrage:

Kann mit Hilfe datengestützter Methoden eine Strategie gefunden werden, welche einen Händler von Aktienoptionen unterstützt, die idealen (intraday) Preise zum Kauf / Verkauf von aufgelaufenem Delta zu finden? 

Als Vergleich bietet sich dabei eine Strategie an, welche keine intraday Anpassungen vornimmt, sondern die aufgelaufene Delta-Position erst am Abend glattstellt. Die Voraussage jeweils eines Verkaufs- und eines Kaufpreises erfolgt dabei am Morgen des Arbeitstages bei Markteröffnung. Bei Tagesende noch vorhandenes Delta (aufgrund nicht erreichter Kaufs- oder Verkaufspreise oder neu aufgelaufenes Delta bis Tagesende) wird zum Tagesschlusskurs ausgeglichen.

Als Vergleichsstrategie dient die Strategie, bei welchem aufgelaufenes Delta erst am Abend zum Börsenschlusskurs ausgeglichen wird.

Mathematisch lässt sich beide Strategien mit Hilfe einer Payoff-Funkt ausdrücken.


Für die einfachere Referenzstrategie, welche die Delta-Neutraliät erst bei Tagesende wieder herstellt, lautet diese wie folgt:

\begin{equation}
\begin{aligned}
Payoff ={} & \frac{1}{2}  \times 100 \times \$\Gamma_{close_{t-1}}  \times \ln{\left(\frac{close_t}{close_{t-1}}\right)}^2
\end{aligned}
\end{equation}

Für die Notation werden log-Returns verwendet. Diskrete Returns sind aber genauso möglich. Als $close_i$ wird der Tagesendkurs am Tag $i$ bezeichnet. Unter  $\$\Gamma_i$ verstehen wir dabei das Gamma Cash (auch Dollar Cash). Die Herleitung des Payoffs ergibt sich aufgrund folgender Überlegungen:[vgl. @sp_finance]

Bezeichne dazu $S_0$ den Preis des Underlying und $P_0$ den Preis der Option zu Beginn und $S_1$ sowie $P_1$ die jeweiligen Preise nach einer Preisbewegung des Underlyings um $d_S$. $\Delta_0$ und $\Gamma_0$ bezeichnen die erste und zweite Ableitung des Optionspreises zum Ausgangszeitpunkt. Ferner sei als Delta Cash ($\$\Delta = \Delta \times S$) der Wert des Hedge Portfolio bezeichnet und Gamma Cash ($\$\Gamma = \Gamma \times S^2/100$) bezeichne die Veränderung des $\$\Delta$ bei einer 1-prozentigen Veränderung des Underlying-Preises. Dies lässt sich wie folgt herleiten: Verändert sich der Underlying-Preis um 1% $S => S + S / 100$, verändert sich auch das Delta $\Delta => \Delta + \Gamma \times S/100$. Das neue Delta Cash verändert sich damit in der Höhe von Gamma Cash $\$\Delta => \$\Delta + \Gamma \times S^2/100$.

Mit Hilfe dieser Nomenklatur lässt sich der Gewinn einer zu Beginn delta-gehedgten Position herleiten: 

1. Veränderung des Underlying Preises $S_1 = S_0 + dS$
1. Veränderung des Delta $\Delta_1 = \Delta_0 + \Gamma_0 \times dS$
1. Durchschnittliches Delta $\Delta_{Avg} = (\Delta_0 + \Delta_1) / 2 = \Delta_0 + \Gamma_0 \times dS / 2$
1. Neuer Optionspreis $P_1 = P_0 + \Delta_{Avg} \times dS = P_0 + \Delta_0 \times dS + \Gamma_0 \times dS² / 2$ 
1. Gewinn als Summe der Veränderungen des Optionspreis und des Hedge-Portfolios: $\Delta_0 \times dS + \Gamma_0 \times dS^2 / 2 - \Delta_0 \times dS = \Gamma_0 \times dS^2 / 2$
1. Etwas umgeformt lässt sich der Gewinn schreiben als: $\Gamma_0 \times dS^2 / 2 = \Gamma_0 \times S^2 /100 * dS^2 / S^2 * 100 = \$\Gamma \times (dS/S)^2 \times 100$ 


Auch im Falle der etwas komplexeren Strategie mit der Möglichkeit von Käufen und Verkäufen innerhalb des Tages lässt sich diese Struktur wiederfinden:

\begin{eqnarray}
{Payoff} & = & \frac{1}{2} \times 100 \times \$\Gamma_{close_{t-1}} \times \ln{\left(\frac{first}{close_{t-1}}\right)}^2  \times I_{first} \times (1 - I_{second}) \ +\\
&  & \frac{1}{2} \times 100 \times \$\Gamma_{first} \times \ln{\left(\frac{close_t}{first}\right)}^2  \times I_{first} \times (1 - I_{second}) \ +\\

&  & \frac{1}{2} \times 100 \times \$\Gamma_{close_{t-1}} \times \ln{\left(\frac{second}{close_{t-1}}\right)}^2  \times I_{second} \times (1 - I_{first}) \ +\\
&  & \frac{1}{2} \times 100 \times \$\Gamma_{second} \times \ln{\left(\frac{close_t}{second}\right)}^2  \times I_{second} \times (1 - I_{first}) \ +\\

&  & \frac{1}{2} \times 100 \times \$\Gamma_{close_{t-1}} \times \ln{\left(\frac{first}{close_{t-1}}\right)}^2  \times I_{second} \times I_{first} \ +\\
&  & \frac{1}{2} \times 100 \times \$\Gamma_{first} \times \ln{\left(\frac{second}{first}\right)}^2  \times I_{first} \times I_{second} \ +\\
&  & \frac{1}{2} \times 100 \times \$\Gamma_{second} \times \ln{\left(\frac{close_t}{second}\right)}^2  \times I_{second} \times I_{first} \ +\\

&  & \frac{1}{2} \times 100 \times \$\Gamma_{close_{t-1}} \times \ln{\left(\frac{close_t}{close_{t_1}}\right)}^2  \times (1 - I_{first}) \times (1 - I_{second})
(\#eq:payoff)
\end{eqnarray}

wobei, 

\begin{equation}
\$\Gamma_i = \Gamma \times S_i^2 / 100
(\#eq:gamma_cash)
\end{equation}

Zu beachten gilt es insbesondere, dass der Payoff quadratisch auf Preisbewegungen reagiert. Dies führt dazu, dass es von Relevanz ist, ob zuerst der Kaufs- (untere Grenze) oder der Verkaufsschwelle erreicht wird. Die Ausführung zum jeweiligen Schwellenwert wird durch die Indikatorvariablen $I_{first}$ und $I_{second}$ modelliert. Damit es zu einer Ausfühung kommt müssen dabei folgende beiden Kriterien kumulativ erfüllt sein:

a) Der Schwellenwert wird erreicht.
b) Die vorangegangene Transaktion hat bei einem höheren Preis für Aktion an unterer Schwelle, resp. tieferen Preis für eine Aktion an der oberen Schwelle stattgefunden. ^[Dies hat dann Relevanz, wenn der Eröffnungspreis eine prognostizierte Schwelle überspringen kann. Effektiv sollte die obere, resp. untere Grenze in solchen Fällen allerdings auf den  Eröffnungskurs gesetzt werden.]


<!-- Können die Verteilungen des Kurses im Laufe des Tages prognostiziert werden, so könnten diejenigen Kauf- und Verkaufpreise gewählt werden, welche unter Berücksichtigung dessen Eintretenswahrscheinlichkeit die höchsten Gewinne ermöglicht. Dies soll in der vorliegenden Arbeit mit Hilfe von Machine Learning Algorithmen untersucht werden. -->


## Daten
Als Datengrundlage stehen historische Open-, Low-, High- und Close- (adjustiert wie unadjustiert) Aktienpreise zur Verfügung. Diese sind öffentlich und können von Yahoo Finance bezogen werden. Zu Testzwecken wurden 9.5 Mio. historische Datenpunkte für die grössten rund 1600 Unternehmen weltweit angezogen. Die Datenqualität scheint auf den ersten Blick gut. Gewisse Datenbereinigungsschritte sind nötig. 

Die Verwendung bankinterner Daten ist aktuell nicht geplant. Dies hat den grossen Vorteil, dass die Daten auch problemlos auf Cloud-Infrastruktur (bsp. unabdingbar im Falle des Einsatzes von Neuronalen Netzen / Deep Learning Techniken) prozessiert werden kann. 

## Methoden
Es werden verschiedene Methoden versucht und gegeneinander verglichen. Diese sind: 

<!-- Anbei aber eine Sammlung von denkbaren Ansätzen: -->

<!-- * Clusteranalysen mit Hilfe diverser Disanzmasse (Manhattan Distance, Cosine Similarity): Lassen sich um den aktuellen Kursverlauf in Verbindung zur Vergangenheit (des aktuellen oder anderer Titel) bringen und darüber allfällige Rückschlüsse auf die Zukunft zu machen. -->
<!-- * k-means: Lassen sich in der Historie ähnliche / sich wiederholende Patterns von Kursverläufen / -verteilungen finden, welche Rückschlüsse auf die Zukunft ermöglichen? -->
<!-- * Welche Aussagen können "klassische" autoregressive statistische Modelle (G)ARCH machen? -->
<!-- * Können Neuronale Netze (z.B. LSTM) chartistische Muster erkennen? -->
<!-- * Deep Learing Methoden: Können anhand visualisierter Kursverläufe / -verteilungen Muster erkennen/gelernt werden? -->
<!-- * Weitere zusammen mit dem Betreuer festgelegte Methoden -->


## Disclaimer
Etliche Untersuchungen haben sich bereits damit beschäftigt, kursrelevante Informationen vorauszusagen. Viele der dabei gefundenen Erkenntnisse sind nicht oder nur unter speziellen Voraussetzungen anwendbar. Es sei deshalb an dieser Stelle erwähnt, dass sich dem sowohl der Autor als auch dessen Auftraggeber bewusst ist. Auch wenn diese ausbleiben bietet diese Arbeit sowohl für den Autoren wie auch dessen Arbeitgeber die Chance, verschiedene moderne Analysetechniken/ -methoden an Finanzdaten anzuwenden, welche auch für andere Projekte wertvoll sein können.




<!--chapter:end:01-intro.Rmd-->

# Daten

## Bezug und Umfang

### Aktienuniversum

```{r, retrieve-etf-data, echo=FALSE, warning=FALSE, message=FALSE}
suppressPackageStartupMessages({
  library(digest)
  library(here)
  library(dplyr)
  library(stringr)
  library(kableExtra)
  library(purrr)
  library(tidyr)
})

source("data/download_scripts.R")
source("R/02-data.R")

# load stocks
stocks_hash <- "a2ea2a17f87576c2bf28c0e2ff80f30e"
file_stocks <- here::here(paste0("data/stocks_", stocks_hash,".rds"))
if (!file.exists(file_stocks)) {
  stocks <- download_stocks() 
  stocks_hash <- digest::digest(stocks)
  file_stocks <- here::here(paste0("data/stocks_", stocks_hash,".rds"))
  saveRDS(stocks, file_stocks)
}
stocks <- readRDS(file_stocks) %>% unnest(cols = names(.))
```

Für die vorliegende Analyse werden die Aktienpreise grosser Unternehmen weltweit herangezogen. Konkret werden alle Aktienkomponenten des "iShares MSCI World UCITS ETF" [vgl. @blackrock] per 28. Februar 2020 ^[Das Startdatum dieser Arbeit.] verwendet. Dieser Exchange Traded Fund (ETF) besteht zu diesem Zeitpunkt aus `r nrow(stocks)` Aktien, welche sich automatisiert auslesen lassen. Ferner stellt die Webseite des ETF Emittenten weitere Attribute zur Verfügung. Diese lauten am Beispiel Nesté wie folgt:

``` {r etf-data-nestle-stock, echo=FALSE}
stock_nesn <- dplyr::filter(stocks, Ticker == "NESN")
knitr::kable(tibble(Attribut = names(stock_nesn), Wert = t(stock_nesn)), caption = "Attribute der Nestlé Aktie per 28. Februar 2020") %>%
  kableExtra::kable_styling(position = "center", latex_options = "HOLD_position")
```

Neben eindeutigen Identifiern wie "Ticker" und "ISIN" enthält der Datensatz auch Informationen zur "Asset Class" der Komponente. Für die vorliegende Analyse von Aktien ist hierbei lediglich die Ausprägung "Equity" zulässig. Die Kennzahlen "Weight", "Market Value" und "Notional Value" geben Auskunft über die Grösse der betachteten Unternehmung und eignen sich auch zum Vergleich ebendieser. Zu beachten gilt, dass im Falle von Aktien der "Notional Value" dem "Market Value" entspricht und sich dieser bis auf rundungsbedingte Differenzen auch aus der Anzahl ausgegebener Titel mal Tagesendkurs ("Shares" x "Price") ermitteln lässt. 

Als weitere Unterscheidungsmerkmale sind der Hauptbörsenplatz "Exchange" (`r length(unique(stocks$Exchange))` unterschiedliche Ausprägungen), der Sitz "Location" (`r length(unique(stocks$Location))` Ausprägungen) sowie die Währung "Market Currency" (`r length(unique(stocks[["Market Currency"]]))` Ausprägungen) aufgeführt. Alle drei Attribute bilden stark verwandte Informationen ab. Geschlüsselt auf Kontinente ergeben sich für den Börsensitz folgende Anteile: 

- Nordamerika: `r round(sum(stocks$Location %in% c("United States", "Canada"))/nrow(stocks) * 100, 0)`% 
- Europa: `r round(sum(stocks$Location %in% c("Portugal", "Austria", "Ireland", "Norway", "Belgium", "Finland", "Israel", "Denmark", "Netherlands", "Spain", "Italy", "Sweden", "Switzerland", "Germany", "France", "United Kingdom"))/nrow(stocks) * 100, 0)`% 
- Asien: `r round(sum(stocks$Location %in% c("Japan", "Hong Kong", "Singapore"))/nrow(stocks) * 100, 0)`% 
- Australien: `r round(sum(stocks$Location %in% c("Australia", "New Zealand"))/nrow(stocks) * 100, 0)`%

Als letzes Attribut enthält der Datensatz Angaben zum "Sector" in welchem die jeweilige Unternehmung tätigt ist. Bis auf die Kategorien "Energy" und "Other" ist jeder der `r length(unique(stocks$Sector))` aufgeführten Sektoren mit mindestens 5 Anteil vorhanden. Die beiden Sektoren mit dem höchsten Anteil sind "Industrials" und "Financials".


``` {r, fig.cap='Anzahl im Datensatz vorhandene Titel je Sektor', fig.asp=1, fig.pos = '!H', analysis-etf-data, echo=FALSE, message=FALSE, warning=FALSE}
table(stocks$Sector) %>%
  sort(decreasing = FALSE) %>%
  tibble(Sektor = factor(names(.), levels = unique(names(.))), Anteil = . / sum(.) ) %>%
  ggplot2::ggplot(ggplot2::aes(x = Sektor, y = Anteil)) + 
  ggplot2::geom_bar(stat = "identity") +
  ggplot2::scale_y_continuous(labels=scales::percent) +
  ggplot2::coord_flip()
```

### Preisinformationen

``` {r, retrieve-quotes-data, echo=FALSE}
# load quotes
quotes_hash <- "6175759dd4876b820a6457a4c341e9cd"
file_quotes <- here::here(paste0("data/quotes_", quotes_hash,".rds"))
if (!file.exists(file_quotes)) {
  quotes <- download_quotes(stocks) # takes about 35mins
  quotes_hash <- digest::digest(quotes)
  file_quotes <- here::here(paste0("data/quotes_", quotes_hash,".rds"))
  saveRDS(quotes, file_quotes)
}
quotes_raw <- readRDS(file_quotes)
```

Für alle Titel werden in einem zweiten Schritt die historischen Aktienkurse inkl. Höchst- und Tiefstkurs bezogen. Diese Daten stehen via Yahoo Finance auf täglicher Basis zur freien Nutzung zur Verfügung [vgl. @yahoo_finance]. Zu beachten gilt es hierbei, dass die Yahoo Ticker für ausserhalb der USA gehandelte Titel einen Suffix je Börsenplatz verwenden. Für die Analyse kommen so `r formatC(nrow(quotes_raw), big.mark = "'")` tägliche Datenwerte für `r formatC(length(unique(quotes_raw$Ticker)), big.mark = "'")` Titel zusammen. Für `r formatC(nrow(stocks) - length(unique(quotes_raw$Ticker)), big.mark = "'")` Aktien können keine Werte gefunden werden. Tabelle \@ref(tab:quote-data-nestle) zeigt einen Beispieleintrag für die Aktie von Nesté per 14. Februar 2019. 


``` {r quote-data-nestle, echo=FALSE}
quote_nesn <- dplyr::filter(quotes_raw, Ticker == "NESN.SW" & Date == as.Date("2019-04-12"))

knitr::kable(tibble(Attribut = names(quote_nesn), Wert = t(quote_nesn)), caption = "Kursinformationen der Nesté Aktie per 12. April 2019") %>%
  kableExtra::kable_styling(position = "center", latex_options = "HOLD_position")
```

Die Werte "Open", "Low", "High" und "Close" zeigen Eröffnungs-, Tiefst-, Höchst- und Schlusskurs des Titels. Mit "Volume" werden die Anzahl gehandelter Titel am jeweiligen Tag angegeben. Unter "Adjusted" ist der um Dividendenausschüttungen korrigierte Schlusskurs aufgeführt.^[Allfällige Aktiensplits sind in allen Werten bereits berücksichtigt.] 

Die Werte in Tabelle \@ref(tab:quote-data-nestle) sind insofern speziell, als dass sie den letzten Tag vor dem Ex-Dividend Datum von Nestlé für 2019 betreffen. Das heisst, es sind die Kurse des letzten Tages, bevor die Aktie ohne die für das Jahr 2019 ausgeschüttete Dividende gahandelt wurde. Die Dividende betrug in jenem Jahr CHF 2.45 [vgl. @nestle]. Diese Differenz widerspiegelt sich in den Daten als Differenz des Close- und Adjusted-Preises. Da alle Werte (auch Open, Low, High und Close) am Folgetag ohne den Anspruch auf diese Dividende gehandelt werden, fallen diese typischerweise tiefer aus. Um eine Vergleichbarkeit der Renditen über die Zeit zu gewährleisten ist daher eine Anpassung der Werte mit Hilfe des Adjustement-Faktors nötig. Dieser ergibt sich als Quotient von Adjusted und Close Preis und wird auf allen Einträgen angewendet.

Weiter erwähnenswert ist, dass Yahoo den Adjusted Kurs des jeweils aktuellsten Tages - ausser eben am Tag vor Ex-Dividend - mit auf aktuellen Kurs festlegt. Im Lauf der Zeit und mit neuen Ausschüttungen verändert sich damit auch die Historie der Adjusted Werte. Dies lässt sich zeigen, wenn der Beispieleintrag von Nestlé per 12. April 2019 nach der nächsten Dividendenausschüttung (27. April 2020) noch einmal abgerufen wird [vgl. @nestle]. Während alle Preise ausser "Adjusted" identisch ausgewiesen sind, hat sich dieser neu auf 90.72 verändert [vgl. @yahoo_finance]. Mit der nächsten Dividenenausschüttung (voraussichtlich im April 2021) wird sich dieser Wert dann wieder ändern. Mit Hilfe der Adjustierung ist aber gewährleistet, dass die Werte vergleichbar bleiben.


## Aufbereitung
### Bereinigung

Mit Yahoo Finance wird ein erfahrener und häufig verwendeter Datenanbieter gewählt. Der Blick auf einige Quantilskennzahlen der Rohdaten in Tabelle \@ref(tab:summary_before_clean) zeigt aber, dass dennoch einige Datenprobleme ausgemacht werden können.

```{r, summary_before_clean, echo=FALSE}
summary(quotes_raw[, setdiff(names(quotes_raw), c("Date", "Ticker", "Volume"))])
```

Die Behebung dieser Mängel und die damit einhergehende Bereinigung erfolgt in verschiedenen Schritten. Allen gemeinsam ist, dass die Bereinigung keinen Ausschluss der Daten zur Folge hat, sondern betroffene Werte als "Nicht verfügbar, (NA)" klassifiziert werden. Diese Unterscheidung ist insbesondere bei rollierender Betrachtung eines Zeitfensters der Vergangenheit von Bedeutung.

``` {r, echo=FALSE}
quotes_clean <- quotes_raw
```

```{r, filter-data-adjusted, echo=FALSE}
wrong_corporate_action_threshold <- 0.001
quotes_clean <- na_wrong_corporate_actions(quotes_clean, wrong_corporate_action_threshold)
quotes_clean <- adjust_quotes(quotes_clean)
```
1. **Entfernen von fehlerhaften Adjustierungsdaten**  
  Eine Eigenschaft von Aktienpreisen ist es, dass sie nicht negativ sein können. Der Datensatz weist aber vereinzelt negative Adjusted Werte aus. Dies lässt sich auch durch die Dividendenbereinigung nicht erklären. Bei diesen Einträgen scheinen daher Datenfehler vorzuliegen. Erschwerend kommt hinzu, dass sich Fehler bei der Adjustierung nicht auf den jeweiligen Eintrag beschränken müssen. Aufgrund der Funktionsweise der Adjustierung (vgl. \@ref(preisinformationen)) ist ein vererben des Fehlers auf andere Einträge des jeweiligen Titels wahrscheinlich. Bei genauerer Betrachtung der Adjusted Werte fällt ferne auf, dass auch einige sehr sehr kleine (im Bereich $-10^{-6}$), wenn auch postive Werte gefunden werden können. Aus diesen Grund werden alle Ticker, welche in ihrer Historie einen Adjusted Preis von weniger als `r wrong_corporate_action_threshold` aufweisen, ausgeschlossen.

```{r, filter-data-measure-order, echo=FALSE}
quotes_clean <- na_unreasonable_measure_order(quotes_clean)
```
1. **Entfernen von Einträgen mit unerwarterer Reihenfolge**  
  Die Werte für Open, Low, High und Close implizieren eine klare Reihenfolge. Keine anderer der Werte darf höher als das High oder kleiner als das Low sein. Ist dies der Fall, werden die entsprechenden Werte von der Analyse ausgeschlossen.

```{r, filter-data-typo, echo=FALSE}
max_typo_ratio <- 8
quotes_clean <- na_typos(quotes_clean, max_typo_ratio)
```
1. **Erkennen und Ausschluss von Tippfehlern**  
  Einzelne Einträge lassen sich als Tippfehler identifizieren. Der Titel "AV.L" weist per 09. August 2019 beispielsweise einen Low-Wert von 3.87 aus, währenddem alle andern Werte des gleichen Tages wie auch der benachbarten Tage bei ca. 380 liegen. Es liegt auf der Hand, dass dieser Wert um einen Faktor 100 falsch erfasst wurde. Solchen Fehlern wird begegnet, indem alle paarweisen Verhältnisse von Open, Low, High und Close Preis kleiner als `r max_typo_ratio` sein müssen. Andernfalls erfolgt ein Ausschluss der als Tippehler identifizierten Kennzahl.

```{r, filter-data-no-intraday-moves, echo=FALSE}
quotes_clean <- na_no_intraday_moves(quotes_clean)
```
1. **Fehlende Preisbewegungen innerhalb des Tages**  
  Der Aktienkurs eines grösseren Unternehens bewegt sich typischerweise auch an ruhigen Börsentagen immer ein wenig. Die vorhandenen Preise  liegen mit der Genauigkeit mehrer Nachkommastellen vor. Unterscheiden sich hierbei Tagestiefst- und Tageshöchstpreis nicht, muss von einem Datenfehler ausgegangen werden. Einträge ohne Preisbewegung innerhalb des Tages werden daher von der Analyse ausgeschlossen.

```{r, filter-data-no-daily-changes, echo=FALSE}
quotes_clean <- quotes_clean %>%
  group_by(Ticker) %>%
  dplyr::group_modify(~na_no_daily_changes(.)) %>%
  ungroup()
```
1. **Fehlende Kursbewegungen über nacheinanderfolgende Börsentage**  
  Ähnlich wie bei den Preisbewegungen innerhalb des Tages verhält es sich auch bei Bewegungen über sich folgende Börsentage hinweg. Es ist zu erwarten, dass sich mindestens einer der fünf betrachteten Preise vom Vortag unterscheidet. Ist dies nicht der Fall, wird der Eintrag ausgeschlossen.

```{r, filter-data-too-large-daily-changes, echo=FALSE}
too_large_daily_change_ratio <- 2
quotes_clean <- quotes_clean %>%
  group_by(Ticker) %>%
  dplyr::group_modify(~na_too_large_daily_changes(., too_large_daily_change_ratio)) %>%
  ungroup()
```
1. **Aussergewöhnlich hohe Preisbewegungen**  
  Es liegt in der Natur der Sache, dass sich Aktienkurse verändern. Grosse Kurssprünge sind bei Aktien sehr grosser Unternehmen wie sie in dieser Arbeit betrachtet werden aber selten. Die Chance, dass das Verhältnis zwischen adjustiertem Preis des Vortages und adjustiertem Preis des aktuellen Tages (und vice versa) um mehr als einen Faktor `r too_large_daily_change_ratio` unterscheidet erachten wir als kleiner, als dass es sich dabei um einen Datenfehler handelt. Entsprechende Einträge werden deshalb entfernt.

```{r, filter-data-extremes, echo=FALSE}
index_factor <- 100 / quotes_clean$Open
quotes_clean <- normalize_quotes(quotes_clean, base_col = "Open", target_cols = c("Low", "High", "Close", "Adjusted"))
quotes_clean<- na_extremes(quotes_clean, col = c("Open", "Low", "High", "Close", "Adjusted"), tail = 0.01)
quotes_clean <- mutate_at(quotes_clean, c("Open", "Low", "High", "Close", "Adjusted"),  ~. / index_factor)
```
1. **Ausschluss von Extremwerten**  
  Die der vorliegenden Analyse zugrundliegende Payoff-Funktion ist abhängig von den Preisbewegungen einer Aktie innerhalb des Tages (vgl. \@ref(forschungsfrage)). Speziell dabei ist, dass die Höhe der Bewegung nicht nur linear, sondern gar quadratisch Niederschlag findet. Dies führt dazu, dass die Analyse sehr sensitiv auf einzelne extreme Ausreisser reagiert. Hinzu kommt, dass es Ziel der Arbeit ist, Aussagen über Kursbewegungen innerhalb eines typischen Börsentages machen zu können. Eine Prognose von Werten an aussergewöhnlichen Tagen liegt ausserhalb des Geltungsbereiches der Analyse. Aus diesem Grund werden nach obigen Breinigungen für jede der fünf Preiskennzahlen die jeweils 1% extremsten Werte nach oben wie auch unten ausgeschlossen. Der Ausschluss erfolgt aufgrund der unterschiedlichen Preislevels der Aktien auf einer täglich auf den Eröffnungspreis indexierten Wert.

Zusammenfassend lässt sich festhalten, dass der rohe Datensatz aus `r nrow(quotes_raw)` Einträgen besteht. Davon weisen `r sum(rowSums(is.na(quotes_raw)) != 0)` Zeilen vor Bereinigung einen fehlenden Wert auf. Nach Bereinigung erhöht sich dieser Wert auf `r sum(rowSums(is.na(quotes_clean)) != 0)`. Für die Analyse bleiben somit `r sum(rowSums(is.na(quotes_clean)) == 0)` verwendbare Einträge.


```{r, echo=FALSE}
# TODO: remove
quotes_clean <- quotes_clean %>% select(-Adjusted)
# arrow::write_feather(quotes_clean, "tmp/quotes_clean.feather")
```


### Normalisierung

Ein weiteres Problem, das sich beim Vergleich von Preisen verschiedener Aktien ergibt, ist deren unterschiedlichesw Preisniveau. Während eine Aktie bei USD 30 handelt, bewegt sich eine andere auf einem Niveau von USD 1000. Eine Vergleichbarkeit lässt sich dadurch herstellen, indem nicht absolue Preise, sondern relative Returns in der Analyse verwendet werden. Tatsächlich ist dies in der Payoff-Funktion (vgl. Gleichung \@ref(eq:payoff)) bereits grösstenteils sichergestellt. Das absolute Preisniveau fliesst allerdings auch in die Berechnung des Gamma Cash (vgl. Gleichung \@ref(eq:gamma_cash)) mit ein. Um auch hier eine Vergleichbarkeit der Werte sicherzustellen, wird bei allen nachfolgenden Analysen der Preis auf ein Niveau von 100 per adjustiertem Vortagesendkurs (dies entspricht dem letzten Neutralisierungszeitpunkt des Deltas) normalisiert.


```{r, reorganize-one-line, echo=FALSE}
# quotes_line <- quotes_clean %>%
#   group_by(Ticker) %>%
#   dplyr::group_modify(~reorganize_to_one_line(.)) %>%
#   ungroup()
```



```{r, round-data, echo=FALSE}
# quotes_line <- mutate_if(quotes_line, is.numeric, ~round(., 6)) # to avoid problems because of system accurracy
```


```{r, echo=FALSE}
# TODO: remove
# arrow::write_feather(quotes_line, "tmp/quotes_line.feather")
```


<!--chapter:end:02-data.Rmd-->

# Analyse


```{r message=FALSE, warning=FALSE, setup_analysis, echo=FALSE}

suppressPackageStartupMessages({
  library(dplyr)
  library(stringr)
  library(purrr)
  library(pbmcapply)
  library(tidyr)
  library(ggplot2)
  library(data.table)
  library(dtplyr)
  library(arrow)
  library(microbenchmark)
  library(profvis)
})
Rcpp::sourceCpp('src/bootstrap_nn_idx.cpp')
source("R/02-data.R")
source("R/03-analysis.R")

# TODO: remove
quotes_clean <- arrow::read_feather("tmp/quotes_clean.feather")

```

Für die Analyse der Daten mit dem Ziel einen Kaufs- sowie einen Verkaufkurs zu prognostizieren, bei dem der Delta-Hedge nachgezogen werden soll, werden nachfolgend verschiedene Techniken eingesetzt. Diese sind:

- Einfache Optimierungen
- KLassifikationsverfahren (bsp. KNN):
- Neuronale Netzwerke (bsp. RNN, LTSM):
- Autogressive Modelle (bsp. GARCH)


## Einfache Optimierungen

Eine einfache Möglichkeit, optimale Kaufs- und Verkaufspreise zu finden besteht darin, diese im Testdatensatz mittels einfacher Optimierung zu evaluieren. In einer ersten sehr einfachen Evaluation bietet es sich an, die Kaufs- und Verkaufsmarken als optimierte prozentuale Bewegungen vom aktuellen Preis zu ermitteln. Dies lässt sich sowohl symmetrische wie auch asymmetrisch machen. 

```{r, echo=FALSE}
opt_payoff_sym <- function(move, data, col, both_first, scale_fct = 1) {
  sum(calc_payoff_const_gamma(data, buy = (1 - move) * data[[col]], sell = (1 + move) * data[[col]], both_first = both_first), na.rm = TRUE) / scale_fct
}

data <- quotes_clean %>%
  dplyr::group_by(Ticker) %>%
  dplyr::group_modify(~widen(., window = 1, cols = "Close", keep = c("Close", "Open", "Low", "High"))) %>%
  ungroup() %>%
  rename(Close_0 = "Close") %>%
  select(-Ticker)

data <- normalize_quotes(data, "Close_1", names(data))
data <- tidyr::drop_na(data)

set.seed(123456)
both_first <- c("buy", "sell")[sample(c(1, 2), nrow(data), replace = TRUE)]


profvis(calc_payoff_const_gamma(data, both_first = "min"))

scale_fct <- sum(calc_payoff_const_gamma(data, both_first = both_first))
sum(opt_payoff_sym(move = 0.02, data = data, col = "Open", both_first = both_first, scale_fct = scale_fct))

move <- seq(0, 0.1, 0.001)

factor_open <- pbmcapply::pbmclapply(
  move,
  opt_payoff_sym,
  data = data,
  col = "Open",
  both_first = both_first,
  scale_fct = scale_fct
)

optimum_open <- optimize(
  f = opt_payoff_sym,
  interval = c(0, 0.1),
  data = data,
  col = "Open",
  both_first = both_first,
  scale_fct = scale_fct,
  maximum = TRUE
)

res_open <- tibble(move = move, factor = unlist(factor_open))
plot(res_open$move, res_open$factor, type = "l")


factor_adjusted_t_1 <- map(
  .x = move,
  .f = opt_payoff_sym,
  quotes_line = dat,
  col = "Adjusted_t_1",
  both_first = both_first,
  scale_fct = sum(calc_payoff_const_gamma(dat, both_first = both_first))
)

optimum_adjusted_t_1 <- optimize(
  f = opt_payoff_sym,
  interval = c(0, 0.2),
  quotes_line = dat,
  col = "Adjusted_t_1",
  both_first = both_first,
  scale_fct = sum(calc_payoff_const_gamma(dat, both_first = both_first)),
  maximum = TRUE
)

res_adjusted_t_1 <- tibble(move = move, factor = unlist(factor_adjusted_t_1))
plot(res_adjusted_t_1$move, res_adjusted_t_1$factor, type = "l")


```


## Klassifikationsverfahren
```{r, train_test_split, echo=FALSE}
# 
# quotes_line <- tidyr::drop_na(quotes_line)
# 
# # out <- find_nn(quotes_line = quotes_line, cols = c("Open", "Low", "High", "Close"), window = 3, norm = "euclidean", mc.cores = 3)
# # saveRDS(out, "tmp/knn_olhc_3_euc.rds")
# # rm(out)
# 
# #out <- find_nn(quotes_line = quotes_line, cols = c("Open", "Low", "High", "Close"), window = 3, norm = "manhattan", mc.cores = 3)
# # saveRDS(out, "tmp/knn_olhc_3_man.rds")
# 
# 
# knn_olhc_3_euc <- readRDS("tmp/knn_olhc_3_euc.rds")
# knn_olhc_3_man <- readRDS("tmp/knn_olhc_3_man.rds")
# 
# 
# plot_history <- function(id, data, window, nn_idx) {
#   
#   
#   test <- sample(1000:2000, 15000, replace = TRUE)
#   
#   
#   
#   comp %>%
#     mutate(entry = seq_len(n())) %>%
#     tidyr::pivot_longer(-entry, names_to = "type")
#   
#   
#   cols_all <- paste0(cols, "_", seq_len(ncol(data) / length(cols)))
#   data_id <- map(seq_len(ncol(data) / length(cols)), ~select_at(data, paste0(cols, "_", .)))
#   
#   
#   cols_all <- do.call(function(...) paste(..., sep = "_"), expand.grid(cols, seq_len(ncol(data) / length(cols))))
#   
#   curr_data <- data_id[[1]] %>% 
#     mutate(id = seq_len(n())) %>%
#     tidyr::pivot_longer(-id, names_to = "type") %>%
#     mutate_at("type", ~factor(., levels = cols_all)) %>%
#     filter(id < 10)
#   
#   
#   
#   ggplot(data=filter(dat, ID %in% seq_len(51)), aes(x=type, y=value, group = ID)) + 
#     
#     
#     ggplot(data=filter(dat, ID %in% seq_len(51)), aes(x=type, y=value, group = ID)) + 
#     geom_point() + 
#     geom_line() +
#     theme_bw()
#   
#   
#   
# }
# 
# 
# x <- quotes_line[123456, ]
# foo <- knn_olhc_3_euc$nn.idx[123456, ]
# foo <- knn_olhc_3_man$nn.idx[123456, ]
# 
# test <- arrange(quotes_line, Ticker, Date)
# 
# data_wide_123456 <- data_query$data_wide
# 
# quotes_line[123453:123455,]
# 
# data_wide_123456[foo, ]
# 
# quotes_line[21352,]
# 
# 
# dat <- x %>%
#   bind_rows(quotes_line[foo, ]) %>%
#   select("Open", "Low", "High", "Close") %>%
#   mutate(ID = seq_len(n())) %>% 
#   tidyr::pivot_longer(-ID, names_to = "type")
# 
# ggplot(data=filter(dat, ID %in% seq_len(51)), aes(x=type, y=value, group = ID)) + 
#   geom_point() + 
#   geom_line() +
#   theme_bw()
# 

```



```{r, knn-calculation, echo=FALSE}
# data_wide_0 <- quotes_line %>%
#   as_tibble() %>%
#   group_by(Ticker) %>%
#   widen(window = 3, cols = c("Open", "Low", "High", "Close"), include_current = TRUE)
# 
# # exclude all columns (also when _0 value is not available)
# na_rows_bool <- rowSums(is.na(data_wide_0)) > 0
# knn_data <- data_wide_0 %>%
#   mutate(Date = quotes_line$Date) %>%
#   mutate_all(~ifelse(na_rows_bool, NA, .)) 
# 
# gc()
# nn <- find_nn(select(knn_data, !c(ends_with("_0"), any_of("Date"))), dates = knn_data$Date, mc.cores = 3)
# 
# 
# nn <- readRDS("data/nn_eucl_olhc_w3_38a896430298c738055505dc89e042ac.rds")
# 
# nn <- readRDS("data/nn_eucl_olhc_w3_38a896430298c738055505dc89e042ac.rds")
# 
# 
# 

```


```{r, knn-history-plot, echo=FALSE}
# id <- 280000
# valid_idx <- seq_len(n)[rowSums(is.na(nn$nn.idx)) == 0]
# 
# k <- 10
# plot_nn(data_wide_0[valid_idx[id], ], data_wide_0[nn$nn.idx[valid_idx[id], seq_len(k)],])

```

```{r, knn-prediction-power, echo=FALSE}
# k <- 20
# 
# nn_idx <- as.matrix(arrow::read_arrow("data/nn_idx_eucl_olhc_w3_38a896430298c738055505dc89e042ac.feather"))
# 
# nn_pred <- pred_nn(select(data_wide_0, c("Low_0", "High_0")), nn_idx = nn_idx[, seq_len(k)])
# na_row_bool <- rowSums(is.na(nn_pred)) > 0
# 
# nn_pred_sample <- nn_pred[!na_row_bool, ] %>% rename(Buy = Low_0, Sell = High_0)
# quotes_line_sample <- quotes_line[!na_row_bool, ]
# 
# plot_ratio_history(quotes_line = quotes_line_sample, data_pred = nn_pred_sample)
# 
# 
# ### perform bootstraping
# size_map <- quotes_line %>%
#   select(Date) %>%
#   group_by(Date) %>% 
#   summarize(count = n()) %>% 
#   ungroup() %>% 
#   mutate(size = cumsum(count) - count) %>% 
#   select(-count)
# size <- quotes_line %>% select(Date) %>% left_join(size_map, by = "Date") %>% .[["size"]]
# ###
# 
# boot_nn_idx_1 <- bootstrap_nn_idx(nn_idx, size, 20, 123456)
# 
# nn_pred_boot <- pred_nn(select(data_wide_0, c("Low_0", "High_0")), nn_idx = boot_nn_idx_1)
# na_row_bool_boot <- rowSums(is.na(nn_pred_boot)) > 0
# 
# nn_pred_boot_sample <- nn_pred_boot[!na_row_bool_boot, ] %>% rename(Buy = Low_0, Sell = High_0)
# quotes_line_boot_sample <- quotes_line[!na_row_bool_boot, ]
# 
# plot_ratio_history(quotes_line = quotes_line_boot_sample, data_pred = nn_pred_boot_sample)
# 
# 

```



```{r, knn-bootstraping, echo=FALSE}
# k <- 20
# quotes_line <- readRDS("tmp/quotes_line.rds")
# quotes_line_sorted <- quotes_line %>% arrange(Date)
# 
# nn_idx <- arrow::read_arrow("data/nn_idx_eucl_olhc_w3_38a896430298c738055505dc89e042ac.feather")
# 
# nn_idx_sorted <- readRDS("data/nn_eucl_olhc_w3_38a896430298c738055505dc89e042ac.rds") %>% .[["nn.idx"]] %>% sort_nn_idx(quotes_line$Date)
# 
# 
# set.seed(123456)
# boot_pred <- map(seq_len(100), function(i) {
#   print(i)
#   curr_nn_idx_boot <- bootstrap_nn(sort(quotes_line$Date), sort_nn_idx(nn_idx_sorted), k = k)
#   pred_nn(select(quotes_line_sorted, c("Low", "High")), nn_idx = curr_nn_idx_boot)
# })
# # saveRDS(boot_pred, "tmp/boot_pred.rds")
# 
# boot_pred <- readRDS("tmp/boot_pred.rds")
# 
# 
# all_complete <- rep(TRUE, nrow(quotes_line_sorted))
# for(i in seq_along(boot_pred)) {
#   print(i)
#   all_complete <- all_complete * rowSums(is.na(boot_pred[[i]])) == 0
#   gc()
# }
# 
# na_row_bool <- rowSums(is.na(nn_idx_sorted)) + rowSums(is.na())
# 
# test <- map(boot_pred, ~sum(calc_payoff_const_gamma(quotes_line_sorted[all_complete], buy = .$Low, sell = .$High, both_first = 234567)))
# 
# na_row_bool <- rowSums(is.na(nn$nn.idx)) == 0

# nn_idx[4418184, ]
# quotes_line[4418184, ]




```

## Neuronale Netzwerke

## Autoregressive Modelle

<!--chapter:end:03-analysis.Rmd-->

