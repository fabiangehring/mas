## Einfache Optimierungen


### Ohne Berücksichtigung der Marktvolatilität
```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
suppressPackageStartupMessages({
  library(dplyr)
  library(stringr)
  library(purrr)
  library(pbmcapply)
  library(tidyr)
  library(ggplot2)
  library(data.table)
  library(dtplyr)
  library(arrow)
  library(microbenchmark)
  library(profvis)
  library(dqrng)
  library(keras)
  library(magrittr)
  library(parallel)
  library(cumstats)
})

source("R/02-data.R")
source("R/03-analysis.R")
Rcpp::sourceCpp('src/calc_payoff_per_title.cpp')

data_wide_10 <- arrow::read_feather("data/steps/data_wide_10.feather")

set.seed(123456)
train_idx <- sample(x = nrow(data_wide_10), size = floor(0.8 * nrow(data_wide_10)))
test_idx <- setdiff(seq_len(nrow(data_wide_10)), train_idx)

both_first <- c("buy", "sell")[sample(c(1, 2), nrow(data_wide_10), replace = TRUE)]

```


Eine erste Möglichkeit, optimale Kauf- und Verkaufspreise zu finden, besteht darin, diese als prozentuale Abweichungen vom aktuellen Eröffnungspreis festzulegen. Die resultierenden Payoffs bei einer solchen Festlegung lassen sich dann ins Verhältnis zum Referenzpayoff mit Ausgleich per Tagesende stellen. Ein Payoff-Verhältnis über 1 kennzeichnet damit eine Strategie, welche der Referenzstrategie überlegen ist. Verhältnisse unter 1 kennzeichnen unterlegene Strategien. 


``` {r symmetric-open-change, fig.cap='Payoffvergleich bei symmetrischer Abweichung vom Eröffnungspreis (ohne Berücksichtigung Marktvolatilität)', fig.asp=0.6, echo=FALSE, message=FALSE, warning=FALSE}

variation_factor_open_file_name <- "data/optimizations/variation_factor_open.feather"
data_variation <- select(data_wide_10, Close_1, Open = "Open_0", Low = "Low_0", High = "High_0", Close_0)
opt_payoff_sym <- function(move, data, col, both_first, scale_fct = 1) {
  sum(calc_payoff_const_gamma(data, buy = (1 - move) * data[[col]], sell = (1 + move) * data[[col]], both_first = both_first), na.rm = TRUE) / scale_fct
}

if (file.exists(variation_factor_open_file_name)) {
  bootstraped_variation_factor_open <- arrow::read_feather(variation_factor_open_file_name)
} else {
  scale_fct_train <- sum(calc_payoff_const_gamma(data_variation[train_idx, ], both_first = both_first[train_idx]))
  bootstraped_variation_factor_open <- bootstrap_variation_factor(
    col = "Open",
    move = seq(0, 0.1, 0.001),
    data = data_variation[train_idx, ],
    both_first = both_first[train_idx],
    R = 100,
    mc.cores = ceiling(detectCores() * 0.51)
  )
  
  arrow::write_feather(bootstraped_variation_factor_open, variation_factor_open_file_name)
}

plot_variation_factor(bootstraped_variation_factor_open)

```

Dieses Verhältnis veranschaulicht Abbildung \@ref(fig:symmetric-open-change) für unterschiedliche Auslenkungshöhen. Lesebeispiel: Werden die Kauf- und Verkaufspreise 2.5% unter und über dem Eröffnungskurs des jeweiligen Tages gesetzt, so resultiert ein Gewinn, welcher rund 8% über demjenigen der Referenzstrategie liegt. 

Bei genauerer Betrachtung weist die Kurve einige interessante Eigenschaften auf: Der höchste Payoff wird bei einer Auslenkung der Preise um `r bootstraped_variation_factor_open$move[which.max(bootstraped_variation_factor_open$original)] * 100`% erreicht. Der Payoffüberschüss beträgt an diesem Punkt rund `r round((bootstraped_variation_factor_open$original[which.max(bootstraped_variation_factor_open$original)] - 1) * 100, 1)`%. Gleichzeitig zeigt sich, dass ein mehr oder weniger konstanter Überschuss von rund 8% im ganzen Auslenkungsbereich von 0.7 bis rund 4% erreicht werden kann.

Während im Bereich tieferer Auslenkungen viele kleinere Gewinne realisiert werden, sind es beim Setzen breiterer Schranken nur noch wenige, dafür grössere. Die Kurve zeigt, dass sich diese beiden Effekte im genannten Bereich in etwa die Waage halten. Dieses Ergebnis ist insofern interessant, als dass die genaue Preisbestimmung gar nicht von so grosser Relevanz sein könnte. Wichtig dabei zu erwähnen ist auch, dass während der Datenbereinigung tendenziell grosse Auslenkungen aus dem Datensatz entfernt wurden (\@ref(bereinigung)). Werden vermehrt auch extreme Marktbewegungen zugelassen, verschiebt sich die optimale Auslenkung der Preisschranken nach oben. In Kombination mit der Erkenntnis, dass auch bei stärkerer Bereinigung gute Payoffs bis 4% Auslenkung erreicht werden, könnte dies eine Motivation sein, die Preise eher breiter zu setzen.

Eine weitere Besonderheit der Kurve zeigt sich mit dem Abwärtsknick bei sehr kleinen Auslenkungen. Erklären lässt sich dieser Knick dadurch, dass bei allen Kursverläufen, bei denen der Eröffnungskurs gleichzeitig Höchst- oder Tiefstkurs ist, mindestens eine Schranke nicht mehr erreicht werden kann. Bereits beim Setzen etwas grösserer Schranken wird dieser Effekt wieder mehr als ausgeglichen.

Auffällig ist auch die Tatsache, dass eine Auslenkung von 0 (und damit einem Wiederherstellen der Delta-Neutralität gleich zum Eröffnungskurs) eine deutlich bessere Performance als die Referenzstrategie aufweist. Dies lässt sich damit erklären, dass die Werte im Datensatz offenbar eine Tendenz des "Overshootings" der Eröffnungspreise zeigen. Das beobachtete Bild lässt vermuten, dass sich die Preise im Laufe des Tages in der Tendenz wieder eher Richtung Schlusskurs des Vortages entwickeln. Der Ausgleich der aufgebauten Delta-Position "über Nacht" gleich zum Eröffnungskurs scheint damit besser, als bis am Abend zu warten. Eine praktische Schwierigkeit könnte aber eine noch geringe Liquidität der Märkte bei Marktöffnung sein.

Schliesslich stellt sich auch die Frage, inwiefern die gefundenen Ergebnisse als statistisch signifikant bezeichnet werden können. Zur Beurteilung dieser Frage wurde mittels Bootstrapverfahren ein 95%-Konfidenzband der Kurve ermittelt. Dieses ist als grau schraffierte Fläche am Rand der Kurve ersichtlich. Es zeigt sich, dass dieses Band relativ schmal ausfällt. Dies kann als Konsequenz der ausführlichen Datenbereinigung gesehen werden. Diese führt dazu, dass auch über verschiedene Boostrap-Samples hinweg die Payoffs stabil und wenig beeinflusst durch einzelne Beobachtungen ausfallen.


``` {r, echo=FALSE}

scale_fct_test <- sum(calc_payoff_const_gamma(data_variation[test_idx, ], both_first = both_first[test_idx]))
payoff_factor_test_max <- opt_payoff_sym(
  move = 0.009, 
  data = data_variation[test_idx, ], 
  col = "Open",
  both_first = both_first[test_idx],
  scale_fct = scale_fct_test
)

payoff_factor_test_4 <- opt_payoff_sym(
  move = 0.04, 
  data = data_variation[test_idx, ], 
  col = "Open",
  both_first = both_first[test_idx],
  scale_fct = scale_fct_test
)

```

Als zweites Mass zur Beurteilung der Aussagekraft lassen sich auch die Werte des Testdatensatzes heranziehen. In diesem beträgt der Payoffüberschuss im Vergleich zur Referenzstrategie bei 0.9% Auslenkung ebenfalls rund `r round((payoff_factor_test_max - 1) * 100, 1)`% und auch bei einer Auslenkung von 4% kommt der Überschuss bei `r round((payoff_factor_test_4 - 1) * 100, 1)`% zu liegen. Beide Werte zeigen damit hohe Ähnlichkeit zum Trainingsdatensatz.


### Mit Berücksichtigung der Marktvolatilität

Die bisherige Analyse untersucht die Auslenkung der Kauf- und Verkaufspreise um den gleichen prozentualen Wert für alle Einträge im Datensatz. Die Bimodalität der Kurve in Abbildung \@ref(fig:symmetric-open-change) deutet darauf hin, dass es sich dabei um eine Überlagerung mehrerer Kurven handeln könnte. Gelänge es, diese zu separieren und einzelnen Gruppen von Kursverkäufen zuzuweisen, könnten individuellere Preisschranken gewählt werden. Mit Hilfe dieser könnte der Payoff im Idealfall weiter gesteigert werden. 

Als Klassifizierungsmerkmal sei dazu die Volatilität der vergangenen 10 Handelstage herangezogen. Die Vermutung liegt nahe, dass eine volatile Marktsituation in der kurzfristigen Vergangenheit auch am nächsten Tag fortgesetzt werden könnte (beispielsweise Zeiten mit vielen marktrelevanten Informationen wie Finanzkrise, Corona-Krise, Dividenden-Saison). Umgekehrt könnten eher ruhig verlaufende Börsentage in den vergangenen Tagen auf eine ruhige Situation auch am aktuellen Tag hinweisen (beispielsweise ruhigere Börsentage während Sommerferien). 

Um dies zu untersuchen werden alle Einträge des Datensatzes in zwei Gruppen aufgeteilt. Einträge, welche eine aktuelle 10-Tages-Volatilität über dem Median aufweisen, werden einer Gruppe hoher Volatilität, die andern Einträge einer Gruppe tiefer Volatilität zugeordnet. Zu beachten gilt es hierbei, dass der Medianwert dabei einerseits nur innerhalb des jeweiligen Tickers betrachtet wird und für dessen Berechnung auch nur vergangene Werte miteinbezogen werden. 

Für beide Gruppen lassen sich danach im Trainingsset die bereits bekannten Payoffvergleiche mit dem Referenzszenario durchführen und graphisch darstellen (vgl. Abbildung \@ref(fig:symmetric-open-change-vol)).


``` {r symmetric-open-change-vol, fig.cap='Payoffvergleich bei symmetrischer Abweichung vom Eröffnungspreis (mit Berücksichtigung Marktvolatilität)', fig.asp=0.6, echo=FALSE, message=FALSE, warning=FALSE}

data_variation_vol_paths <- c(
  "data/steps/data_variation_vol.feather",
  "data/optimizations/bootstraped_variation_factor_open_low.feather", 
  "data/optimizations/bootstraped_variation_factor_open_high.feather"
)

data_wide_10_vol <- arrow::read_feather("data/steps/data_wide_10_vol.feather")

if (!all(map_lgl(data_variation_vol_paths, ~file.exists(.)))) {
  
  # split data into low and high varioance groups
  vol_10 <- data_wide_10_vol$Vol_10
  data_variation_vol <- data_wide_10 %>% select(Ticker, Date, Close_1, Open = "Open_0", Low = "Low_0", High = "High_0", Close_0) %>% mutate(Vol_10 = vol_10)
  
  vol_10_median <- data_variation_vol %>%
    mutate(ID = seq_len(nrow(.))) %>%
    arrange(Ticker, Date) %>%
    group_by(Ticker) %>%
    select(ID, Ticker, Vol_10) %>%
    group_split() %>%
    pbmclapply(function(x) list(ID = x$ID, Vol_10_med = cummedian(x$Vol_10)), mc.cores = 3) %>%
    bind_rows()
  
  data_variation_vol <- data_variation_vol %>% 
    mutate(ID = seq_len(nrow(.))) %>% 
    left_join(vol_10_median, by = "ID") %>% 
    select(-ID)
  
  data_variation_vol <- data_variation_vol %>% mutate(Vol_Group = factor(if_else(Vol_10 <= Vol_10_med, "Low", "High"), levels = c("Low", "High")))
  write_feather(data_variation_vol, data_variation_vol_paths[1])
  
  
  # find opimal move
  train_idx_low <- intersect(train_idx, which(data_variation_vol$Vol_Group == "Low"))
  bootstraped_variation_factor_open_low <- bootstrap_variation_factor(
    col = "Open",
    move = seq(0, 0.1, 0.001),
    data = data_variation_vol[train_idx_low, ],
    both_first = both_first[train_idx_low],
    R = 0,
    mc.cores = ceiling(detectCores() * 0.51)
  )
  write_feather(bootstraped_variation_factor_open_low, data_variation_vol_paths[2])
  
  train_idx_high <- intersect(train_idx, which(data_variation_vol$Vol_Group == "High"))
  bootstraped_variation_factor_open_high <- bootstrap_variation_factor(
    col = "Open",
    move = seq(0, 0.1, 0.001),
    data = data_variation_vol[train_idx_high, ],
    both_first = both_first[train_idx_high],
    R = 0,
    mc.cores = ceiling(detectCores() * 0.51)
  )
  write_feather(bootstraped_variation_factor_open_high, data_variation_vol_paths[3])
  
  
} else {
  data_variation_vol <- read_feather(data_variation_vol_paths[1])
  bootstraped_variation_factor_open_low <- read_feather(data_variation_vol_paths[2])
  bootstraped_variation_factor_open_high <- read_feather(data_variation_vol_paths[3])
}

bind_rows(
  mutate(bootstraped_variation_factor_open_low, group = factor("Tiefe Vol_10", c("Tiefe Vol_10", "Hohe Vol_10"))),
  mutate(bootstraped_variation_factor_open_high, group = factor("Hohe Vol_10", c("Tiefe Vol_10", "Hohe Vol_10")))
) %>% plot_variation_factor()

```



```{r}

opt_move_low <- bootstraped_variation_factor_open_low$move[which.max(bootstraped_variation_factor_open_low$original)]
opt_move_high <- bootstraped_variation_factor_open_high$move[which.max(bootstraped_variation_factor_open_high$original)]

test_idx_low <- intersect(test_idx, which(data_variation_vol$Vol_Group == "Low"))
test_idx_high <- intersect(test_idx, which(data_variation_vol$Vol_Group == "High"))

scale_fct_test <- sum(calc_payoff_const_gamma(data_variation[test_idx, ], both_first = both_first[test_idx]))
payoff_test_low <- opt_payoff_sym(move = opt_move_low, data = data_variation_vol[test_idx_low, ], both_first = both_first[test_idx_low], col = "Open")
payoff_test_high <- opt_payoff_sym(move = opt_move_high, data = data_variation_vol[test_idx_high, ], both_first = both_first[test_idx_high], col = "Open")

payoff_factor_test_vol <- (payoff_test_low + payoff_test_high) / scale_fct_test
saveRDS(payoff_factor_test_vol, "data/optimizations/payoff_factor_sym_grouped.rds")

```

Die Grafiken zeigen das erwartete Bild. Für die Gruppe tieferer vergangener Volatilitäten wird der maximale Payoff bei einer Auslenkung von `r round(opt_move_low * 100, 1)`% erreicht. Bei der Gruppe höherer Volatilität bei `r round(opt_move_high * 100, 1)`%. Angewendet auf das Testset resultiert ein Payoffüberschuss von rund `r round((payoff_factor_test_vol - 1) * 100, 1)`%. Dies ist höher aus als im ungruppierten Fall. 

Insbesondere bei der Gruppe der höheren Volatilitäten weist die Kurve aber weiterhin eine bimodale Form auf. Es scheint, als ob mit der gemachten Gruppierung zwar ein Teil der Varianz des aktuellen Tages erklärt werden konnte, Teile davon aber weiter unerklärt bleiben. Eine Möglichkeit bestünde nun darin, die Anzahl Gruppierungen weiter zu erhöhen, indem beispielsweise nicht nur der Median, sondern die Quartile, Dezile, Percentile etc. als Klassifikationsgrenzen gewählt werden. Darauf sei an dieser Stelle verzichtet und zu andern Ansätzen übergegangen.

```{r, echo=FALSE, results='hide'}
rm(list = ls())
gc()
```




