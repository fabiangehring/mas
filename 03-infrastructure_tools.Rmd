# Infrastruktur und Tools

## Cloud Setup
Gewisse in dieser Arbeit durchgeführte Analysen sind sehr rechen- und speicherintensiv. Sie stellen damit erhöhte Anforderungen an die eingesetzte Infrastruktur. Für die vorliegenden Analysen stellte sich folgender Setup mit drei unterschiedlichen virtuellen Maschinentypen (VMs) als geeignet heraus:

-	Datenaufbereitung: Eine Maschine mit 4 Cores und mindestens 32 GB Memory zum Prototyping und die Datenaufbereitung
-	Multi-CPU: Eine Maschine mit mindestens 16 CPUs zur Berechnung parallelisierbarer Aufgaben (z.B. KNN) auf grösseren Datensätzen
-	GPU: Eine Maschine mit mindestens 16 GB RAM, 4 CPUs und einer für kleinere Machine Learning Probleme geeigneten GPU (beispielsweise NVIDIA Tesla K80 oder NVIDIA Tesla M60).

Im Rahmen dieser Arbeit wurden drei der bekanntesten Cloud-Anbieter ausprobiert. Aus Sicht des Autors unterscheiden sich diese in ihrer Handhabung stärker, als dies ursprünglich hätte vermutet werden können. Der Aufbau eines geeigneten Infrastruktursetups stellte sich - trotz auf den ersten Blick vorgefertigter Lösungen - als zeitintensiv heraus. Da sich eine einmal gefundene Einstellung aber auch in späteren Projekten wiederverwenden lässt, rechtfertigt sich dieser Aufwand. Die gemachten Erkenntnisse seien an dieser Stelle daher festgehalten.

Allen nachfolgend beschriebenen Lösungen sehen ein für Studenten kostenfreies Start-Kontingent vor. Die Ausführungen beziehen sich auf diese. Ebenfalls allen Anbietern gemein ist, dass sich virtuelle Computer mit wenigen Klicks und dem gewünschten Betriebssystem erstellen lassen. Alle drei getesteten Dienste bieten ferner neben normalen Instanzen auch sogenannte "Spot" Instanzen an. Diese unterscheiden sich von normalen VMs insofern, als dass es sich dabei um Einmalinstanzen handelt. Einmal beendet erlöschen diese. Bei grosser Nachfrage nach Rechenkapazität können Spot Instanzen vom Anbieter zudem ohne Vorwarnung heruntergefahren werden. Sie eignen sich daher nur für nicht unterbrechungsanfällige Prozesse. Im Gegensatz sind sie deutlich günstiger als reguläre Instanzen.


### Microsoft Azure
Die Cloud-Computing Dienste von Microsoft nennen sich Azure. Die Plattform bietet verschiedene vordefinierte Maschinentypen, welche sich im Wesentlichen in der Anzahl CPU, RAM und Grösse des persistenten Speichers unterscheiden. Einige Maschinen bieten zudem Zugriff auf eine oder mehrere GPU(s). 

Beim Test zeigte sich allerdings, dass trotz Account für Bildungseinrichtungen oft keine Maschinen verfügbar waren. Die Verfügbarkeit war im Test dabei sehr abhängig von der jeweiligen Tageszeit. Während am frühen Morgen Mitteleuropäischer Zeit oft Maschinen verfügbar waren, war dies oft weder am Nachmittag noch am Abend der Fall. Die grösste VM, welche während des Tests erstellt werden konnte, war eine NC6 Instanz mit 6 CPUs 56 GB Memory und einer NVIDIA Tesla K80 GPU. Insbesondere für CPU-lastige Analysen stellte sich dieser Setup als zu wenig leistungsstark heraus. 

Bei allen anderen Anbietern unterscheidet sich die Verfügbarkeit der VMs je nach Region. Bei Azure kommt allerdings erschwerend hinzu, dass alle Regionen einzeln durchprobiert werden müssen. Spot Instanzen stehen zudem nur für gewisse Maschinentypen zur Verfügung. Auch für die oben erwähnte Instanz stand diese Option im Test nicht zur Verfügung.

Spot Instanzen lassen sich vor allem dann gut nutzen, wenn Systemabbilder einfach erstellt und davon später wieder neue Instanzen erzeugt werden können. Azure unterscheidet sich hier von seinen Konkurrenten insofern, als dass dies nicht einfach auf Knopfdruck gelingt. Viel mehr muss manuell erst eine "Generalisierung" der Instanz vorgenommen werden [vgl. @azure].

Die Beurteilung der Qualität der Dokumentation ist subjektiv. Aus Sicht des Autors ist diejenige von Azure weniger ausführlich und selbsterklärend als diejenige der anderen getesteten Kandidaten. 

### Google Cloud
Die Cloud-Computing Dienste von Google nennen sich "Google Cloud". Neu registrierte Kunden profitieren im Vergleich mit den anderen Anbietern vom höchsten kostenlosen Startguthaben (Azure mit Studenten Account: $100, AWS ($30) mit Github Starter Package ($70): $100, Google Cloud: $300).

Die Management Oberfläche wirkt im Vergleich zu den Konkurrenten sehr aufgeräumt. Die Dokumentation ist gut. Als einzige der getesteten Anbieter konnten die VMs zudem völlig individuell gestaltet werden (Anzahl CPU, Memory, Anzahl und Art GPU). 

Als Nachteil entpuppte sich im Test allerdings das komplexe Limitensystem. Fast alle Komponenten unterliegen verschiedenen Limiten. Um eine GPU hinzuzufügen, müssen beispielsweise gleichzeitig die Limiten für "GPU global", "GPU der gewählten Region" und "GPU des jeweiligen Typs" (Beispielsweise NVIDIA Tesla K80) erfüllt sein. 

Die Standardeinstellungen des Testaccounts erlaubten Instanzen bis 24 CPUs. Mehr Einheiten waren nicht möglich. Die Standardeinstellung für GPUs beträgt gar 0. Eine Erhöhung der Quote kann im Management Portal beantragt werden. Im Test zeigte sich jedoch, dass sowohl Anträge zu Erhöhung der CPU wie auch GPU Limiten automatisch abgelehnt wurden. Die Erstellung einer Instanz mit GPU gelang so auch über mehrere Tage hinweg nicht. Schriftliche Kontaktaufnahmen mit der Bitte die GPU Limite auf 1 zu erhöhen wurden trotz Begründung wiederholt abgelehnt. Auch nach telefonischer Kontaktaufnahme mit dem Support gelang es nicht, die Quote zu erhöhen. Das Problem stiess beim Mitarbeitenden von Google zwar auf Verständnis, er selbst konnte die Quote allerdings ebenfalls nicht erhöhen. 

### Amazon Web Services (AWS)
Das Cloud Computing Angebot von Amazon nennt sich "Elastic Compute Cloud" (Amazon EC2). Als Einziger der getesteten Anbieter fällt dieser im Test nicht durch nicht verfügbare Maschinentypen oder unzureichende Limiten auf. Ferner können auch die meisten VMs (im Test bis 40 CPUs) als Spot Instanzen ausgeführt werden. Da die Imageerstellung erfolgt sehr einfach auf Knopfdruck. AWS fällt zudem durch eine sehr gute und ausführliche Dokumentation auf. So ist beispielsweise die Erstellung eines von der Instanz unabhängigen persistenten Speichers (in AWS-Lingo: EBS) sowie der Prozess zum Mounten ebendieses Schritt-für-Schritt erklärt [vgl. @aws]. Damit kann auch bei der Verwendung von Spot Instanzen dauerhaft gespeichert werden. Ferner sind auch die Preise im Vergleich zur Konkurrenz sehr transparent und wettbewerbsfähig.

Da auch nach längeren Versuchen nur mit AWS die oben beschriebene Ziel-Infrastruktur erreicht werden konnte, wurde für die vorliegende Arbeit dieser Anbieter als Cloud Computing Lösung verwendet.

## Verwendete Software
Zur Durchführung der Analysen hat sich der abwechselnde Einsatz von R und Python bewährt. Es zeigte sich, dass beide Tools / Sprachen ihre Stärken in verschiedenen Bereichen haben. Grundsätzlich lässt sich dies so zusammenfassen, dass datenaufbereitende Schritte in R durchgeführt wurden. Für das Training der neuronalen Netze stellten sich die entsprechenden Bibliotheken von Python als geeigneter heraus.

### R / RStudio Server
Die Datenaufbereitung und -bereinigung sowie Analysen mit Ausnahme der neuronalen Netze wurden in der Sprache R und der Entwicklungsumgebung RStudio Server durchgeführt. Erstere kann auf eine sehr breite Community mit entsprechend gut dokumentierten Beispielen in diversen Foren zurückgreifen. Ferner hat sich in den letzten Jahren unter dem Namen "tidyverse" eine Sammlung gut unterhaltener und dokumentierter Packages zum defacto-Standard etabliert. Dieses wird von Mitarbeitern der Firma RStudio unterhalten und steht zur freien Nutzung zur Verfügung. Gleiche Firma ist es auch, welche unter dem Namen RStudio Server eine hostbare Entwicklungsumgebung (IDE) anbietet. Diese wird in einer gratis verfügbaren und einer kostenpflichtigen Variante angeboten. Für die Arbeit wurde die freie Version verwendet. Verschiedene Builds für die gängigsten Linux Distributionen stehen auf der Unternehmenswebseite zur Verfügung [vgl. @rstudio]. Der Zugriff auf die IDE erfolgt über den Browser. Das Look and Feel unterscheidet sich nicht von der ebenfalls frei verfügbaren lokalen Installation. Dies ist insbesondere darum interessant, da sich der Programmcode so direkt auf den erstellten Cloud Instanzen entwickeln lässt.

Schwächen von R zeigten sich in der inherenten Single-Threadigkeit des Tools. Die Multithreadigkeit kann mit Hilfe zusätzlicher Packages (beispielsweise parallel, pbapply) erreicht werden und ist insbesondere beim Einsatz auf der oben beschriebenen Multi-CPU Instanz von Relevanz. Zwar stehen auf R Packages für Keras und Tensorflow zur Verfügung, deren Verwendung im Zusammenspiel mit RStudio Server stellte sich allerdings als sehr instabil heraus. Dies manifestierte sich in mehreren Abstürzen der Entwicklungsumgebung. Da diese Packages selber lediglich Wrapper auf die gleichnamigen Python Libraries sind, stellte sich deren Verwendung direkt in Python als die bessere Variante heraus. 

Zum von Daten zwischen den beiden Sprachen stellte sich der Weg über zwischengespeicherte "Feather" Files als am geeignetsten heraus. Es handelt sich dabei um ein Format, das beide Sprachen auch für grössere Datenmengen sehr performant laden und speichern können. Tatsächlich kann in R / RStudio mit Hilfe des Packages "reticulate" auch Python Code direkt ausgeführt werden, respesktive Objekte beider Sprachen in einem gewissen Masse automatisch ausgetauscht werden. Auf diesen Austausch wurde im vorliegenden Fall allerdings verzichtet.

Eine Eigenheit der Sprache R stellt zudem die matrix- und verktorbasierte programmierweise dar. Zwar sind gleiche Manipulation auch mit klassischer "loop-Ansätzen" möglich. Diese gehen allerdings mit erheblichen Performanceeinbussen (speziell bei der Bearbeitung grosser Datenmengen) einher. Es hat sich daher bewährt, performancekritische Funktionen in C++ zu schreiben. Das Package "Rcpp" übernimmt dabei die Verknüpfung dieser Funktionen in R.

### Python / (Ana)conda
Für Python stand keine auf dem Server ausführbare Entwicklungsumgebung zur Verfügung. Da sich der Einsatz von Python allerdings auf das Training neuronaler Netze beschränkte, reichte die Verwendung eines Jupyter Notebook Servers. Auf diesen lässt sich wiederum einfach via Webbrowser zugreifen.

Ebenfalls als wertvoll zeigte sich die Verwendung von conda Environments. Diese erlauben projektspezifische Bibliotheksinstallationen sowohl für Python als auch R. Es zeigte sich, dass das erwähnte Problem der Instabilität von Keras und Tensorflow in R bei der direkten Verwendung von Python nicht auftauchte. Erwähnent sei hier aber, dass für die Verwendung der GPU Version auf dem System sowohl passende Grafiktreiber sowie die zur Tensorflow Version passende Version von CUDA installiert sein muss [vgl. @tensorflow].


```{r, echo=FALSE, results='hide'}
rm(list = ls())
gc()
```


