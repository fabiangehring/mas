# Analyse


```{r message=FALSE, warning=FALSE, setup_analysis, echo=FALSE}
suppressPackageStartupMessages({
  library(dplyr)
  library(stringr)
  library(purrr)
  library(pbmcapply)
  library(tidyr)
  library(ggplot2)
  library(data.table)
  library(dtplyr)
  library(arrow)
  library(microbenchmark)
  library(profvis)
  library(dqrng)
  library(keras)
  library(magrittr)
})

source("R/02-data.R")
source("R/03-analysis.R")

Rcpp::sourceCpp('src/bootstrap_nn_idx.cpp')
Rcpp::sourceCpp('src/calc_payoff_per_title.cpp')
Rcpp::sourceCpp('src/drop_late_nn_idx.cpp')
```

Für die Analyse der Daten mit dem Ziel einen Kaufs- sowie einen Verkaufkurs zu prognostizieren, bei dem der Delta-Hedge nachgezogen werden soll, werden nachfolgend verschiedene Techniken eingesetzt. Diese sind:

- Einfache Optimierungen
- Klassifikationsverfahren (KNN):
- Neuronale Netzwerke (RNN, LTSM):
- Autogressive Modelle (GARCH)

Allen Analysen gemein ist, dass jeweils gefundene Strategien mit der Referenzstrategie verglichen wird, welche keine innertägliche Anpassung des Deltas vorsieht, sondern diese gänzlich zum Tagesschlusskurz durchführt. Eine weitere Gemeinsamkeit liegt darin, dass die verwendeten Daten keine Ausage über den Verlauf des Preises innerhalb des Tages zulassen. Inbesondere kann nicht ermittelt werden, ob zuerst eine obere oder eine untere Grenze Preisgrenze überschritten wurde. Da diese Reihenfolge aber wie in Kapitel \@ref(forschungsfrage) ausgeführt von Relevanz ist, wird für alle Analysen ein Ansatz verwendet, bei welchem zufällig bestimmt wird, ob am jeweiligen Tag zuerst eine Abwärts- oder eine Aufwärtsbewegung stattgefunden hat.[^Alternative denkbare Vorgehensweisen sind: Immer zuerst Aufwärtsbewegung, immer zuerst Abwärtsbewegung, immer die bezügl. Payoff schlechtere Reihenfolge oder immer die bezügl. Payoff bessere Variante] Auch ein mehrmaliges Erreichen der Kaufs- und Verkaufsschwelle ist innerhalb des Tages bei sehr fluktierenden Preisen in Realität denkbar. Es wären bezüglich Optimierung des Payoffs sogar sehr wünschenswerte Ereignisse. Auf die Berücksichtigung solcher Fälle wird in der Analyse allerdings verzichtet. Das Bewusstsein über deren Exsistenz ist aber bei der Interprätation der Ergebnisse dennoch inneressant, da die Payoffs der gefundenen Strategien diebezüglich als untere Grenzen des Payoffs betrachtet werden können.


## Einfache Optimierungen

Eine erste Möglichkeit, optimale Kaufs- und Verkaufspreise zu finden besteht darin, diese im Testdatensatz mittels einfacher Optimierung zu evaluieren. In einer ersten sehr einfachen Evaluation bietet es sich an, die Kaufs- und Verkaufsmarken als prozentuale Abweichungen vom aktuellen Preis festzulegen. Als Startgrösse bietet sich hierbei der "Open" Kurs des jeweiligen Tages an. Die resultierenden Payoffs bei einer solchen Festlegung lassen sich dann ins Verhältnis zum Referenzpayoff mit Glattstellung der Deltaposition bei Tagesende stellen. Ein Payoff-Verhältnis über 1 kennzeichnet damit eine Strategie, welche der Referenzstrategie überlegen ist. Verhälntisse unter 1 kennzeichnen unterlegene Strategien. 

``` {r, fig.cap='Payoffvergleich bei symmetrischer Abweichung vom Eröffnungspreis', fig.asp=1, fig.pos = '!H', analysis-etf-data, echo=FALSE, message=FALSE, warning=FALSE}

variation_factor_open_file_name <- paste0("data/variation_factor_open.feather")

if (file.exists(variation_factor_open_file_name)) {
  bootstraped_variation_factor_open <- arrow::read_feather(variation_factor_open_file_name)
} else {
  
  opt_payoff_sym <- function(move, data, col, both_first, scale_fct = 1) {
    sum(calc_payoff_const_gamma(data, buy = (1 - move) * data[[col]], sell = (1 + move) * data[[col]], both_first = both_first), na.rm = TRUE) / scale_fct
  }
  
  quotes_clean <- arrow::read_feather(paste0("data/quotes_clean.feather"))
  data <- quotes_clean %>%
    dplyr::group_by(Ticker) %>%
    dplyr::group_modify(~widen(., window = 1, cols = "Close", keep = c("Close", "Open", "Low", "High"))) %>%
    ungroup() %>%
    rename(Close_0 = "Close") %>%
    select(-Ticker)
  data <- normalize_quotes(data, "Close_1", names(data))
  data <- tidyr::drop_na(data)
  rm(quotes_clean)
  
  set.seed(123456)
  both_first <- c("buy", "sell")[sample(c(1, 2), nrow(data), replace = TRUE)]
  
  bootstraped_variation_factor_open <- bootstrap_variation_factor(
    move = seq(0, 0.1, 0.001), 
    data = data, 
    both_first = both_first, 
    R = 100,
    mc.cores = 4
  )
  
  arrow::write_feather(bootstraped_variation_factor_open, variation_factor_open_file_name)
  
}

plot_variation_factor(bootstraped_variation_factor_open)

```

Abbildung \@ref(fig:symmetric_open_change) veranschaulicht dieses Verhältnis bei varierender symmetrischer Abweichung vom Startpreis. Lesebeispiel: 


## Klassifikationsverfahren
```{r, knn-data-preparation, echo=FALSE}

quotes_clean <- arrow::read_feather(paste0("data/quotes_clean.feather"))
window <- 3
data_wide_3_all <- quotes_clean %>%
  dplyr::group_by(Ticker) %>%
  dplyr::group_modify(~widen(., window = window + 1, cols = c("Close", "Open", "Low", "High"), keep = c("Date", "Close", "Open", "Low", "High"))) %>%
  dplyr::ungroup() %>%
  dplyr::rename(Close_0 = "Close", Open_0 = "Open", Low_0 = "Low", High_0 = "High") %>%
  normalize_quotes(paste0("Close_", window + 1), setdiff(names(.), c("Ticker", "Date")))

# remove unneeded (after normalization) day before window start
data_wide_3 <- dplyr::select(data_wide_3_all, c("Ticker", "Date", str_subset(names(data_wide_3_all), paste0("_[^", window + 1, "]$"))))

# drop NA (for window and current)
data_wide_3 <- tidyr::drop_na(data_wide_3)

# remove current date except "Open"
data_wide_3 <- dplyr::select(data_wide_3, c("Ticker", "Date", "Open_0", str_subset(names(data), paste0("_[^", 0, "]$"))))

# sort columns nicely
data_wide_3 <- data_wide_3 %>% dplyr::select(c("Ticker", "Date"), sort(stringr::str_subset(names(.), "_[0-9]+$"), decreasing = TRUE))

# arrow::write_feather(data_wide_3, "data/data_wide_3.feather")
# data_wide_3 <- arrow::read_feather("data/data_wide_3.feather")
# data_wide_3_hash <- digest::digest(data_wide_3)


id_cols <- c("Ticker", "Date")
data_wide_3_all <- data_wide_3[, id_cols] %>% inner_join(data_wide_3_all, by = id_cols)
# arrow::write_feather(data_wide_3_all, "data/data_wide_3_all.feather")
# data_wide_3_all <- arrow::read_feather("data/data_wide_3_all.feather")
# data_wide_3_all_hash <- digest::digest(data_wide_3_all)

rm(quotes_clean)

```


```{r, knn-calculation-classic, echo=FALSE}

data_wide_3 <- arrow::read_feather("data/data_wide_3.feather")

knn_eucl_chunkwise <- find_nn_chunkwise(
  data = arrange(data_wide_3, Date, Ticker),
  distance = "euclidean",
  k = 75,
  n_chunks = 200,
  mc.cores = 4
)

saveRDS(knn_eucl_chunkwise, "data/knn_eucl_chunkwise.rds")

```


```{r, keras, echo=FALSE}

library(magrittr)

data_wide_3 <- arrow::read_feather("data/data_wide_3.feather")
data_wide_3_all <- arrow::read_feather("data/data_wide_3_all.feather")

set.seed(321654)
train_idx <- sample(x = nrow(data_wide_3), size = floor(0.8 * nrow(data_wide_3)))
test_idx <- setdiff(seq_len(nrow(data_wide_3)), train_idx)

all_data <- as.matrix(dplyr::select(data_wide_3, -Ticker, -Date))
train_data <- all_data[train_idx, ]
test_data <- all_data[test_idx, ]

n_groups_per_col <- 10
cols <- c("Low_0", "High_0", "Close_0")
all_labels <- multivariate_discretization(data_wide_3_all, train_idx, test_idx, cols, n_groups_per_col) %$% groups
train_labels <- all_labels[train_idx]
test_labels <- all_labels[test_idx]

model <- keras::keras_model_sequential() %>%
  keras::layer_dense(units = 512, activation = "relu",  input_shape = dim(train_data)[2]) %>%
  keras::layer_dense(units = 512, activation = "relu") %>%
  keras::layer_dense(units = n_groups_per_col^length(cols), activation = "softmax")


model %>% keras::compile(
  optimizer = 'adam', 
  loss = 'sparse_categorical_crossentropy',
  metrics = c('accuracy')
)

history <- model %>% keras::fit(
  train_data,
  train_labels,
  epochs = 50,
  batch_size = 64, 
  validation_split = 0.2
)

summary(model)
keras::save_model_hdf5(model, "data/2_dense_512_window_3_epoch_50_batch_64_val_split_20.hdf5")
print("done")

```










##### Old






```{r, knn-calculation, echo=FALSE}
# data_bkp <- data
# data <- data_bkp
orig_order <- order(desc(data$Date))
data <- data %>% select(-Ticker) %>% arrange(desc(Date))

counts <- data %>% select(Date) %>% group_by(Date) %>% summarise(cnt = n()) %>% ungroup() %>% mutate(cum_sum = cumsum(cnt))
n_chunks <- 100
breaks <- nrow(data) / n_chunks * seq_len(n_chunks)

split_dates <- map(breaks, ~counts$Date[[min(which(counts$cum_sum>=.))]])

nn <- function(i, split_dates, data, dates){
  split_date <- split_dates[[i]]
  curr_data <- data[dates <= split_date, ]
  curr_dates <- dates[dates <= split_date]
  curr_query <- data[dates <= split_date & dates > ifelse(i == 1, -Inf, split_dates[[i-1]]), ]
  RANN2::nn2_cpp2(data = curr_data, query = curr_query, group = as.integer(curr_dates), k = 50, k_internal = 1.2*50)
}

knn_eucl_list <- pbmcapply::pbmclapply(
  X = rev(seq_len(n_chunks)),
  FUN = nn,
  split_dates = split_dates,
  data = as.matrix(select(data, -Date)),
  dates = data$Date,
  mc.cores = parallel::detectCores()
)

knn_eucl_list_hash <- digest::digest(knn_eucl_list)
# saveRDS(knn_eucl_list_hash, paste0("tmp/knn_eucl_list_", knn_eucl_list_hash, ".rds"))

knn_eucl <- purrr::transpose(knn_eucl_list) %>% map(~do.call(rbind, .[rev(seq_along(.))]))
knn_eucl_hash <- digest::digest(knn_eucl)
# arrow::write_feather(as_tibble(knn_eucl$nn.idx), paste0("tmp/knn_eucl_idx_", knn_eucl_hash, ".feather"))
# arrow::write_feather(as_tibble(knn_eucl$nn.dists), paste0("tmp/knn_eucl_dists_", knn_eucl_hash, ".feather"))

```



```{r, knn-single-plot, echo=FALSE}

knn_eucl_idx <- arrow::read_feather("tmp/knn_eucl_idx_7d3356cf64a5f81081840f6b1b370d77.feather")
knn_eucl_dists <- arrow::read_feather("tmp/knn_eucl_dists_7d3356cf64a5f81081840f6b1b370d77.feather")


idx <- 123456 

knn_eucl_idx[idx, ]
knn_eucl_dists[idx, ]
select(data[idx, ], -c("Ticker", "Date"))
select(data[head(as.integer(knn_eucl_idx[idx, ]), 1), ], -c("Ticker", "Date"))

sqrt(sum((select(data[idx, ], -c("Ticker", "Date")) - select(data[head(as.integer(knn_eucl_idx[idx, ]), 1), ], -c("Ticker", "Date")))^2))

id_cols <- c("Ticker", "Date")
knn <- RANN::nn2(
  data = select(data, -id_cols),
  query = select(data[idx, ], -id_cols),
  k = 10
)

plot_nn(
  data_wide_curr = data_all[idx, ],
  data_wide_nn = data_all[c(3660437, 2876442, 1411227), ]
)

# id <- 280000
# valid_idx <- seq_len(n)[rowSums(is.na(nn$nn.idx)) == 0]
# 
# k <- 10
# plot_nn(data_wide_0[valid_idx[id], ], data_wide_0[nn$nn.idx[valid_idx[id], seq_len(k)],])

```

```{r, knn-prediction-power, echo=FALSE}
# k <- 20
# 
# nn_idx <- as.matrix(arrow::read_arrow("data/nn_idx_eucl_olhc_w3_38a896430298c738055505dc89e042ac.feather"))
# 
# nn_pred <- pred_nn(select(data_wide_0, c("Low_0", "High_0")), nn_idx = nn_idx[, seq_len(k)])
# na_row_bool <- rowSums(is.na(nn_pred)) > 0
# 
# nn_pred_sample <- nn_pred[!na_row_bool, ] %>% rename(Buy = Low_0, Sell = High_0)
# quotes_line_sample <- quotes_line[!na_row_bool, ]
# 
# plot_ratio_history(quotes_line = quotes_line_sample, data_pred = nn_pred_sample)
# 
# 
# ### perform bootstraping
# size_map <- quotes_line %>%
#   select(Date) %>%
#   group_by(Date) %>% 
#   summarize(count = n()) %>% 
#   ungroup() %>% 
#   mutate(size = cumsum(count) - count) %>% 
#   select(-count)
# size <- quotes_line %>% select(Date) %>% left_join(size_map, by = "Date") %>% .[["size"]]
# ###
# 
# boot_nn_idx_1 <- bootstrap_nn_idx(nn_idx, size, 20, 123456)
# 
# nn_pred_boot <- pred_nn(select(data_wide_0, c("Low_0", "High_0")), nn_idx = boot_nn_idx_1)
# na_row_bool_boot <- rowSums(is.na(nn_pred_boot)) > 0
# 
# nn_pred_boot_sample <- nn_pred_boot[!na_row_bool_boot, ] %>% rename(Buy = Low_0, Sell = High_0)
# quotes_line_boot_sample <- quotes_line[!na_row_bool_boot, ]
# 
# plot_ratio_history(quotes_line = quotes_line_boot_sample, data_pred = nn_pred_boot_sample)
# 
# 

```



```{r, knn-bootstraping, echo=FALSE}
# k <- 20
# quotes_line <- readRDS("tmp/quotes_line.rds")
# quotes_line_sorted <- quotes_line %>% arrange(Date)
# 
# nn_idx <- arrow::read_arrow("data/nn_idx_eucl_olhc_w3_38a896430298c738055505dc89e042ac.feather")
# 
# nn_idx_sorted <- readRDS("data/nn_eucl_olhc_w3_38a896430298c738055505dc89e042ac.rds") %>% .[["nn.idx"]] %>% sort_nn_idx(quotes_line$Date)
# 
# 
# set.seed(123456)
# boot_pred <- map(seq_len(100), function(i) {
#   print(i)
#   curr_nn_idx_boot <- bootstrap_nn(sort(quotes_line$Date), sort_nn_idx(nn_idx_sorted), k = k)
#   pred_nn(select(quotes_line_sorted, c("Low", "High")), nn_idx = curr_nn_idx_boot)
# })
# # saveRDS(boot_pred, "tmp/boot_pred.rds")
# 
# boot_pred <- readRDS("tmp/boot_pred.rds")
# 
# 
# all_complete <- rep(TRUE, nrow(quotes_line_sorted))
# for(i in seq_along(boot_pred)) {
#   print(i)
#   all_complete <- all_complete * rowSums(is.na(boot_pred[[i]])) == 0
#   gc()
# }
# 
# na_row_bool <- rowSums(is.na(nn_idx_sorted)) + rowSums(is.na())
# 
# test <- map(boot_pred, ~sum(calc_payoff_const_gamma(quotes_line_sorted[all_complete], buy = .$Low, sell = .$High, both_first = 234567)))
# 
# na_row_bool <- rowSums(is.na(nn$nn.idx)) == 0

# nn_idx[4418184, ]
# quotes_line[4418184, ]




```

## Neuronale Netzwerke

## Autoregressive Modelle
