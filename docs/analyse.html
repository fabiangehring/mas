<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Kapitel 4 Analyse | Die Bestimmung optimaler Kauf- und Verkaufspreise von Basiswerten zur Wahrung der täglichen Delta-Neutralität beim Handel von Aktienoptionen mit Hilfe verschiedener Machine Learning Methoden</title>
  <meta name="description" content="Kapitel 4 Analyse | Die Bestimmung optimaler Kauf- und Verkaufspreise von Basiswerten zur Wahrung der täglichen Delta-Neutralität beim Handel von Aktienoptionen mit Hilfe verschiedener Machine Learning Methoden" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Kapitel 4 Analyse | Die Bestimmung optimaler Kauf- und Verkaufspreise von Basiswerten zur Wahrung der täglichen Delta-Neutralität beim Handel von Aktienoptionen mit Hilfe verschiedener Machine Learning Methoden" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Kapitel 4 Analyse | Die Bestimmung optimaler Kauf- und Verkaufspreise von Basiswerten zur Wahrung der täglichen Delta-Neutralität beim Handel von Aktienoptionen mit Hilfe verschiedener Machine Learning Methoden" />
  
  
  

<meta name="author" content="Fabian Gehring, Sattelbogenstrasse 31, 5610 Wohlen" />
<meta name="author" content="Betreut durch Dr. Thomas Oskar Weinmann, ZHAW" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="infrastruktur-und-tools.html"/>
<link rel="next" href="summary-and-outlook.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Management Summary</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Einleitung</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#motivation"><i class="fa fa-check"></i><b>1.1</b> Motivation</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#forschungsfrage"><i class="fa fa-check"></i><b>1.2</b> Forschungsfrage</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="daten.html"><a href="daten.html"><i class="fa fa-check"></i><b>2</b> Daten</a><ul>
<li class="chapter" data-level="2.1" data-path="daten.html"><a href="daten.html#bezug-und-umfang"><i class="fa fa-check"></i><b>2.1</b> Bezug und Umfang</a><ul>
<li class="chapter" data-level="2.1.1" data-path="daten.html"><a href="daten.html#aktienuniversum"><i class="fa fa-check"></i><b>2.1.1</b> Aktienuniversum</a></li>
<li class="chapter" data-level="2.1.2" data-path="daten.html"><a href="daten.html#preisinformationen"><i class="fa fa-check"></i><b>2.1.2</b> Preisinformationen</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="daten.html"><a href="daten.html#aufbereitung"><i class="fa fa-check"></i><b>2.2</b> Aufbereitung</a><ul>
<li class="chapter" data-level="2.2.1" data-path="daten.html"><a href="daten.html#bereinigung"><i class="fa fa-check"></i><b>2.2.1</b> Bereinigung</a></li>
<li class="chapter" data-level="2.2.2" data-path="daten.html"><a href="daten.html#normalisierung"><i class="fa fa-check"></i><b>2.2.2</b> Normalisierung</a></li>
<li class="chapter" data-level="2.2.3" data-path="daten.html"><a href="daten.html#volatilitätsmass"><i class="fa fa-check"></i><b>2.2.3</b> Volatilitätsmass</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="infrastruktur-und-tools.html"><a href="infrastruktur-und-tools.html"><i class="fa fa-check"></i><b>3</b> Infrastruktur und Tools</a><ul>
<li class="chapter" data-level="3.1" data-path="infrastruktur-und-tools.html"><a href="infrastruktur-und-tools.html#cloud-setup"><i class="fa fa-check"></i><b>3.1</b> Cloud Setup</a><ul>
<li class="chapter" data-level="3.1.1" data-path="infrastruktur-und-tools.html"><a href="infrastruktur-und-tools.html#microsoft-azure"><i class="fa fa-check"></i><b>3.1.1</b> Microsoft Azure</a></li>
<li class="chapter" data-level="3.1.2" data-path="infrastruktur-und-tools.html"><a href="infrastruktur-und-tools.html#google-cloud"><i class="fa fa-check"></i><b>3.1.2</b> Google Cloud</a></li>
<li class="chapter" data-level="3.1.3" data-path="infrastruktur-und-tools.html"><a href="infrastruktur-und-tools.html#amazon-web-services-aws"><i class="fa fa-check"></i><b>3.1.3</b> Amazon Web Services (AWS)</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="infrastruktur-und-tools.html"><a href="infrastruktur-und-tools.html#verwendete-software"><i class="fa fa-check"></i><b>3.2</b> Verwendete Software</a><ul>
<li class="chapter" data-level="3.2.1" data-path="infrastruktur-und-tools.html"><a href="infrastruktur-und-tools.html#r-rstudio-server"><i class="fa fa-check"></i><b>3.2.1</b> R / RStudio Server</a></li>
<li class="chapter" data-level="3.2.2" data-path="infrastruktur-und-tools.html"><a href="infrastruktur-und-tools.html#python-anaconda"><i class="fa fa-check"></i><b>3.2.2</b> Python / (Ana)conda</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="analyse.html"><a href="analyse.html"><i class="fa fa-check"></i><b>4</b> Analyse</a><ul>
<li class="chapter" data-level="4.1" data-path="analyse.html"><a href="analyse.html#einfache-optimierungen"><i class="fa fa-check"></i><b>4.1</b> Einfache Optimierungen</a><ul>
<li class="chapter" data-level="4.1.1" data-path="analyse.html"><a href="analyse.html#ohne-berücksichtigung-der-marktvolatilität"><i class="fa fa-check"></i><b>4.1.1</b> Ohne Berücksichtigung der Marktvolatilität</a></li>
<li class="chapter" data-level="4.1.2" data-path="analyse.html"><a href="analyse.html#mit-berücksichtigung-der-marktvolatilität"><i class="fa fa-check"></i><b>4.1.2</b> Mit Berücksichtigung der Marktvolatilität</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="analyse.html"><a href="analyse.html#nearest-neighbor-ansätze"><i class="fa fa-check"></i><b>4.2</b> Nearest Neighbor Ansätze</a><ul>
<li class="chapter" data-level="4.2.1" data-path="analyse.html"><a href="analyse.html#distanzmasse"><i class="fa fa-check"></i><b>4.2.1</b> Distanzmasse</a></li>
<li class="chapter" data-level="4.2.2" data-path="analyse.html"><a href="analyse.html#setup-und-berechnungsdauer"><i class="fa fa-check"></i><b>4.2.2</b> Setup und Berechnungsdauer</a></li>
<li class="chapter" data-level="4.2.3" data-path="analyse.html"><a href="analyse.html#bestimmung-kauf--und-verkaufspreise"><i class="fa fa-check"></i><b>4.2.3</b> Bestimmung Kauf- und Verkaufspreise</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="analyse.html"><a href="analyse.html#neuronale-netzwerke"><i class="fa fa-check"></i><b>4.3</b> Neuronale Netzwerke</a><ul>
<li class="chapter" data-level="4.3.1" data-path="analyse.html"><a href="analyse.html#diskretisierung"><i class="fa fa-check"></i><b>4.3.1</b> Diskretisierung</a></li>
<li class="chapter" data-level="4.3.2" data-path="analyse.html"><a href="analyse.html#architektur"><i class="fa fa-check"></i><b>4.3.2</b> Architektur</a></li>
<li class="chapter" data-level="4.3.3" data-path="analyse.html"><a href="analyse.html#ergebnisse"><i class="fa fa-check"></i><b>4.3.3</b> Ergebnisse</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="summary-and-outlook.html"><a href="summary-and-outlook.html"><i class="fa fa-check"></i><b>5</b> Fazit und Ausblick</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Die Bestimmung optimaler Kauf- und Verkaufspreise von Basiswerten zur Wahrung der täglichen Delta-Neutralität beim Handel von Aktienoptionen mit Hilfe verschiedener Machine Learning Methoden</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="analyse" class="section level1">
<h1><span class="header-section-number">Kapitel 4</span> Analyse</h1>
<p>Nachfolgende Analyse hat das Ziel einen Kauf- sowie einen Verkaufskurs zu prognostizieren, bei dem der Delta-Hedge von Optionen mit maximalem Payoff erfolgen kann. Zu diesem Zweck werden verschiedene Methoden ausprobiert und miteinander verglichen. Diese sind:</p>
<ul>
<li>Einfache Optimierungen</li>
<li>Nearest Neighbors (KNN)</li>
<li>Neuronale Netze</li>
</ul>
<p>Allen Analysen gemein ist, dass gefundene Strategien mit der Referenzstrategie verglichen wird, welche keine innertägliche Anpassung des Deltas vorsieht. Eine weitere Gemeinsamkeit liegt darin, dass die verwendeten Daten keine Aussage über den Verlauf des Preises innerhalb des Tages zulassen. Insbesondere kann nicht ermittelt werden, ob zuerst eine obere oder eine untere Preisgrenze überschritten wurde. Da diese Reihenfolge aber wie in Kapitel <a href="intro.html#forschungsfrage">1.2</a> ausgeführt von Relevanz ist, wird für alle Analysen ein Ansatz verwendet, bei welchem zufällig bestimmt wird, ob am jeweiligen Tag zuerst eine Abwärts- oder eine Aufwärtsbewegung stattgefunden hat.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> Auch ein mehrmaliges Erreichen der Kauf- und Verkaufsschwelle ist innerhalb des Tages bei sehr fluktierenden Preisen in Realität denkbar. Es wären bezüglich Optimierung des Payoffs sogar sehr wünschenswerte Ereignisse. Auf die Berücksichtigung solcher Fälle wird in der Analyse allerdings verzichtet. Das Bewusstsein über deren Existenz ist aber bei der Interpretation der Ergebnisse interessant, da die Payoffs der gefundenen Strategien diesbezüglich als untere Grenzen betrachtet werden können.</p>
<p>Eine weitere Gemeinsamkeit der Analysen ist, dass der analysierte Datensatz in einem ersten Schritt in ein Trainings- (80%) und ein Testdatensatz (20%) aufgeteilt wird. Diese Aufteilung erfolgt zufällig und zwecks Vergleichbarkeit für alle Methoden gleich.</p>
<div id="einfache-optimierungen" class="section level2">
<h2><span class="header-section-number">4.1</span> Einfache Optimierungen</h2>
<div id="ohne-berücksichtigung-der-marktvolatilität" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Ohne Berücksichtigung der Marktvolatilität</h3>
<p>Eine erste Möglichkeit, optimale Kauf- und Verkaufspreise zu finden, besteht darin, diese als prozentuale Abweichungen vom aktuellen Eröffnungspreis festzulegen. Die resultierenden Payoffs bei einer solchen Festlegung lassen sich dann ins Verhältnis zum Referenzpayoff mit Ausgleich per Tagesende stellen. Ein Payoff-Verhältnis über 1 kennzeichnet damit eine Strategie, welche der Referenzstrategie überlegen ist. Verhältnisse unter 1 kennzeichnen unterlegene Strategien.</p>
<div class="figure"><span id="fig:symmetric-open-change"></span>
<img src="Masterarbeit_files/figure-html/symmetric-open-change-1.png" alt="Payoffvergleich bei symmetrischer Abweichung vom Eröffnungspreis (ohne Berücksichtigung Marktvolatilität)" width="672" />
<p class="caption">
Abbildung 4.1: Payoffvergleich bei symmetrischer Abweichung vom Eröffnungspreis (ohne Berücksichtigung Marktvolatilität)
</p>
</div>
<p>Dieses Verhältnis veranschaulicht Abbildung <a href="analyse.html#fig:symmetric-open-change">4.1</a> für unterschiedliche Auslenkungshöhen. Lesebeispiel: Werden die Kauf- und Verkaufspreise 2.5% unter und über dem Eröffnungskurs des jeweiligen Tages gesetzt, so resultiert ein Gewinn, welcher rund 8% über demjenigen der Referenzstrategie liegt.</p>
<p>Bei genauerer Betrachtung weist die Kurve einige interessante Eigenschaften auf: Der höchste Payoff wird bei einer Auslenkung der Preise um 0.9% erreicht. Der Payoffüberschüss beträgt an diesem Punkt rund 8.3%. Gleichzeitig zeigt sich, dass ein mehr oder weniger konstanter Überschuss von rund 8% im ganzen Auslenkungsbereich von 0.7 bis rund 4% erreicht werden kann.</p>
<p>Während im Bereich tieferer Auslenkungen viele kleinere Gewinne realisiert werden, sind es beim Setzen breiterer Schranken nur noch wenige, dafür grössere. Die Kurve zeigt, dass sich diese beiden Effekte im genannten Bereich in etwa die Waage halten. Dieses Ergebnis ist insofern interessant, als dass die genaue Preisbestimmung gar nicht von so grosser Relevanz sein könnte. Wichtig dabei zu erwähnen ist auch, dass während der Datenbereinigung tendenziell grosse Auslenkungen aus dem Datensatz entfernt wurden (<a href="daten.html#bereinigung">2.2.1</a>). Werden vermehrt auch extreme Marktbewegungen zugelassen, verschiebt sich die optimale Auslenkung der Preisschranken nach oben. In Kombination mit der Erkenntnis, dass auch bei stärkerer Bereinigung gute Payoffs bis 4% Auslenkung erreicht werden, könnte dies eine Motivation sein, die Preise eher breiter zu setzen.</p>
<p>Eine weitere Besonderheit der Kurve zeigt sich mit dem Abwärtsknick bei sehr kleinen Auslenkungen. Erklären lässt sich dieser Knick dadurch, dass bei allen Kursverläufen, bei denen der Eröffnungskurs gleichzeitig Höchst- oder Tiefstkurs ist, mindestens eine Schranke nicht mehr erreicht werden kann. Bereits beim Setzen etwas grösserer Schranken wird dieser Effekt wieder mehr als ausgeglichen.</p>
<p>Auffällig ist auch die Tatsache, dass eine Auslenkung von 0 (und damit einem Wiederherstellen der Delta-Neutralität gleich zum Eröffnungskurs) eine deutlich bessere Performance als die Referenzstrategie aufweist. Dies lässt sich damit erklären, dass die Werte im Datensatz offenbar eine Tendenz des “Overshootings” der Eröffnungspreise zeigen. Das beobachtete Bild lässt vermuten, dass sich die Preise im Laufe des Tages in der Tendenz wieder eher Richtung Schlusskurs des Vortages entwickeln. Der Ausgleich der aufgebauten Delta-Position “über Nacht” gleich zum Eröffnungskurs scheint damit besser, als bis am Abend zu warten. Eine praktische Schwierigkeit könnte aber eine noch geringe Liquidität der Märkte bei Marktöffnung sein.</p>
<p>Schliesslich stellt sich auch die Frage, inwiefern die gefundenen Ergebnisse als statistisch signifikant bezeichnet werden können. Zur Beurteilung dieser Frage wurde mittels Bootstrapverfahren ein 95%-Konfidenzband der Kurve ermittelt. Dieses ist als grau schraffierte Fläche am Rand der Kurve ersichtlich. Es zeigt sich, dass dieses Band relativ schmal ausfällt. Dies kann als Konsequenz der ausführlichen Datenbereinigung gesehen werden. Diese führt dazu, dass auch über verschiedene Boostrap-Samples hinweg die Payoffs stabil und wenig beeinflusst durch einzelne Beobachtungen ausfallen.</p>
<p>Als zweites Mass zur Beurteilung der Aussagekraft lassen sich auch die Werte des Testdatensatzes heranziehen. In diesem beträgt der Payoffüberschuss im Vergleich zur Referenzstrategie bei 0.9% Auslenkung ebenfalls rund 8.3% und auch bei einer Auslenkung von 4% kommt der Überschuss bei 7.7% zu liegen. Beide Werte zeigen damit hohe Ähnlichkeit zum Trainingsdatensatz.</p>
</div>
<div id="mit-berücksichtigung-der-marktvolatilität" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Mit Berücksichtigung der Marktvolatilität</h3>
<p>Die bisherige Analyse untersucht die Auslenkung der Kauf- und Verkaufspreise um den gleichen prozentualen Wert für alle Einträge im Datensatz. Die Bimodalität der Kurve in Abbildung <a href="analyse.html#fig:symmetric-open-change">4.1</a> deutet darauf hin, dass es sich dabei um eine Überlagerung mehrerer Kurven handeln könnte. Gelänge es, diese zu separieren und einzelnen Gruppen von Kursverkäufen zuzuweisen, könnten individuellere Preisschranken gewählt werden. Mit Hilfe dieser könnte der Payoff im Idealfall weiter gesteigert werden.</p>
<p>Als Klassifizierungsmerkmal sei dazu die Volatilität der vergangenen 10 Handelstage herangezogen. Die Vermutung liegt nahe, dass eine volatile Marktsituation in der kurzfristigen Vergangenheit auch am nächsten Tag fortgesetzt werden könnte (beispielsweise Zeiten mit vielen marktrelevanten Informationen wie Finanzkrise, Corona-Krise, Dividenden-Saison). Umgekehrt könnten eher ruhig verlaufende Börsentage in den vergangenen Tagen auf eine ruhige Situation auch am aktuellen Tag hinweisen (beispielsweise ruhigere Börsentage während Sommerferien).</p>
<p>Um dies zu untersuchen werden alle Einträge des Datensatzes in zwei Gruppen aufgeteilt. Einträge, welche eine aktuelle 10-Tages-Volatilität über dem Median aufweisen, werden einer Gruppe hoher Volatilität, die andern Einträge einer Gruppe tiefer Volatilität zugeordnet. Zu beachten gilt es hierbei, dass der Medianwert dabei einerseits nur innerhalb des jeweiligen Tickers betrachtet wird und für dessen Berechnung auch nur vergangene Werte miteinbezogen werden.</p>
<p>Für beide Gruppen lassen sich danach im Trainingsset die bereits bekannten Payoffvergleiche mit dem Referenzszenario durchführen und graphisch darstellen (vgl. Abbildung <a href="analyse.html#fig:symmetric-open-change-vol">4.2</a>).</p>
<div class="figure"><span id="fig:symmetric-open-change-vol"></span>
<img src="Masterarbeit_files/figure-html/symmetric-open-change-vol-1.png" alt="Payoffvergleich bei symmetrischer Abweichung vom Eröffnungspreis (mit Berücksichtigung Marktvolatilität)" width="672" />
<p class="caption">
Abbildung 4.2: Payoffvergleich bei symmetrischer Abweichung vom Eröffnungspreis (mit Berücksichtigung Marktvolatilität)
</p>
</div>
<p>Die Grafiken zeigen das erwartete Bild. Für die Gruppe tieferer vergangener Volatilitäten wird der maximale Payoff bei einer Auslenkung von 0.6% erreicht. Bei der Gruppe höherer Volatilität bei 3.7%. Angewendet auf das Testset resultiert ein Payoffüberschuss von rund 9.6%. Dies ist höher aus als im ungruppierten Fall.</p>
<p>Insbesondere bei der Gruppe der höheren Volatilitäten weist die Kurve aber weiterhin eine bimodale Form auf. Es scheint, als ob mit der gemachten Gruppierung zwar ein Teil der Varianz des aktuellen Tages erklärt werden konnte, Teile davon aber weiter unerklärt bleiben. Eine Möglichkeit bestünde nun darin, die Anzahl Gruppierungen weiter zu erhöhen, indem beispielsweise nicht nur der Median, sondern die Quartile, Dezile, Percentile etc. als Klassifikationsgrenzen gewählt werden. Darauf sei an dieser Stelle verzichtet und zu andern Ansätzen übergegangen.</p>
</div>
</div>
<div id="nearest-neighbor-ansätze" class="section level2">
<h2><span class="header-section-number">4.2</span> Nearest Neighbor Ansätze</h2>
<p>Die ökomische Theorie scheint keinen offensichtlichen Grund zu liefern, weshalb Aktien in genau 2 (oder x) Volatilitätsgruppen eingeteilt werden sollten. Auf der andern Seite haben bisherige Analysen gezeigt, dass die aktuelle Volatilität ein erklärender Faktor für die aktuelle Volatilität sein kann. Im vorliegenden Kapitel soll dieser Gedanke weiter verfolgt werden.</p>
<div id="distanzmasse" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Distanzmasse</h3>
<p>Die Idee des Ansatzes dieses Kapitels besteht darin, nicht lediglich x vordefinierte Gruppen für symmetrische Abweichungen zu finden, sondern ähnliche Kursverläufe in der Vergangenheit zu finden und individuell darauf zu reagieren. Die zugrunde liegende Hypothese ist dabei, dass bei ausreichender Historie ähnliche Muster erkannt und daraus Rückschlüsse auf die Kursentwicklung des aktuellen Tages gemacht werden können. Dieses Vorgehen hat gegenüber klassischeren Regressionsansätzen den Vorteil, dass keine Annahmen über Form der Abhängigkeit gemacht werden musst <span class="citation">(vgl. Altman <a href="#ref-altman">1992</a>)</span>.</p>
<p>Da es sehr unwahrscheinlich ist, die genau gleichen Kursverläufe in der Historie wiederzufinden, muss ein Distanzmass definiert werden, welches die Ähnlichkeit der Verläufe quantifiziert. Sei dafür jeder Kursverlauf <span class="math inline">\(i\)</span> als Vektor <span class="math inline">\(hist_{i_t}\)</span> notiert.</p>
<p><span class="math display">\[ hist_{i, t} = (Open_{i, t}, High_{i, t}, Low_{j, t+1}, Close_{i, t+1}, Open_{i, t+1}, ..., Close_{i, 1}, Open_{i, 0}) \]</span></p>
<p>Der Index <span class="math inline">\(t\)</span> gibt dabei an, wie viele Tage der Vergangenheit mit einbezogen werden. Ein Index von 0 bezieht sich auf den zu prognostizierenden, aktuellen Tag. Um die Ähnlichkeit zweier Einträge <span class="math inline">\(i\)</span> und <span class="math inline">\(j\)</span> zu berechnen, bieten sich zwei Distanzmasse an:</p>
<ol style="list-style-type: decimal">
<li><p>Die Manhattan Distanz (auch L1-Norm)<br />
<span class="math display">\[ 
\Vert hist_{i,t} - hist_{j,t}\rVert_1 =  \lvert Open_{i,t} - Open_{j,t} \rvert + \lvert High_{i,t} - High_{j,t} \rvert + \ldots + \lvert Open_{j,0} - Open_{j,0} \rvert
\]</span></p></li>
<li><p>Die euklidische Distanz (auch L2-Norm)
<span class="math display">\[ 
\Vert hist_{i,t} - hist_{j,t}\rVert_2  = \sqrt{(Open_{i,t} - Open_{j,t})^2 + (High_{i,t} - High_{j,t})^2 + \ldots}
\]</span></p></li>
</ol>
</div>
<div id="setup-und-berechnungsdauer" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Setup und Berechnungsdauer</h3>
<p>Mithilfe obiger Distanzmasse lassen sich für jeden Kursverlauf, die k ähnlichsten Verläufe der Vergangenheit ermitteln. Zuvor seien an dieser Stelle aber einige Überlegungen zur Berechnungskomplexität des Problems gemacht: Der vorliegende bereinigte Datensatz weist 5’572’087 tägliche Kursverläufe auf. Soll jeder Kursverlauf mit jedem andern verglichen werden, so ergeben sich bei einem Brute-Force Ansatz 5’572’087 x 5’572’087 Distanzberechnungen. Unter Berücksichtigung der Tatsache, dass das Problem symmetrisch ist und jeder Kursverlauf nicht mit sich selbst verglichen werden muss, halbiert sich die Komplexität zwar um mehr als die Hälfte, bleibt aber so gross, dass es zum Zeitpunkt des Schreibens dieser Arbeit nicht innerhalb weniger Sekunden oder Minuten auf einem handelsüblichen Heim-Computer berechnet werden kann.</p>
<p>Die für die Nearest Neighbors Suche eingesetzten Bibliotheken greifen daher typischerweise auf sophistiziertere Vorgehensweisen zurück. Eine davon sieht die Verwendung von multidimensionalen Suchbäumen (Search Trees, auch Kd-Trees) vor. Diese gehen auf eine Idee von <span class="citation">Bentley (<a href="#ref-bentley">1975</a>)</span> zurück. Sie basiert darauf, dass jede Dimension in zwei Bereiche aufgeteilt wird (beispielsweise beim Median). Dadurch wird der Raum in viele kleinere Sub-Räume aufgeteilt. Der Algorithmus macht sich danach zu Nutze, dass er nicht den ganzen Raum absuchen muss. Beginnend im aktuellen Sub-Raum werden schrittweise alle benachbarten Räume abgesucht, bis die geforderte Anzahl nächster Nachbarn gefunden ist. Allerdings benötigt dabei sowohl der Aufbau des Baumes wie auch die Suche im Baum Berechnungszeit.</p>
<p>Die Anzahl der Dimensionen beeinflusst dabei die benötigte Zeit erheblich. Im vorliegenden Fall liegt diese im Falle einer Historie von 10 Tagen à 4 Werten bei 41, wenn zusätzlich auch der (bekannte) Eröffnungspreis des aktuellen Tages mit einbezogen wird. Tests ergaben, dass die dafür benötigte Rechenzeit zu hoch war. Die zu verwendende Zeitperiode wurde daher auf 3 Tage beschränkt. Dies resultiert in einer deutlichen Reduktion der Suchdimensionen auf 13.</p>
<p>Der vorliegende Fall unterscheidet sich von andern Nearest Neighbor Problemen ferner dadurch, dass für jeden Eintrag lediglich Kursverläufe der Vergangenheit betrachtet werden sollten. Im Hinblick auf die Suchstrukur bedeutet dies, dass der KD-Tree nicht nur einmalig aufgebaut und danach für alle Verläufe auf Nachbarn durchsucht werden kann. Vielmehr muss der KD-Tree für jedes Datum mit allen vorangegangenen Kursverläufen neu aufgebaut werden. Dieser spezielle Setup kommt einer Brute-Force Methode ihrerseits wieder entgegen, da aufgrund des Datums-Filter nicht stets alle Einträge durchsucht werden müssten.</p>
<p>Beiden Ansätzen gemein ist hingegen, dass sie sich sehr gut parallelisieren lassen und Bibliotheken zur Verfügung stehen, welche diese Methoden effizient implementieren. Wir entscheiden uns für eine KD-Tree Implementation ohne Approximation. Auf einem Rechner mit 40 Cores (20 physisch, 20 Hyperthreading Cores) dauert die Berechnung von 50 Nachbarn ca. 2 Stunden im Falle des euklidischen Distanzmasses und ca. 3 Stunden im Falle der Manhattan Distanz. Erwähnt sei an dieser Stelle aber auch, dass eine allfällige Anwendung der Methode später nur wenige Titel (respektive nur diejenigen des aktuellen Tages) umfasst. Dies ist auch unter der Verwendung einer KNN-Methode in wenigen Sekunden möglich.</p>
</div>
<div id="bestimmung-kauf--und-verkaufspreise" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Bestimmung Kauf- und Verkaufspreise</h3>
<p>Sind die Nachbarn eines Eintrages ermittelt, lässt sich eine Prognose für den weiteren Kursverlauf des aktuellen Eintrages ableiten. Dazu wird aus den bekannten zukünftigen Kursverläufen der Nachbarn mittels geeigneter Metrik ein Prognosewert für den aktuellen Tag abgeleitet.</p>
<p>Neben dem verwendeten Distanzmass bietet dieses Vorgehen weitere Freiheiten. Namentlich lassen sich die Anzahl Nachbarn und die eingesetzte Metrik zur Prognosebestimmung variieren:</p>
<ul>
<li><p><strong>Anzahl Nachbarn:</strong><br />
In der vorliegenden Arbeit werden die 50 nächsten Nachbarn jedes Eintrages ermittelt. Einmal ermittelt lassen sich davon auch weniger verwenden. Damit kann sehr einfach der Einfluss der Anzahl Nachbarn auf die Prognosequalität ermittelt werden. Ex ante lässt sich keine Präferenz für mehr oder weniger Nachbarn finden: Es lässt sich argumentieren, dass bei der Verwendung weniger Nachbarn auch diejenigen mit grösster Ähnlichkeit verwendet werden. Insbesondere bei Marktbewegungen die relativ selten sind, könnten fernere, weniger gut passende Nachbarn das Ergebnis verzerren. Umgekehrt lässt sich argumentieren, dass bei häufigeren Marktsituationen die Berücksichtigung und Mittelung von mehr Nachbarn zu einem unverzerrteren Ergebnis führt. Schliesslich könnte sich als drittes Ergebnis auch eine Konfiguration mit “mittlerer” Anzahl Nachbarn bewähren, wenn beide vorherigen Argumentationen verschmolzen werden. Aus diesem Grund analysiert und vergleicht die vorliegende Arbeit die Ergebnisse bei der Verwendung von 5, 20 und 50 Nachbarn.</p></li>
<li><p><strong>Metrik der Prognoseermittlung:</strong> data_pred = hist_pred_train
Eine erste Möglichkeit zur Prognose von Tiefst-, Höchst- und Schlusskurs besteht darin, den Mittelwert der Kursfortsetzungen der Nachbarn zu wählen. Alternativ zur einfachen Mittelwertbildung sind auch andere Verfahren denkbar. Beispielsweise wäre auch die Berücksichtigung der Distanz als Gewichtungsfaktor möglich. Aus Gründen der Einfachheit verzichten wir an dieser Stelle darauf und verwenden neben dem gleichgewichteten Mittelwert den Median als zweites Prognosemass. Dieses reagiert weniger sensitiv auf Ausreisser innerhalb der Nachbarn.</p></li>
</ul>
<p>Abbildung <a href="analyse.html#fig:example-nearest-neighbor-outlook">4.3</a> illustriert dieses Vorgehen anhand eines Beispiels. Die Grafiken zeigen die Kursverläufe des aktuellen Eintrages (rot) sowie diejenigen der nächsten 5 Nachbarn für den zu prognostizierenden Tag (t) sowie die 3 jeweils vorangegangenen Handelstage (t-1, … , t-3). Die gestrichelte rote Linie zeigt den als Mittelwert der Nachbarn prognostizierten Wert.</p>
<div class="figure"><span id="fig:example-nearest-neighbor-outlook"></span>
<img src="Masterarbeit_files/figure-html/example-nearest-neighbor-outlook-1.png" alt="Exemplarische Kursprognose auf Basis nächster Nachbarn" width="672" />
<p class="caption">
Abbildung 4.3: Exemplarische Kursprognose auf Basis nächster Nachbarn
</p>
</div>
<p>Sind Höchst- und Tiefstpreise prognostiziert und werden diese als Kaufs- respektive Verkaufsschranken gewählt, lässt sich der Payoff berechnen und mit demjenigen der Referenzstrategie vergleichen.</p>
<p>Da bei der Suche nach Nachbarn nur Einträge der Vergangenheit berücksichtigt werden, nimmt die Anzahl verfügbarer Vergleichsverläufe mit der Zeit zu. Stellt man den Payoff-Vergleich für das Trainingsset über die Zeit dar (vgl. Abbildung <a href="analyse.html#fig:knn-learning-history">4.4</a>), lassen sich folgende Eigenschaften der Kurve identifizieren:</p>
<ol style="list-style-type: decimal">
<li>Hohe Volatilität des Vergleichsfaktors zu Beginn</li>
<li>Steigende Faktorhöhe mit fortlaufender Zeitdauer</li>
<li>Abflachung im Laufe der Zeit auf ein stabiles Level</li>
</ol>
<p>Alle Eigenschaften decken sich mit der Intuition. Da zu Beginn der Datenreihe nur sehr wenige Vergleichsverläufe zur Verfügung stehen, reagiert die Kurve sehr sensitiv und schlägt entsprechend aus. Dies glättet sich im Laufe der Zeit und dem Vorhandensein von mehr Vergleichsmöglichkeiten. Die zweite Eigenschaft des steigenden Faktors zeigt, dass der erhoffte Lerneffekt einzutreten scheint. Tatsächlich scheinen ähnliche Kursverläufe in der Vergangenheit zukünftige Entwicklungen teilweise erklären zu können. Dies deckt sich mit der Erkenntnis des vergangenen Kapitels. Anders als zuvor kann dieser Lernmechanismus hier aber sehr individuell und nicht beschränkt auf zwei Gruppen erfolgen. Die dritte Eigenschaft zeigt, dass dieser Lerneffekt nach gewisser Zeit gesättigt scheint.</p>
<div class="figure"><span id="fig:knn-learning-history"></span>
<img src="Masterarbeit_files/figure-html/knn-learning-history-1.png" alt="Entwicklung der Modellperformance über die Zeit" width="672" />
<p class="caption">
Abbildung 4.4: Entwicklung der Modellperformance über die Zeit
</p>
</div>
<p>Eine zweite Möglichkeit, welche sich alternativ zur direktem Prognose der Preise anbietet, besteht darin, die Werte als Abweichung vom Eröffnungskurs zu modellieren. Hierzu wird für jeden Nachbarn die Differenz von Eröffnungs- und Tiefstpreis respektive Eröffnungs- und Höchstpreis berechnet. Die entsprechende Metrik (Mittelwert oder Median) wird dann auf diese Werte angewandt und auf den tatsächlichen Eröffnungspreis des aktuellen Tages appliziert. Dies hat den Vorteil, dass der bekannte Eröffnungspreis keiner Unsicherheit mehr unterliegt.</p>
<p>Tabelle <a href="analyse.html#tab:knn-payoff-factors">4.1</a> stellt die Ergebnisse aller Parametrisierungen ermittelt für den Testdatensatz einander gegenüber. Es zeigt sich, dass eine direkte Prognose der Preise der Vorhersage der Preisabweichungen vom Eröffnungspreis klar unterlegen ist. Bezüglich Distanzmass lässt sich kein eindeutiger Gewinner feststellen, beide Masse weisen ähnliche Performance aus. Ähnliches gilt für die Anzahl der berücksichtigten Nachbarn.</p>
<p>Allen Ergebnissen gemein ist hingegen, dass sie die Strategie der symmetrischen Preisauslenkungen nicht übertreffen können. Im Gegenteil fallen die Payoffs im Vergleich trotz deutlich höherem Berechnungsaufwand in der Tendenz schlechter aus. Der Payoff der Referenzstrategie wird aber weiter deutlich geschlagen. Die Hoffnung, dass mit der Individualisierung der einzelnen Einträge mehr kursrelevante Information extrahiert werden kann, bestätigt sich damit vorerst nicht. Ein Grund könnte die durch die Berechnungskomplexität beschränkte Begrenzung auf ein Zeitfenster von 3 vorangegangenen Tagen sein.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:knn-payoff-factors">Tabelle 4.1: </span>Überschussfaktoren im Vergleich zur Referenzstrategie
</caption>
<thead>
<tr>
<th style="border-bottom:hidden" colspan="1">
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Euklidisch
</div>
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Manhattan
</div>
</th>
</tr>
<tr>
<th style="text-align:right;">
</th>
<th style="text-align:right;">
Mittelwert
</th>
<th style="text-align:right;">
Median
</th>
<th style="text-align:right;">
Mittelwert
</th>
<th style="text-align:right;">
Median
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;font-weight: bold;">
5
</td>
<td style="text-align:right;">
1.014
</td>
<td style="text-align:right;">
1.076
</td>
<td style="text-align:right;">
1.012
</td>
<td style="text-align:right;">
1.075
</td>
</tr>
<tr>
<td style="text-align:right;font-weight: bold;">
20
</td>
<td style="text-align:right;">
1.002
</td>
<td style="text-align:right;">
1.084
</td>
<td style="text-align:right;">
1.000
</td>
<td style="text-align:right;">
1.083
</td>
</tr>
<tr>
<td style="text-align:right;font-weight: bold;">
50
</td>
<td style="text-align:right;">
0.995
</td>
<td style="text-align:right;">
1.083
</td>
<td style="text-align:right;">
0.993
</td>
<td style="text-align:right;">
1.082
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="neuronale-netzwerke" class="section level2">
<h2><span class="header-section-number">4.3</span> Neuronale Netzwerke</h2>
<p>Als dritte Methode werden im vorliegenden Kapitel neuronale Netzwerke analysiert. Diese Methode erlaubt den Umgang mit sehr grossen Datenmengen, weshalb die Beschränkung auf ein sehr kurzes Zeitfenster hier nicht nötig ist. Die Architektur wird sehr einfach gehalten und besteht aus mehreren vollständig miteinander verknüpfter Schichten (Dense Layer). Das Ziel der Analyse bleibt das Gleiche wie in den Kapiteln zuvor: Es sollen basierend auf vergangenen Kursverläufen möglichst optimale Kauf- und Verkaufskurse für den laufenden Tag prognositiert werden.</p>
<p>Wie bereits ausgeführt, ist der Payoff abhängig von der Höhe der Preisbewegung und der Wahrscheinlichkeit der Ausführung. Da der Payoff in quadratischer Form von der Höhe der Kursbewegung abhängt, haben frühere Überlegungen bereits gezeigt, dass es allenfalls vorteilhaft sein könnte, eher breite Preisschranken zu setzen. Diese werden zwar weniger oft erreicht, werfen im Falle der Ausführung aber einen umso höheren Payoff ab. Ziel ist es daher, nicht nur Punktprognosen für die Preisextreme des Tages zu machen, sondern auch Erkenntnisse über deren Verteilungen zu gewinnen. Sind diese bekannt, können unter Berücksichtigung der Eintretenswahrscheinlichkeiten der jeweiligen Preisentwicklungen die optimalen Kauf- und Verkaufsschranken eruiert werden.</p>
<div id="diskretisierung" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Diskretisierung</h3>
<p>Zur Modellierung der Verteilung werden die Tiefst-, Höchst- und Schlusspreise in einem ersten Schritt diskretisiert. Für jeden Preisbereich [Bucket] lässt sich mit dem Modell später eine Eintretenswahrscheinlichkeit ermitteln. Zu beachten gilt es hierbei, dass die einzelnen Buckets möglichst gleich viele Werte enthalten. Ist dies nicht der Fall, bestehen für den Algorithmus Anreize, ein Element dem grössten Bucket zuzuordnen. Es gibt verschiedene Oversampling und Undersampling Techniken, um mit solchen Ungleichgewichten umzugehen. Im vorliegenden Fall lassen sich die Buckets aber gut so wählen, dass sie in häufigen Datenbereichen enger sind als in Bereichen mit weniger Daten. Wir verwenden dazu zwei unterschiedliche Einteilungsverfahren:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Unabhängige Modellierung von Low, High und Close Preisen</strong><br />
Eine erste Möglichkeit besteht darin, die Diskretisierung von Tiefst-, Höchst- und Schlusspreisen unabhängig voneinander zu gestalten. Dies hat den Nachteil, dass allenfalls wertvolle Information verloren geht. Dies zeigt sich insbesondere dadurch, dass Bucketkombinationen entstehen, welche in Realität nicht möglich sind. Beispielsweise sind dies Kombinationen, bei denen der Tiefstpreis höher ausfällt als der Höchstpreis.
Der Vorteil dieses Vorgehens liegt andererseits darin, dass die Ermittlung gleich grosser Buckets mittels Quantilbildung sehr einfach möglich ist. Zudem lässt sich das Problem bei Annahme von Unabhängigkeit in drei kleinere Klassifikationsprobleme aufteilen. Werden beispielsweise sowohl Tiefst-, Höchst- und Schlusspreise mit jeweils 30 Buckets modelliert, resultieren drei Klassifikationsprobleme à 30 Klassen. Kombiniert man diese, resultieren <span class="math inline">\(27&#39;000 \ (= 30^3)\)</span> Preisszenarien, wobei sich die Wahrscheinlichkeit für jedes dieser Szenarien als Multiplikation der einzelnen Preiswahrscheinlichkeiten ergibt.</p></li>
<li><p><strong>Abhängige Modellierung von Low, High und Close Preisen</strong><br />
Eine zweite Möglichkeit besteht darin, die Diskretisierung mit Annahme einer Abhängigkeit der einzelnen Tagespreise vorzunehmen. Die Diskretisierung unter dem Ziel möglichst gleich grosser Buckets gestaltet sich dabei etwas schwieriger. Wir verwenden dazu den Ansatz, welcher zuerst die Tiefstpreise in gleich grosse Buckets aufteilt. Für jedes dieser Buckets werden danach die darin enthaltenen Höchstkurse in gleich grosse Buckets aufgeteilt. Die resultierenden Buckets werden danach wiederum in möglichst gleich grosse Buckets bezüglich Schlusspreisen aufgeteilt. Anders als im unabhängigen Fall sind die Bucket-Grenzen damit nicht immer gleich. Das Klassifikationsproblem lässt auch nicht mehr auf kleinere Modelle aufteilen. Alle 27’000 Szenarien müssen damit in einem grösseren Modell auf einmal bearbeitet werden. Der Vorteil dieser Methode liegt darin, dass eine Abhängigkeit der Preise ökonomisch plausibler ist und auch unmögliche Szenarien nicht mehr vorkommen.</p></li>
</ol>
</div>
<div id="architektur" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Architektur</h3>
<p>Neuronale Netze haben sich insbesondere im Bereich der Computer Vision - beispielsweise zur Klassifikation von Bildern - als sehr leistungsfähig herausgestellt <span class="citation">(vgl. Krizhevsky, Sutskever, and Hinton <a href="#ref-krizhevsky_sutskever_hinton">2012</a>)</span>. Diese Probleme zeichnen sich dadurch aus, dass mit der Verwendung von Pixel-Daten sehr grosse Datenmengen verarbeitet werden müssen. Im vorliegenden Fall sind die Datenmengen deutlich kleiner. Aus diesem Grund und aus Überlegungen der Einfachheit entscheiden wir uns daher für eine einfache Architektur mit zwei versteckten Dense Layer mit jeweils 512 Knoten. Diverse Tests zur Erhöhung der Knotenanzahl oder dem Beifügen weiterer Layer haben nicht zu wesentlich andern Ergebnissen geführt. Neben der Anzahl Layer stellt sich zudem die Frage nach der geeigneten Abbildung des Output Layers. Hierbei werden zwei Vorgehensweisen untersucht.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Klassische Klassifikation</strong><br />
Eine erste Möglichkeit besteht darin, die Ordinalität der Klassen zu vernachlässigen. Eine solche Vernachlässigung macht insbesondere dann Sinn, wenn die einzelnen Klassen in keiner Abhängigkeit zueinander stehen. Wiederum bietet sich der Vergleich mit der Bildklassifikation an - beispielsweise der Klassifikation des Bildes einer Katze. Es lässt sich argumentieren, dass es bei einer Fehlklassifikation keine Rolle spielt, ob das Bild als Auto oder als Apfel erkannt wurde. Keines der beiden ist “weniger” oder “mehr” richtig, beide sind schlicht falsch. Für diese Art der Klassifikation bietet sich eine Verlustfunktion der Art “Categorical Crossentropy” und eine Aktivierung des Outputlayers mittels “Softmax”-Funktion an.</p></li>
<li><p><strong>Ordinale Klassifikation</strong><br />
Im vorliegenden Fall - insbesondere bei unabhängiger Klassifikation von Tiefst-, Höchst- und Schlusspreisen - lassen sich die Buckets in eine logische Reihenfolge bringen. Wird nun ein Wert anstatt in Bucket 10 fälschlicherweise in Bucket 9 klassifiziert, scheint dies der kleinere Fehler zu sein, als wenn die Klassifikation in Klasse 1 erfolgt wäre. Eine Möglichkeit, diese Art von Klassifikationen in neuronalen Netzen zu modellieren, zeigen <span class="citation">Frank (<a href="#ref-frank_hall">2001</a>)</span>. Grob besteht die Idee darin, die Wahrscheinlichkeit der aktuellen Klasse nicht direkt, sondern als kumulierte Wahrscheinlichkeit zu modellieren. Die Wahrscheinlichkeit einer spezifischen Klasse lässt sich dann als Differenz der kumulierten Wahrscheinlichkeiten benachbarter Klassen berechnen. Zu berücksichtigen gilt es hierbei, dass die kumulative Wahrscheinlichkeit in der Theorie nicht sinken kann. Dies ist durch das Modell aber nicht garantiert. Wir lösen dieses Problem, indem für die Auswertung jeweils das Minimum der aktuellen kumulierten Wahrscheinlichkeit und der Vorhergehenden verwendet wird. In ihrer Architektur unterscheiden sich diese Art der Modelle durch eine andere Verlustfunktion (Binary Crossentropy), eine andere Aktivierungsfunktion (Sigmoid) des Output Layers sowie eine etwas anderen Kodierung der Labels.</p></li>
</ol>
</div>
<div id="ergebnisse" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Ergebnisse</h3>
<p>Wie bereits erwähnt werden alle Modelle mit jeweils 30 Buckets im Falle unabhängig modellierter Preise, respektive 27’000 Preisszenarien im Falle abhängiger Preise berechnet. Ebenfalls allen Modellen gemein ist, dass eine Batch-Grösse von 512 und ein Training über 30 Epochen gewählt wird. Ferner werden als Features alle vier Preiskennzahlen der letzten 10 Tage plus der Eröffnungskurs des aktuellen Tages verwendet. Jedes dieser Features wird vor dem Training skaliert.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> Das eigentliche Training erfolgt auf 80% des Trainingssets, 20% der Trainingsdaten dienen der Validierung.</p>
<div id="unabhängige-modelle" class="section level4">
<h4><span class="header-section-number">4.3.3.1</span> Unabhängige Modelle</h4>
<p>Während des Trainings der Netze unabhängiger Preise zeigt sich der in Abbildung <a href="analyse.html#fig:ind-train-progress">4.5</a> dargestellte Lernfortschritt. Als Genauigkeit wird hierbei der Anteil richtiger Klassifizierungen angegeben. Folgende Aussagen lassen sich aus der Darstellung gewinnen:</p>
<ul>
<li>Die Genauigkeit richtig zugeordneter Klassen liegt deutlich über derjenigen einer zufälligen Zuteilung von 3.3% <span class="math inline">\((1/30)\)</span>.</li>
<li>Die Genauigkeit der Prognose für Tiefst- und Höchstkurs ist deutlich höher als diejenige des Schlusskurses.</li>
<li>Über den Trainingsverlauf nimmt die Genauigkeit mit abnehmender Geschwindigkeit zu. Dies gilt sowohl für die Trainings- wie auch die Validierungsdaten. Ein Overfitting ist im vorliegenden Fall nicht auszumachen.</li>
</ul>
<div class="figure"><span id="fig:ind-train-progress"></span>
<img src="Masterarbeit_files/figure-html/ind-train-progress-1.png" alt="Trainingsfortschritt im Falle unabhängiger Preise" width="672" />
<p class="caption">
Abbildung 4.5: Trainingsfortschritt im Falle unabhängiger Preise
</p>
</div>
<p>Die trainierten Modelle lassen sich danach zur Prognose der Wahrscheinlichkeitsverteilungen der Daten im Testset heranziehen. Für einzelne Beobachtungen lassen sich diese einfach visualisieren. Abbildung <a href="analyse.html#fig:plot-categorical-histogram">4.6</a> zeigt die prognostizierten Verteilungen zweier exemplarischer Einträge. Diese unterscheiden sich deutlich. Während für ersteren Eintrag ein wenig volatiler Kursverlauf prognostiziert wird, erwartet das Modell im zweiten Fall eine deutlich volatilere Entwicklung.</p>
<div class="figure"><span id="fig:plot-categorical-histogram"></span>
<img src="Masterarbeit_files/figure-html/plot-categorical-histogram-1.png" alt="Exemplarische Verteilung der vorausgesagten Preise ohne explizite Modellierung der Ordinalität" width="672" />
<p class="caption">
Abbildung 4.6: Exemplarische Verteilung der vorausgesagten Preise ohne explizite Modellierung der Ordinalität
</p>
</div>
<p>Tatsächlich unterscheiden sich die realisierten Tiefts- und Höchstkurse, wenn auch nicht ganz so deutlich wie dies vom Modell prognostiziert. Die entsprechenden Realisierungen sind 99.37 und 100.83 für das erste und 98.75 und 101.5 für das zweite Beispiel.</p>
<p>Um den Payoff des ganzen Testsets berechnen zu können, müssen aus den geschätzten Verteilungen die optimalen Kauf- und Verkaufsschranken ermittelt werden. Eine erste Möglichkeit besteht darin, die Kauf- und Verkaufsschranken als Tiefst- respektive Höchstpreis desjenigen Buckets vorauszusagen, für welches das Modell die höchste Eintretenswahrscheinlichkeit prognostiziert.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p>Mit Hilfe der so ermittelten Schranken lässt sich wie bei den vorangegangenen Methoden der Payoff des Testsets ermitteln und ins Verhältnis zur Referenzstategie setzen. Es resultiert ein Überschusspayoff von 9.8%. Bereits in dieser einfachen Form ist das gelernte Modell damit ähnlich gut, wie das Modell basierend auf symmetrischen Abweichungen vom aktuellen Eröffnungskurs.</p>
<p>Während der Konzeption des Modelles wurde grosser Wert darauf gelegt, auch die Verteilung der Preise zu prognostizieren. Lediglich das wahrscheinlichste Szenario für die Prognose der Preisschranken zu verwenden, greift damit etwas kurz. Tatsächlich erlauben es die vorhandenen Werte nun auch, den Payoff verschiedener Kauf- und Verkaufsschranken für jedes der Preisszenarien zu berechnen und mit der jeweiligen Wahrscheinlichkeit zu gewichten. Als optimale Strategie lässt sich dann diejenige mit dem höchsten erwarteten Payoff auswählen.</p>
<p>Wiederum lohnt sich dabei erst ein Blick auf die Berechnungskomplexität des Problems: In der vorliegenden Analyse wurden je Preistyp 30 Buckets verwendet. Dies resultiert in 27’000 möglichen Preisszenarien. Diese können für jeden Eintrag des Testsets (rund 1’000’000 Einträge) ermittelt werden. Für jedes dieser Preisszenarien sollen wiederum verschiedene Paare von Kauf- und Verkaufsschranken getestet werden. Orientiert man sich dabei an einer ähnlichen Granularität wie bei den Preisen, resultieren 900 Szenarien für die Schranken (je 30 Werte für die Kauf- und Verkaufsschranke). Damit müssen zur kompletten Evaluation <span class="math inline">\(27&#39;000 \times 1&#39;000&#39;000 \times 900\)</span> Payoffs berechnet werden. Dies ist mit Hilfe der vorgestellten Cloud-Infrastruktur und Tools nicht innerhalb kurzer Zeit möglich.</p>
<p>Zur Reduktion der Komplexität lässt sich aber ausnützen, dass nicht alle Szenarien von gleicher Bedeutung sind. Während die Verwendung des häufigsten Szenarios und die Verwendung aller Szenarien die beiden Extrempositionen bilden, ist auch die Verwendung einiger wichtiger Preis- und Schrankenszenarien denkbar. Konkret lohnt es sich, diejenigen Szenarien auszuwählen, welche die höchsten Eintretenswahrscheinlichkeiten aufweisen. Dabei hat es sich bewährt, diese Grenze als Quantil der jeweiligen Wahrscheinlichkeiten zu wählen. Damit können nur sehr wenige, dafür wichtige Szenarien berücksichtigt werden. Die Berechnungskomplexität lässt sich damit deutlich senken. Diese Vorgehensweise hat zudem den Vorteil, dass mit einer Reduzierung des Quantils auch sehr einfach mehr Werte einbezogen werden können, sollte dies gewünscht werden. Ferner werden auch die Schrankenszenarien sehr stark reduziert, indem als Kauf- und Verkaufspreise nur die Tiefst- und Höchstwerte der betrachteten Preisszenarien berechnet werden.</p>
<p>Das beschriebene Vorgehen ist sowohl für unabhängige wie später auch für abhängige Modelle möglich. Bei den unabhängigen Modellen gibt es ferner zu beachten, dass nicht mögliche Szenarien (beispielsweise prognostizierter Höchstpreis &lt; prognostizierter Tiefstpreis) entfernt, respektive deren prognostizierte Wahrscheinlichkeiten vor der Auswertung auf 0 gesetzt werden. Bei den Modellen mit voneinander abhängigen Preisen treten solche Szenarien nicht auf.</p>
<p>Bezieht man die Buckets mit den 0.1% höchsten Eintretenswahrscheinlichkeiten (99.9% Quantil) in die Auswertung mit ein, resultiert ein Überschusspayoff von 12% gegenüber der Referenzstrategie. Durch Berücksichtigung (eines Teils) der geschätzten Verteilung kann die Modell-Performance damit noch einmal um 2.2 Prozentpunkte gesteigert werden und übertrifft damit die Performance früherer Modelle.</p>
<div class="figure"><span id="fig:plot-binary-histogram"></span>
<img src="Masterarbeit_files/figure-html/plot-binary-histogram-1.png" alt="Exemplarische Verteilung der vorausgesagten Preise mit expliziter Modellierung der Oridinalität" width="672" />
<p class="caption">
Abbildung 4.7: Exemplarische Verteilung der vorausgesagten Preise mit expliziter Modellierung der Oridinalität
</p>
</div>
<p>Führt man die gleichen Analysen auch für das Modell durch, welches die Ordinalität der Klassen explizit modelliert, ergeben sich ähnliche Ergebnisse. Wie in Abbildung <a href="analyse.html#fig:plot-binary-histogram">4.7</a> ersichtlich, weisen die beiden exemplarischen Einträge im Testdatensatz auch bei diesem Modell vergleichbare Preisverteilungen auf. Es scheint, als ob eine explizite Modellierung der Ordinalität nicht unbedingt nötig ist. Die Verteilung scheint auch ohne explizite Modellierung richtig gelernt worden zu sein.</p>
<p>Der Komplettheit halber seien die Überschusspayoffs des ordinalen Modelles an dieser Stelle dennoch aufgeführt. Diese betragen bei der Berücksichtigung des Buckets mit höchster Wahrscheinlichkeit 10.4% und bei Berücksichtigung der 0.1% grössten Wahrscheinlichkeiten 12.5%. Beide Werte sind damit etwas besser als bei der Modellierung ohne Ordinalität.</p>
</div>
<div id="abhängige-modelle" class="section level4">
<h4><span class="header-section-number">4.3.3.2</span> Abhängige Modelle</h4>
<p>Die meisten Erkenntnisse aus der Analyse der unabhängigen Modelle lassen sich auch auf die abhängigen Modelle übertragen. Ein Unterschied liegt allerdings darin, dass sich jedes der 27’000 Buckets im abhängigen Fall nicht nur auf den Bereich eines Preises, sondern auf den Bereich aller der drei Preise für Tiefst-. Höchst- und Schlusskurs gleichzeitig bezieht. Die Bildung einer Reihenfolge der Buckets für eine ordinale Modellierung sind damit nicht mehr gegeben. Ein weiterer Unterschied ergibt sich bei der Berechnungsdauer des Modelles. Diese erhöht sich im Falle des abhängigen Modelles deutlich und es empfiehlt sich die Berechnung auf einer oder mehrerer GPUs, um die Berechnungsdauer zu verkürzen.</p>
<div class="figure"><span id="fig:dep-train-progress"></span>
<img src="Masterarbeit_files/figure-html/dep-train-progress-1.png" alt="Trainingsfortschritt im Falle abhängiger Preise" width="672" />
<p class="caption">
Abbildung 4.8: Trainingsfortschritt im Falle abhängiger Preise
</p>
</div>
<p>Wie in Abbildung <a href="analyse.html#fig:dep-train-progress">4.8</a> ersichtlich, zeigt sich ein weiterer Unterschied darin, dass es während des Trainings des abhängigen Modelles innerhalb der verwendeten 30 Epochen zu einem Overfitting kommt. Dies zeigt sich dadurch, dass sich sich der Verlust des Validierungsset nach Epoche 5 wieder deutlich erhöht. Wir entscheiden uns darum dafür, das Modell aus Epoche 5 zu verwenden.</p>
<p>Für dieses lassen sich die gleichen Auswertungen wie zuvor durchführen. Wiederum lässt sich zum Vergleich mit den andern Modellen der Überschusspayoff im Vergleich mit der Referenzstrategie ermitteln. Bei Verwendung des Buckets mit höchster Eintretenswahrscheinlichkeit liegt dieser bei 11.6% und vermag vorherige Modelle zu übertreffen.</p>
<p>Werden wiederum gar die Buckets mit höchsten 0.1% Eintretenswahrscheinlichkeiten beigezogen, resultiert gar ein Überschusspayoff von 15.5%. Es handelt sich damit innerhalb der analysierten Modelle um dasjenige Modell mit der besten Performance.</p>

</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-altman">
<p>Altman, N.S. 1992. “An Introduction to Kernel and Nearest Neighbor Nonparametric Regression.” <em>The American Statistician</em> 46 (3): 175–85. <a href="https://doi.org/doi:10.1080/00031305.1992.10475879">https://doi.org/doi:10.1080/00031305.1992.10475879</a>.</p>
</div>
<div id="ref-bentley">
<p>Bentley, J.L. 1975. “Multidimensional Binary Search Trees Used for Associative Searching.” <em>Communications of the ACM</em> 18 (9): 509–17.</p>
</div>
<div id="ref-frank_hall">
<p>Frank, M., E. und Hall. 2001. “A Simple Approach to Ordinal Classification.” In <em>Lecture Notes in Computer Science</em>, 2167:145–56. <a href="https://doi.org/10.1007/3-540-44795-4_13">https://doi.org/10.1007/3-540-44795-4_13</a>.</p>
</div>
<div id="ref-krizhevsky_sutskever_hinton">
<p>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “ImageNet Classification with Deep Convolutional Neural Networks.” In <em>Advances in Neural Information Processing Systems 25</em>, edited by F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, 1097–1105. Curran Associates, Inc. <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>Alternative denkbare Vorgehensweisen sind: Immer zuerst Aufwärtsbewegung, immer zuerst Abwärtsbewegung, immer die bezügl. Payoff schlechtere Reihenfolge oder immer die bezüglich Payoff bessere Variante.<a href="analyse.html#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>Im Detail erfolgt die Skalierung auf Basis des Mittelwertes und Standardabweichung jedes Features des Trainingssets.<a href="analyse.html#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>Da ein Bucket durch untere und obere Grenze bestimmt ist, verwenden wir den Mittelwert von oberer und unterer Grenze als Vorhersagewert. Ist eine der Grenzen nicht finit, wird die andere Grenze als Vorhersagewert verwendet.<a href="analyse.html#fnref5" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="infrastruktur-und-tools.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary-and-outlook.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
